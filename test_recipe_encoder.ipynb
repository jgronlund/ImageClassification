{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13496\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Title</th>\n",
       "      <th>Instructions</th>\n",
       "      <th>Image_Name</th>\n",
       "      <th>Cleaned_Ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Miso-Butter Roast Chicken With Acorn Squash Pa...</td>\n",
       "      <td>[Pat chicken dry with paper towels, season all...</td>\n",
       "      <td>miso-butter-roast-chicken-acorn-squash-panzanella</td>\n",
       "      <td>[1 (3½–4-lb.) whole chicken, 2¾ tsp. kosher sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Crispy Salt and Pepper Potatoes</td>\n",
       "      <td>[Preheat oven to 400°F and line a rimmed bakin...</td>\n",
       "      <td>crispy-salt-and-pepper-potatoes-dan-kluger</td>\n",
       "      <td>[2 large egg whites, 1 pound new potatoes (abo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Thanksgiving Mac and Cheese</td>\n",
       "      <td>[Place a rack in middle of oven; preheat to 40...</td>\n",
       "      <td>thanksgiving-mac-and-cheese-erick-williams</td>\n",
       "      <td>[1 cup evaporated milk, 1 cup whole milk, 1 ts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Italian Sausage and Bread Stuffing</td>\n",
       "      <td>[Preheat oven to 350°F with rack in middle. Ge...</td>\n",
       "      <td>italian-sausage-and-bread-stuffing-240559</td>\n",
       "      <td>[1 (¾- to 1-pound) round Italian loaf, cut int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Newton's Law</td>\n",
       "      <td>[Stir together brown sugar and hot water in a ...</td>\n",
       "      <td>newtons-law-apple-bourbon-cocktail</td>\n",
       "      <td>[1 teaspoon dark brown sugar, 1 teaspoon hot w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              Title  \\\n",
       "0           0  Miso-Butter Roast Chicken With Acorn Squash Pa...   \n",
       "1           1                    Crispy Salt and Pepper Potatoes   \n",
       "2           2                        Thanksgiving Mac and Cheese   \n",
       "3           3                 Italian Sausage and Bread Stuffing   \n",
       "4           4                                       Newton's Law   \n",
       "\n",
       "                                        Instructions  \\\n",
       "0  [Pat chicken dry with paper towels, season all...   \n",
       "1  [Preheat oven to 400°F and line a rimmed bakin...   \n",
       "2  [Place a rack in middle of oven; preheat to 40...   \n",
       "3  [Preheat oven to 350°F with rack in middle. Ge...   \n",
       "4  [Stir together brown sugar and hot water in a ...   \n",
       "\n",
       "                                          Image_Name  \\\n",
       "0  miso-butter-roast-chicken-acorn-squash-panzanella   \n",
       "1         crispy-salt-and-pepper-potatoes-dan-kluger   \n",
       "2         thanksgiving-mac-and-cheese-erick-williams   \n",
       "3          italian-sausage-and-bread-stuffing-240559   \n",
       "4                 newtons-law-apple-bourbon-cocktail   \n",
       "\n",
       "                                 Cleaned_Ingredients  \n",
       "0  [1 (3½–4-lb.) whole chicken, 2¾ tsp. kosher sa...  \n",
       "1  [2 large egg whites, 1 pound new potatoes (abo...  \n",
       "2  [1 cup evaporated milk, 1 cup whole milk, 1 ts...  \n",
       "3  [1 (¾- to 1-pound) round Italian loaf, cut int...  \n",
       "4  [1 teaspoon dark brown sugar, 1 teaspoon hot w...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "import torch\n",
    "import models.recipe_encoder as recipe\n",
    "# import torchtext; torchtext.disable_torchtext_deprecation_warning()\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import Vocab\n",
    "from collections import Counter, OrderedDict\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "csv_file = 'Data/updated_data_with_lists.csv'\n",
    "df = pd.read_csv(csv_file, converters={\"Cleaned_Ingredients\": literal_eval, \"Instructions\": literal_eval})\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column length 13496\n",
      "['1', '(', '3½–4-lb', '.', ')', 'whole', 'chicken', '2¾', 'tsp', '.']\n",
      "['1', '(', '3½–4-lb', '.', ')', 'whole', 'chicken', '2¾', 'tsp', '.', 'kosher', 'salt', ',', 'divided', ',', 'plus', 'more', '2', 'small', 'acorn', 'squash', '(', 'about', '3', 'lb', '.', 'total', ')', '2', 'Tbsp', '.', 'finely', 'chopped', 'sage', '1', 'Tbsp', '.', 'finely', 'chopped', 'rosemary', '6', 'Tbsp', '.', 'unsalted', 'butter', ',', 'melted', ',', 'plus', '3', 'Tbsp', '.', 'room', 'temperature', '¼', 'tsp', '.', 'ground', 'allspice', 'Pinch', 'of', 'crushed', 'red', 'pepper', 'flakes', 'Freshly', 'ground', 'black', 'pepper', '⅓', 'loaf', 'good-quality', 'sturdy', 'white', 'bread', ',', 'torn', 'into', '1', 'pieces', '(', 'about', '2½', 'cups', ')', '2', 'medium', 'apples', '(', 'such', 'as', 'Gala', 'or', 'Pink', 'Lady;', 'about', '14', 'oz', '.', 'total', ')', ',', 'cored', ',', 'cut', 'into', '1', 'pieces', '2', 'Tbsp', '.', 'extra-virgin', 'olive', 'oil', '½', 'small', 'red', 'onion', ',', 'thinly', 'sliced', '3', 'Tbsp', '.', 'apple', 'cider', 'vinegar', '1', 'Tbsp', '.', 'white', 'miso', '¼', 'cup', 'all-purpose', 'flour', '2', 'Tbsp', '.', 'unsalted', 'butter', ',', 'room', 'temperature', '¼', 'cup', 'dry', 'white', 'wine', '2', 'cups', 'unsalted', 'chicken', 'broth', '2', 'tsp', '.', 'white', 'miso', 'Kosher', 'salt', 'freshly', 'ground', 'pepper']\n",
      "Len Elements: 13496\n",
      "Size: 12676\n",
      "Max: 110\n"
     ]
    }
   ],
   "source": [
    "column = df[\"Cleaned_Ingredients\"]\n",
    "\n",
    "ingr_list = []\n",
    "element_list = []\n",
    "ingr_max = 0\n",
    "print(\"Column length\", len(column))\n",
    "for w_list in column:\n",
    "    tmp =[]\n",
    "    for str_list in w_list:\n",
    "        # for word in str_list:\n",
    "        formatted = str_list.replace('.', ' . ').replace(',',' , ').replace('(',' ( ').replace(')',' ) ').replace('\"','').split()\n",
    "        ingr_list.extend(formatted)\n",
    "        new_len = len(formatted)\n",
    "        tmp.extend(formatted)\n",
    "        if new_len>ingr_max:\n",
    "            ingr_max=new_len\n",
    "    element_list.append(tmp)\n",
    "\n",
    "print(ingr_list[:10])\n",
    "print(element_list[0])\n",
    "print(\"Len Elements:\", len(element_list))\n",
    "ingr_vocab = set(ingr_list)\n",
    "ingr_vocab_size = len(ingr_vocab)\n",
    "print(\"Size:\", ingr_vocab_size)\n",
    "print(\"Max:\", ingr_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1 (3½–4-lb.) whole chicken',\n",
       " '2¾ tsp. kosher salt, divided, plus more',\n",
       " '2 small acorn squash (about 3 lb. total)',\n",
       " '2 Tbsp. finely chopped sage',\n",
       " '1 Tbsp. finely chopped rosemary',\n",
       " '6 Tbsp. unsalted butter, melted, plus 3 Tbsp. room temperature',\n",
       " '¼ tsp. ground allspice',\n",
       " 'Pinch of crushed red pepper flakes',\n",
       " 'Freshly ground black pepper',\n",
       " '⅓ loaf good-quality sturdy white bread, torn into 1\" pieces (about 2½ cups)',\n",
       " '2 medium apples (such as Gala or Pink Lady; about 14 oz. total), cored, cut into 1\" pieces',\n",
       " '2 Tbsp. extra-virgin olive oil',\n",
       " '½ small red onion, thinly sliced',\n",
       " '3 Tbsp. apple cider vinegar',\n",
       " '1 Tbsp. white miso',\n",
       " '¼ cup all-purpose flour',\n",
       " '2 Tbsp. unsalted butter, room temperature',\n",
       " '¼ cup dry white wine',\n",
       " '2 cups unsalted chicken broth',\n",
       " '2 tsp. white miso',\n",
       " 'Kosher salt',\n",
       " 'freshly ground pepper']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Cleaned_Ingredients\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pat', 'chicken', 'dry', 'with', 'paper', 'towels', 'season', 'all', 'over', 'with']\n",
      "Size: 21164\n",
      "Max: 247\n"
     ]
    }
   ],
   "source": [
    "column = df[\"Instructions\"]\n",
    "\n",
    "inst_list = []\n",
    "inst_max = 0\n",
    "for w_list in column:\n",
    "    for str_list in w_list:\n",
    "        # for word in str_list:\n",
    "        formatted = str_list.replace('.', ' . ').replace(',',' , ').replace('(','').replace(')','').replace('\"','').split()\n",
    "        inst_list.extend(formatted)\n",
    "        new_len = len(formatted)\n",
    "        if new_len>inst_max:\n",
    "            inst_max=new_len\n",
    "            \n",
    "print(inst_list[:10])\n",
    "inst_vocab = set(inst_list)\n",
    "inst_vocab_size = len(inst_vocab)\n",
    "print(\"Size:\",inst_vocab_size)\n",
    "print(\"Max:\", inst_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Miso-Butter', 'Roast', 'Chicken', 'With', 'Acorn', 'Squash', 'Panzanella', 'Crispy', 'Salt', 'and']\n",
      "Size: 6959\n",
      "Max: 21\n"
     ]
    }
   ],
   "source": [
    "column = df[\"Title\"].to_list()\n",
    "\n",
    "title_list = []\n",
    "title_max = 0\n",
    "for str_list in column:\n",
    "        # for word in str_list:\n",
    "        formatted = str_list.replace('.', ' . ').replace(',',' , ').replace('(','').replace(')','').replace('\"','').split()\n",
    "        title_list.extend(formatted)\n",
    "        new_len = len(formatted)\n",
    "        if new_len>title_max:\n",
    "            title_max=new_len\n",
    "\n",
    "print(title_list[:10])\n",
    "title_vocab = set(title_list)\n",
    "title_vocab_size = len(title_vocab)\n",
    "print(\"Size:\", title_vocab_size)\n",
    "print(\"Max:\", title_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 31551\n",
      "Max: 247\n"
     ]
    }
   ],
   "source": [
    "total_vocab = set(list(inst_vocab) + list(ingr_vocab) + list(title_vocab)) \n",
    "total_vocab_size = len(total_vocab)\n",
    "print(\"Size:\",total_vocab_size)\n",
    "total_max = max([inst_max, ingr_max, title_max])\n",
    "print(\"Max:\",total_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = {word: i for i, word in enumerate(total_vocab)}\n",
    "\n",
    "word_to_ix_ingr = {word: i for i, word in enumerate(ingr_vocab)}\n",
    "word_to_ix_inst = {word: i for i, word in enumerate(inst_vocab)}\n",
    "word_to_ix_title = {word: i for i, word in enumerate(title_vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchtext.vocab import vocab\n",
    "\n",
    "# Get longest sentence in Ingredients or Instructions for max_len\n",
    "# Use torchtext to make vocab: vocab_size = input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2644584\n",
      "2649226\n",
      "['pat', 'chicken', 'dry', 'with', 'paper', 'towels', 'season', 'all', 'over', 'with', '2', 'tsp', '.', 'salt', 'and', 'tie', 'legs', 'together', 'with', 'kitchen', 'twine', '.', 'let', 'sit', 'at', 'room', 'temperature', '1', 'hour', '.']\n",
      "21164\n",
      "21945\n",
      "['alfalfa', 'may', 'apple-filled', 'puffs', '4-inch', 'thermostatically', 'gray-ashed', 'contracts', 'flourlike', 'dime-size', 'cumin', 'churn', 'tops', 'thanks', 'crumb', 'crumb-coating', 'ranch', 'flex', '22–26', 'versatile', 'rye-crisp', 'separately', 'broth—or', 'drag', 'fried', 'twelve', 'personal', 'stage', 'vailable', 'else']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokens = tokenizer(' '.join(inst_list))\n",
    "print(len(inst_list))\n",
    "# print(inst_list[:30])\n",
    "print(len(tokens))\n",
    "print(tokens[:30])\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokens = tokenizer(' '.join(inst_vocab))\n",
    "print(len(inst_vocab))\n",
    "# print(list(inst_vocab)[:30])\n",
    "print(len(tokens))\n",
    "print(tokens[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Instructions tokens 247\n",
      "Max Ingredients tokens 110\n"
     ]
    }
   ],
   "source": [
    "# Basic\n",
    "# tokenizer = get_tokenizer(\"basic_english\")\n",
    "# # from torchtext.transforms import CLIPTokenizer\n",
    "# # MERGES_FILE = \"http://download.pytorch.org/models/text/clip_merges.bpe\"\n",
    "# # ENCODER_FILE = \"http://download.pytorch.org/models/text/clip_encoder.json\"\n",
    "# # tokenizer = CLIPTokenizer(merges_path=MERGES_FILE, encoder_json_path=ENCODER_FILE)\n",
    "\n",
    "# title_words = tokenizer(' '.join(title_list))\n",
    "# inst_words = tokenizer(' '.join(inst_list))\n",
    "# ingr_words = tokenizer(' '.join(ingr_list))\n",
    "# all_words = title_words + inst_words + ingr_words\n",
    "# print(\"All words\", len(all_words))\n",
    "\n",
    "# counter = Counter(all_words)\n",
    "# sorted_by_freq_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "# ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "# total_vocab = vocab(counter)\n",
    "# print(\"Vocab size:\", len(total_vocab))\n",
    "\n",
    "def get_max_len(list_object):\n",
    "    max_len = 0\n",
    "    # Tokenize each sentence\n",
    "    for sentence in list_object:\n",
    "        tokens = tokenizer(sentence)\n",
    "        if len(tokens)>max_len:\n",
    "            max_len = len(tokens)\n",
    "\n",
    "    return max_len\n",
    "\n",
    "inst_max = df[\"Instructions\"].apply(lambda x: get_max_len(x)).max()\n",
    "ingr_max = df[\"Cleaned_Ingredients\"].apply(lambda x: get_max_len(x)).max()\n",
    "\n",
    "print(\"Max Instructions tokens\", inst_max)\n",
    "print(\"Max Ingredients tokens\", ingr_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# import requests\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# if len(sys.argv) != 2:\n",
    "#     print('You must enter the model name as a parameter, e.g.: download_model.py 124M')\n",
    "#     sys.exit(1)\n",
    "\n",
    "# model = sys.argv[1]\n",
    "\n",
    "# subdir = os.path.join('models', model)\n",
    "# if not os.path.exists(subdir):\n",
    "#     os.makedirs(subdir)\n",
    "# subdir = subdir.replace('\\\\','/') # needed for Windows\n",
    "\n",
    "# for filename in ['checkpoint','encoder.json','hparams.json','model.ckpt.data-00000-of-00001', 'model.ckpt.index', 'model.ckpt.meta', 'vocab.bpe']:\n",
    "\n",
    "#     r = requests.get(\"https://openaipublic.blob.core.windows.net/gpt-2/\" + subdir + \"/\" + filename, stream=True)\n",
    "\n",
    "#     with open(os.path.join(subdir, filename), 'wb') as f:\n",
    "#         file_size = int(r.headers[\"content-length\"])\n",
    "#         chunk_size = 1000\n",
    "#         with tqdm(ncols=100, desc=\"Fetching \" + filename, total=file_size, unit_scale=True) as pbar:\n",
    "#             # 1k for chunk_size, since Ethernet packet size is around 1500 bytes\n",
    "#             for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "#                 f.write(chunk)\n",
    "#                 pbar.update(chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All words 3685814\n",
      "Vocab size: 23945\n",
      "Max Instructions tokens 247\n",
      "Max Ingredients tokens 110\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "# from torchtext.transforms import CLIPTokenizer, CharBPETokenizer, GPT2BPETokenizer\n",
    "# MERGES_FILE = \"http://download.pytorch.org/models/text/clip_merges.bpe\"\n",
    "# ENCODER_FILE = \"http://download.pytorch.org/models/text/clip_encoder.json\"\n",
    "# tokenizer = CLIPTokenizer(merges_path=MERGES_FILE, encoder_json_path=ENCODER_FILE, num_merges=50)\n",
    "# tokenizer = CharBPETokenizer(ENCODER_FILE,MERGES_FILE)\n",
    "# tokenizer = GPT2BPETokenizer(\"models/gpt_2/encoder.json\",\n",
    "#                              \"models/gpt_2/vocab.bpe\")\n",
    "\n",
    "title_words = tokenizer(' '.join(title_list))\n",
    "inst_words = tokenizer(' '.join(inst_list))\n",
    "ingr_words = tokenizer(' '.join(ingr_list))\n",
    "all_words = title_words + inst_words + ingr_words\n",
    "print(\"All words\", len(all_words))\n",
    "\n",
    "counter = Counter(all_words)\n",
    "sorted_by_freq_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "total_vocab = Vocab(counter)\n",
    "print(\"Vocab size:\", len(total_vocab))\n",
    "\n",
    "def get_max_len(list_object):\n",
    "    max_len = 0\n",
    "    # Tokenize each sentence\n",
    "    for sentence in list_object:\n",
    "        tokens = tokenizer(sentence)\n",
    "        if len(tokens)>max_len:\n",
    "            max_len = len(tokens)\n",
    "\n",
    "    return max_len\n",
    "\n",
    "inst_max = df[\"Instructions\"].apply(lambda x: get_max_len(x)).max()\n",
    "ingr_max = df[\"Cleaned_Ingredients\"].apply(lambda x: get_max_len(x)).max()\n",
    "\n",
    "print(\"Max Instructions tokens\", inst_max)\n",
    "print(\"Max Ingredients tokens\", ingr_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pat', 'chicken', 'dry', 'with', 'paper', 'towels', 'season', 'all', 'over', 'with', '2', 'tsp', '.', 'salt', 'and', 'tie', 'legs', 'together', 'with', 'kitchen', 'twine', '.', 'let', 'sit', 'at', 'room', 'temperature', '1', 'hour', '.', 'meanwhile', 'halve', 'squash', 'and', 'scoop', 'out', 'seeds', '.', 'run', 'a', 'vegetable', 'peeler', 'along', 'ridges', 'of', 'squash', 'halves', 'to', 'remove', 'skin', '.', 'cut', 'each', 'half', 'into', '½-thick', 'wedges', 'arrange', 'on', 'a', 'rimmed', 'baking', 'sheet', '.', 'combine', 'sage', 'rosemary', 'and', '6', 'tbsp']\n",
      "['Pat', 'chicken', 'dry', 'with', 'paper', 'towels', 'season', 'all', 'over', 'with', '2', 'tsp', '.', 'salt', 'and', 'tie', 'legs', 'together', 'with', 'kitchen', 'twine', '.', 'Let', 'sit', 'at', 'room', 'temperature', '1', 'hour', '.', 'Meanwhile', 'halve', 'squash', 'and', 'scoop', 'out', 'seeds', '.', 'Run', 'a', 'vegetable', 'peeler', 'along', 'ridges', 'of', 'squash', 'halves', 'to', 'remove', 'skin', '.', 'Cut', 'each', 'half', 'into', '½-thick', 'wedges;', 'arrange', 'on', 'a', 'rimmed', 'baking', 'sheet', '.', 'Combine', 'sage', 'rosemary', 'and', '6', 'Tbsp']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(' '.join(inst_list))[:70])\n",
    "print(inst_list[:70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "# tokenizer.pad_token = 100\n",
    "# process = TensorDictTokenizer(tokenizer, max_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_value(x, val):\n",
    "    if x.shape[1]>val:\n",
    "        raise ValueError(f\"{x.shape[1]}>{val}\")\n",
    "    diff = val - x.shape[-1]\n",
    "    # top, bottom\n",
    "    pad = (0, diff)\n",
    "    out = torch.nn.functional.pad(x, pad, mode='constant', value=0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using device: cuda\n",
      "Input shapes:\n",
      "torch.Size([2, 247])\n",
      "torch.Size([2, 5, 247])\n",
      "torch.Size([2, 7, 247])\n"
     ]
    }
   ],
   "source": [
    "# Test recipe encoder:\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"You are using device: %s\" % device)\n",
    "\n",
    "\n",
    "## Dummy data between 0 and 10\n",
    "# Size: batch=2, title_len = 8, ingr_len=15, instr_len=30\n",
    "src_list = [\n",
    "            # title\n",
    "            np.random.rand(2, 8) * 10,\n",
    "            # ingredients with 5 lines\n",
    "            np.random.rand(2, 5, 15) * 10,\n",
    "             # instructions with 7 lines\n",
    "            np.random.rand(2, 7, 15) * 10\n",
    "            ]\n",
    "# Convert to torch and pad to max length\n",
    "x = [ pad_to_value(torch.tensor(t, dtype=torch.long),total_max) for t in src_list]\n",
    "print(\"Input shapes:\")\n",
    "print(x[0].shape)\n",
    "print(x[1].shape)\n",
    "print(x[2].shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[ 0.0808,  0.4999,  1.3792,  ...,  0.2700,  0.1253, -1.1917],\n",
      "         [ 0.5416,  0.3807,  0.9894,  ...,  1.1628,  0.0718, -0.3322],\n",
      "         [ 0.4275, -0.3802,  1.4338,  ..., -0.4330, -0.0523,  0.2777],\n",
      "         ...,\n",
      "         [ 0.2969,  1.2286, -0.9710,  ...,  1.3662,  1.8854,  0.7023],\n",
      "         [-0.9929,  1.0657, -1.0301,  ...,  0.4181,  1.0254,  0.3174],\n",
      "         [-0.3701,  1.7279, -0.2846,  ...,  1.6393,  2.0879, -0.0570]],\n",
      "\n",
      "        [[ 0.2215,  0.5239,  2.2554,  ..., -0.0745, -0.7323, -0.7702],\n",
      "         [ 0.2834,  0.7428,  0.5610,  ...,  0.9669, -0.0091, -1.0082],\n",
      "         [-0.1378,  1.0216,  0.1937,  ...,  0.6632,  0.1825, -0.7075],\n",
      "         ...,\n",
      "         [ 0.1702,  1.2358, -0.6299,  ...,  0.8203,  1.6653,  0.8110],\n",
      "         [-0.2664,  1.2936, -1.0354,  ...,  1.5330,  1.5045,  0.5183],\n",
      "         [-0.4329,  1.6971, -0.5873,  ...,  1.2488,  2.0607,  0.0758]]],\n",
      "       device='cuda:0', grad_fn=<CatBackward0>), tensor([[-0.2227,  0.0957, -0.4042,  ..., -0.4722, -0.2344, -0.6170],\n",
      "        [-0.2204,  0.0937, -0.3905,  ..., -0.4922, -0.2362, -0.5861]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>))\n"
     ]
    }
   ],
   "source": [
    "recipe_encoder = recipe.RecipeEncoder(device=\"cuda\", vocab_size=total_vocab_size,\n",
    "                                      max_len=total_max)\n",
    "outputs = recipe_encoder.forward(x)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfood_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
