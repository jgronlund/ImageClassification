{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13496\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Title</th>\n",
       "      <th>Instructions</th>\n",
       "      <th>Image_Name</th>\n",
       "      <th>Cleaned_Ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Miso-Butter Roast Chicken With Acorn Squash Pa...</td>\n",
       "      <td>[Pat chicken dry with paper towels, season all...</td>\n",
       "      <td>miso-butter-roast-chicken-acorn-squash-panzanella</td>\n",
       "      <td>[1 (3½–4-lb.) whole chicken, 2¾ tsp. kosher sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Crispy Salt and Pepper Potatoes</td>\n",
       "      <td>[Preheat oven to 400°F and line a rimmed bakin...</td>\n",
       "      <td>crispy-salt-and-pepper-potatoes-dan-kluger</td>\n",
       "      <td>[2 large egg whites, 1 pound new potatoes (abo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Thanksgiving Mac and Cheese</td>\n",
       "      <td>[Place a rack in middle of oven; preheat to 40...</td>\n",
       "      <td>thanksgiving-mac-and-cheese-erick-williams</td>\n",
       "      <td>[1 cup evaporated milk, 1 cup whole milk, 1 ts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Italian Sausage and Bread Stuffing</td>\n",
       "      <td>[Preheat oven to 350°F with rack in middle. Ge...</td>\n",
       "      <td>italian-sausage-and-bread-stuffing-240559</td>\n",
       "      <td>[1 (¾- to 1-pound) round Italian loaf, cut int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Newton's Law</td>\n",
       "      <td>[Stir together brown sugar and hot water in a ...</td>\n",
       "      <td>newtons-law-apple-bourbon-cocktail</td>\n",
       "      <td>[1 teaspoon dark brown sugar, 1 teaspoon hot w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              Title  \\\n",
       "0           0  Miso-Butter Roast Chicken With Acorn Squash Pa...   \n",
       "1           1                    Crispy Salt and Pepper Potatoes   \n",
       "2           2                        Thanksgiving Mac and Cheese   \n",
       "3           3                 Italian Sausage and Bread Stuffing   \n",
       "4           4                                       Newton's Law   \n",
       "\n",
       "                                        Instructions  \\\n",
       "0  [Pat chicken dry with paper towels, season all...   \n",
       "1  [Preheat oven to 400°F and line a rimmed bakin...   \n",
       "2  [Place a rack in middle of oven; preheat to 40...   \n",
       "3  [Preheat oven to 350°F with rack in middle. Ge...   \n",
       "4  [Stir together brown sugar and hot water in a ...   \n",
       "\n",
       "                                          Image_Name  \\\n",
       "0  miso-butter-roast-chicken-acorn-squash-panzanella   \n",
       "1         crispy-salt-and-pepper-potatoes-dan-kluger   \n",
       "2         thanksgiving-mac-and-cheese-erick-williams   \n",
       "3          italian-sausage-and-bread-stuffing-240559   \n",
       "4                 newtons-law-apple-bourbon-cocktail   \n",
       "\n",
       "                                 Cleaned_Ingredients  \n",
       "0  [1 (3½–4-lb.) whole chicken, 2¾ tsp. kosher sa...  \n",
       "1  [2 large egg whites, 1 pound new potatoes (abo...  \n",
       "2  [1 cup evaporated milk, 1 cup whole milk, 1 ts...  \n",
       "3  [1 (¾- to 1-pound) round Italian loaf, cut int...  \n",
       "4  [1 teaspoon dark brown sugar, 1 teaspoon hot w...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "import torch\n",
    "import models.recipe_encoder as recipe\n",
    "import torchtext; torchtext.disable_torchtext_deprecation_warning()\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import vocab\n",
    "from collections import Counter, OrderedDict\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "csv_file = 'Data/updated_data_with_lists.csv'\n",
    "df = pd.read_csv(csv_file, converters={\"Cleaned_Ingredients\": literal_eval, \"Instructions\": literal_eval})\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column length 13496\n",
      "['1', '(', '3½–4-lb', '.', ')', 'whole', 'chicken', '2¾', 'tsp', '.']\n",
      "['1', '(', '3½–4-lb', '.', ')', 'whole', 'chicken', '2¾', 'tsp', '.', 'kosher', 'salt', ',', 'divided', ',', 'plus', 'more', '2', 'small', 'acorn', 'squash', '(', 'about', '3', 'lb', '.', 'total', ')', '2', 'Tbsp', '.', 'finely', 'chopped', 'sage', '1', 'Tbsp', '.', 'finely', 'chopped', 'rosemary', '6', 'Tbsp', '.', 'unsalted', 'butter', ',', 'melted', ',', 'plus', '3', 'Tbsp', '.', 'room', 'temperature', '¼', 'tsp', '.', 'ground', 'allspice', 'Pinch', 'of', 'crushed', 'red', 'pepper', 'flakes', 'Freshly', 'ground', 'black', 'pepper', '⅓', 'loaf', 'good-quality', 'sturdy', 'white', 'bread', ',', 'torn', 'into', '1', 'pieces', '(', 'about', '2½', 'cups', ')', '2', 'medium', 'apples', '(', 'such', 'as', 'Gala', 'or', 'Pink', 'Lady;', 'about', '14', 'oz', '.', 'total', ')', ',', 'cored', ',', 'cut', 'into', '1', 'pieces', '2', 'Tbsp', '.', 'extra-virgin', 'olive', 'oil', '½', 'small', 'red', 'onion', ',', 'thinly', 'sliced', '3', 'Tbsp', '.', 'apple', 'cider', 'vinegar', '1', 'Tbsp', '.', 'white', 'miso', '¼', 'cup', 'all-purpose', 'flour', '2', 'Tbsp', '.', 'unsalted', 'butter', ',', 'room', 'temperature', '¼', 'cup', 'dry', 'white', 'wine', '2', 'cups', 'unsalted', 'chicken', 'broth', '2', 'tsp', '.', 'white', 'miso', 'Kosher', 'salt', 'freshly', 'ground', 'pepper']\n",
      "Len Elements: 13496\n",
      "Size: 12676\n",
      "Max: 110\n"
     ]
    }
   ],
   "source": [
    "column = df[\"Cleaned_Ingredients\"]\n",
    "\n",
    "ingr_list = []\n",
    "element_list = []\n",
    "ingr_max = 0\n",
    "print(\"Column length\", len(column))\n",
    "for w_list in column:\n",
    "    tmp =[]\n",
    "    for str_list in w_list:\n",
    "        # for word in str_list:\n",
    "        formatted = str_list.replace('.', ' . ').replace(',',' , ').replace('(',' ( ').replace(')',' ) ').replace('\"','').split()\n",
    "        ingr_list.extend(formatted)\n",
    "        new_len = len(formatted)\n",
    "        tmp.extend(formatted)\n",
    "        if new_len>ingr_max:\n",
    "            ingr_max=new_len\n",
    "    element_list.append(tmp)\n",
    "\n",
    "print(ingr_list[:10])\n",
    "print(element_list[0])\n",
    "print(\"Len Elements:\", len(element_list))\n",
    "ingr_vocab = set(ingr_list)\n",
    "ingr_vocab_size = len(ingr_vocab)\n",
    "print(\"Size:\", ingr_vocab_size)\n",
    "print(\"Max:\", ingr_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1 (3½–4-lb.) whole chicken',\n",
       " '2¾ tsp. kosher salt, divided, plus more',\n",
       " '2 small acorn squash (about 3 lb. total)',\n",
       " '2 Tbsp. finely chopped sage',\n",
       " '1 Tbsp. finely chopped rosemary',\n",
       " '6 Tbsp. unsalted butter, melted, plus 3 Tbsp. room temperature',\n",
       " '¼ tsp. ground allspice',\n",
       " 'Pinch of crushed red pepper flakes',\n",
       " 'Freshly ground black pepper',\n",
       " '⅓ loaf good-quality sturdy white bread, torn into 1\" pieces (about 2½ cups)',\n",
       " '2 medium apples (such as Gala or Pink Lady; about 14 oz. total), cored, cut into 1\" pieces',\n",
       " '2 Tbsp. extra-virgin olive oil',\n",
       " '½ small red onion, thinly sliced',\n",
       " '3 Tbsp. apple cider vinegar',\n",
       " '1 Tbsp. white miso',\n",
       " '¼ cup all-purpose flour',\n",
       " '2 Tbsp. unsalted butter, room temperature',\n",
       " '¼ cup dry white wine',\n",
       " '2 cups unsalted chicken broth',\n",
       " '2 tsp. white miso',\n",
       " 'Kosher salt',\n",
       " 'freshly ground pepper']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Cleaned_Ingredients\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pat', 'chicken', 'dry', 'with', 'paper', 'towels', 'season', 'all', 'over', 'with']\n",
      "Size: 21164\n",
      "Max: 247\n"
     ]
    }
   ],
   "source": [
    "column = df[\"Instructions\"]\n",
    "\n",
    "inst_list = []\n",
    "inst_max = 0\n",
    "for w_list in column:\n",
    "    for str_list in w_list:\n",
    "        # for word in str_list:\n",
    "        formatted = str_list.replace('.', ' . ').replace(',',' , ').replace('(','').replace(')','').replace('\"','').split()\n",
    "        inst_list.extend(formatted)\n",
    "        new_len = len(formatted)\n",
    "        if new_len>inst_max:\n",
    "            inst_max=new_len\n",
    "            \n",
    "print(inst_list[:10])\n",
    "inst_vocab = set(inst_list)\n",
    "inst_vocab_size = len(inst_vocab)\n",
    "print(\"Size:\",inst_vocab_size)\n",
    "print(\"Max:\", inst_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Miso-Butter', 'Roast', 'Chicken', 'With', 'Acorn', 'Squash', 'Panzanella', 'Crispy', 'Salt', 'and']\n",
      "Size: 6959\n",
      "Max: 21\n"
     ]
    }
   ],
   "source": [
    "column = df[\"Title\"].to_list()\n",
    "\n",
    "title_list = []\n",
    "title_max = 0\n",
    "for str_list in column:\n",
    "        # for word in str_list:\n",
    "        formatted = str_list.replace('.', ' . ').replace(',',' , ').replace('(','').replace(')','').replace('\"','').split()\n",
    "        title_list.extend(formatted)\n",
    "        new_len = len(formatted)\n",
    "        if new_len>title_max:\n",
    "            title_max=new_len\n",
    "\n",
    "print(title_list[:10])\n",
    "title_vocab = set(title_list)\n",
    "title_vocab_size = len(title_vocab)\n",
    "print(\"Size:\", title_vocab_size)\n",
    "print(\"Max:\", title_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 31551\n",
      "Max: 247\n"
     ]
    }
   ],
   "source": [
    "total_vocab = set(list(inst_vocab) + list(ingr_vocab) + list(title_vocab)) \n",
    "total_vocab_size = len(total_vocab)\n",
    "print(\"Size:\",total_vocab_size)\n",
    "total_max = max([inst_max, ingr_max, title_max])\n",
    "print(\"Max:\",total_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = {word: i for i, word in enumerate(total_vocab)}\n",
    "\n",
    "word_to_ix_ingr = {word: i for i, word in enumerate(ingr_vocab)}\n",
    "word_to_ix_inst = {word: i for i, word in enumerate(inst_vocab)}\n",
    "word_to_ix_title = {word: i for i, word in enumerate(title_vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchtext.vocab import vocab\n",
    "\n",
    "# Get longest sentence in Ingredients or Instructions for max_len\n",
    "# Use torchtext to make vocab: vocab_size = input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2644584\n",
      "2649226\n",
      "['pat', 'chicken', 'dry', 'with', 'paper', 'towels', 'season', 'all', 'over', 'with', '2', 'tsp', '.', 'salt', 'and', 'tie', 'legs', 'together', 'with', 'kitchen', 'twine', '.', 'let', 'sit', 'at', 'room', 'temperature', '1', 'hour', '.']\n",
      "21164\n",
      "21945\n",
      "['tender-firm', 'jiggles', '3/4”', 'reduced', 'cob', 'chicken’s', 'squeamish', 'crêpev', 'butter-flour', 'ways', 'aside—it', 'overhangs', 'indoors', 'currants', 'while', 'neck', 'evenly', 'convinced', 'pulled', 'vegetables', 'today', '50–65', '350°f–365°f', 'dozen', 'aguardiente', 'diameter', 'brilliantly', 'tomatoes', 'organized', 'pecan']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokens = tokenizer(' '.join(inst_list))\n",
    "print(len(inst_list))\n",
    "# print(inst_list[:30])\n",
    "print(len(tokens))\n",
    "print(tokens[:30])\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokens = tokenizer(' '.join(inst_vocab))\n",
    "print(len(inst_vocab))\n",
    "# print(list(inst_vocab)[:30])\n",
    "print(len(tokens))\n",
    "print(tokens[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Instructions tokens 247\n",
      "Max Ingredients tokens 110\n"
     ]
    }
   ],
   "source": [
    "# Basic\n",
    "# tokenizer = get_tokenizer(\"basic_english\")\n",
    "# # from torchtext.transforms import CLIPTokenizer\n",
    "# # MERGES_FILE = \"http://download.pytorch.org/models/text/clip_merges.bpe\"\n",
    "# # ENCODER_FILE = \"http://download.pytorch.org/models/text/clip_encoder.json\"\n",
    "# # tokenizer = CLIPTokenizer(merges_path=MERGES_FILE, encoder_json_path=ENCODER_FILE)\n",
    "\n",
    "# title_words = tokenizer(' '.join(title_list))\n",
    "# inst_words = tokenizer(' '.join(inst_list))\n",
    "# ingr_words = tokenizer(' '.join(ingr_list))\n",
    "# all_words = title_words + inst_words + ingr_words\n",
    "# print(\"All words\", len(all_words))\n",
    "\n",
    "# counter = Counter(all_words)\n",
    "# sorted_by_freq_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "# ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "# total_vocab = vocab(counter)\n",
    "# print(\"Vocab size:\", len(total_vocab))\n",
    "\n",
    "def get_max_len(list_object):\n",
    "    max_len = 0\n",
    "    # Tokenize each sentence\n",
    "    for sentence in list_object:\n",
    "        tokens = tokenizer(sentence)\n",
    "        if len(tokens)>max_len:\n",
    "            max_len = len(tokens)\n",
    "\n",
    "    return max_len\n",
    "\n",
    "inst_max = df[\"Instructions\"].apply(lambda x: get_max_len(x)).max()\n",
    "ingr_max = df[\"Cleaned_Ingredients\"].apply(lambda x: get_max_len(x)).max()\n",
    "\n",
    "print(\"Max Instructions tokens\", inst_max)\n",
    "print(\"Max Ingredients tokens\", ingr_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching checkpoint: 1.00kit [00:00, 2.17Mit/s]                                                     \n",
      "Fetching encoder.json: 1.00kit [00:00, 6.61Mit/s]                                                   \n",
      "Fetching hparams.json: 1.00kit [00:00, 8.49Mit/s]                                                   \n",
      "Fetching model.ckpt.data-00000-of-00001: 1.00kit [00:00, 9.06Mit/s]                                 \n",
      "Fetching model.ckpt.index: 1.00kit [00:00, 5.38Mit/s]                                               \n",
      "Fetching model.ckpt.meta: 1.00kit [00:00, 5.53Mit/s]                                                \n",
      "Fetching vocab.bpe: 1.00kit [00:00, 7.77Mit/s]                                                      \n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# import requests\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# if len(sys.argv) != 2:\n",
    "#     print('You must enter the model name as a parameter, e.g.: download_model.py 124M')\n",
    "#     sys.exit(1)\n",
    "\n",
    "# model = sys.argv[1]\n",
    "\n",
    "# subdir = os.path.join('models', model)\n",
    "# if not os.path.exists(subdir):\n",
    "#     os.makedirs(subdir)\n",
    "# subdir = subdir.replace('\\\\','/') # needed for Windows\n",
    "\n",
    "# for filename in ['checkpoint','encoder.json','hparams.json','model.ckpt.data-00000-of-00001', 'model.ckpt.index', 'model.ckpt.meta', 'vocab.bpe']:\n",
    "\n",
    "#     r = requests.get(\"https://openaipublic.blob.core.windows.net/gpt-2/\" + subdir + \"/\" + filename, stream=True)\n",
    "\n",
    "#     with open(os.path.join(subdir, filename), 'wb') as f:\n",
    "#         file_size = int(r.headers[\"content-length\"])\n",
    "#         chunk_size = 1000\n",
    "#         with tqdm(ncols=100, desc=\"Fetching \" + filename, total=file_size, unit_scale=True) as pbar:\n",
    "#             # 1k for chunk_size, since Ethernet packet size is around 1500 bytes\n",
    "#             for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "#                 f.write(chunk)\n",
    "#                 pbar.update(chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Unexpected UTF-8 BOM (decode using utf-8-sig): line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CLIPTokenizer, CharBPETokenizer, GPT2BPETokenizer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# MERGES_FILE = \"http://download.pytorch.org/models/text/clip_merges.bpe\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# ENCODER_FILE = \"http://download.pytorch.org/models/text/clip_encoder.json\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# tokenizer = CLIPTokenizer(merges_path=MERGES_FILE, encoder_json_path=ENCODER_FILE, num_merges=50)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# tokenizer = CharBPETokenizer(ENCODER_FILE,MERGES_FILE)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m GPT2BPETokenizer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/gpt_2/encoder.json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m                              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/gpt_2/vocab.bpe\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m title_words \u001b[38;5;241m=\u001b[39m tokenizer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(title_list))\n\u001b[1;32m     11\u001b[0m inst_words \u001b[38;5;241m=\u001b[39m tokenizer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(inst_list))\n",
      "File \u001b[0;32m~/anaconda3/envs/tfood_torch2.2/lib/python3.12/site-packages/torchtext/transforms.py:317\u001b[0m, in \u001b[0;36mGPT2BPETokenizer.__init__\u001b[0;34m(self, encoder_json_path, vocab_bpe_path, return_tokens)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;66;03m# load bpe encoder and bpe decoder\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(get_asset_local_path(encoder_json_path), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 317\u001b[0m     bpe_encoder \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;66;03m# load bpe vocab\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(get_asset_local_path(vocab_bpe_path), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/anaconda3/envs/tfood_torch2.2/lib/python3.12/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(fp\u001b[38;5;241m.\u001b[39mread(),\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, object_hook\u001b[38;5;241m=\u001b[39mobject_hook,\n\u001b[1;32m    295\u001b[0m         parse_float\u001b[38;5;241m=\u001b[39mparse_float, parse_int\u001b[38;5;241m=\u001b[39mparse_int,\n\u001b[1;32m    296\u001b[0m         parse_constant\u001b[38;5;241m=\u001b[39mparse_constant, object_pairs_hook\u001b[38;5;241m=\u001b[39mobject_pairs_hook, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[0;32m~/anaconda3/envs/tfood_torch2.2/lib/python3.12/json/__init__.py:335\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(s, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m s\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\ufeff\u001b[39;00m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 335\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected UTF-8 BOM (decode using utf-8-sig)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    336\u001b[0m                               s, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(s, (\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mbytearray\u001b[39m)):\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Unexpected UTF-8 BOM (decode using utf-8-sig): line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "# tokenizer = get_tokenizer(\"basic_english\")\n",
    "from torchtext.transforms import CLIPTokenizer, CharBPETokenizer, GPT2BPETokenizer\n",
    "# MERGES_FILE = \"http://download.pytorch.org/models/text/clip_merges.bpe\"\n",
    "# ENCODER_FILE = \"http://download.pytorch.org/models/text/clip_encoder.json\"\n",
    "# tokenizer = CLIPTokenizer(merges_path=MERGES_FILE, encoder_json_path=ENCODER_FILE, num_merges=50)\n",
    "# tokenizer = CharBPETokenizer(ENCODER_FILE,MERGES_FILE)\n",
    "tokenizer = GPT2BPETokenizer(\"models/gpt_2/encoder.json\",\n",
    "                             \"models/gpt_2/vocab.bpe\")\n",
    "\n",
    "title_words = tokenizer(' '.join(title_list))\n",
    "inst_words = tokenizer(' '.join(inst_list))\n",
    "ingr_words = tokenizer(' '.join(ingr_list))\n",
    "all_words = title_words + inst_words + ingr_words\n",
    "print(\"All words\", len(all_words))\n",
    "\n",
    "counter = Counter(all_words)\n",
    "sorted_by_freq_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "total_vocab = vocab(counter)\n",
    "print(\"Vocab size:\", len(total_vocab))\n",
    "\n",
    "def get_max_len(list_object):\n",
    "    max_len = 0\n",
    "    # Tokenize each sentence\n",
    "    for sentence in list_object:\n",
    "        tokens = tokenizer(sentence)\n",
    "        if len(tokens)>max_len:\n",
    "            max_len = len(tokens)\n",
    "\n",
    "    return max_len\n",
    "\n",
    "inst_max = df[\"Instructions\"].apply(lambda x: get_max_len(x)).max()\n",
    "ingr_max = df[\"Cleaned_Ingredients\"].apply(lambda x: get_max_len(x)).max()\n",
    "\n",
    "print(\"Max Instructions tokens\", inst_max)\n",
    "print(\"Max Ingredients tokens\", ingr_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pat', 'chicken', 'dry', 'with', 'paper', 'towels', 'season', 'all', 'over', 'with', '2', 'tsp', '.', 'salt', 'and', 'tie', 'legs', 'together', 'with', 'kitchen', 'twine', '.', 'let', 'sit', 'at', 'room', 'temperature', '1', 'hour', '.', 'meanwhile', 'halve', 'squash', 'and', 'scoop', 'out', 'seeds', '.', 'run', 'a', 'vegetable', 'peeler', 'along', 'ridges', 'of', 'squash', 'halves', 'to', 'remove', 'skin', '.', 'cut', 'each', 'half', 'into', '½-thick', 'wedges', 'arrange', 'on', 'a', 'rimmed', 'baking', 'sheet', '.', 'combine', 'sage', 'rosemary', 'and', '6', 'tbsp']\n",
      "['Pat', 'chicken', 'dry', 'with', 'paper', 'towels', 'season', 'all', 'over', 'with', '2', 'tsp', '.', 'salt', 'and', 'tie', 'legs', 'together', 'with', 'kitchen', 'twine', '.', 'Let', 'sit', 'at', 'room', 'temperature', '1', 'hour', '.', 'Meanwhile', 'halve', 'squash', 'and', 'scoop', 'out', 'seeds', '.', 'Run', 'a', 'vegetable', 'peeler', 'along', 'ridges', 'of', 'squash', 'halves', 'to', 'remove', 'skin', '.', 'Cut', 'each', 'half', 'into', '½-thick', 'wedges;', 'arrange', 'on', 'a', 'rimmed', 'baking', 'sheet', '.', 'Combine', 'sage', 'rosemary', 'and', '6', 'Tbsp']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(' '.join(inst_list))[:70])\n",
    "print(inst_list[:70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "# tokenizer.pad_token = 100\n",
    "# process = TensorDictTokenizer(tokenizer, max_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_value(x, val):\n",
    "    if x.shape[1]>val:\n",
    "        raise ValueError(f\"{x.shape[1]}>{val}\")\n",
    "    diff = val - x.shape[-1]\n",
    "    # top, bottom\n",
    "    pad = (0, diff)\n",
    "    out = torch.nn.functional.pad(x, pad, mode='constant', value=0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using device: cuda\n",
      "Input shapes:\n",
      "torch.Size([2, 247])\n",
      "torch.Size([2, 5, 247])\n",
      "torch.Size([2, 7, 247])\n"
     ]
    }
   ],
   "source": [
    "# Test recipe encoder:\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"You are using device: %s\" % device)\n",
    "\n",
    "\n",
    "## Dummy data between 0 and 10\n",
    "# Size: batch=2, title_len = 8, ingr_len=15, instr_len=30\n",
    "src_list = [\n",
    "            # title\n",
    "            np.random.rand(2, 8) * 10,\n",
    "            # ingredients with 5 lines\n",
    "            np.random.rand(2, 5, 15) * 10,\n",
    "             # instructions with 7 lines\n",
    "            np.random.rand(2, 7, 15) * 10\n",
    "            ]\n",
    "# Convert to torch and pad to max length\n",
    "x = [ pad_to_value(torch.tensor(t, dtype=torch.long),total_max) for t in src_list]\n",
    "print(\"Input shapes:\")\n",
    "print(x[0].shape)\n",
    "print(x[1].shape)\n",
    "print(x[2].shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[-0.7389, -0.4791, -0.2176,  ..., -1.4426, -1.9327,  0.8481],\n",
      "         [-1.5415, -0.0725,  1.1229,  ...,  0.6512, -0.6983, -1.0268],\n",
      "         [-1.8480, -0.6308,  1.4833,  ..., -0.1999, -3.4345,  0.3573],\n",
      "         ...,\n",
      "         [-0.3580, -0.0849, -0.2160,  ..., -0.1603,  0.0300, -0.5300],\n",
      "         [-0.6370, -0.5890,  0.0827,  ..., -0.2887, -0.7205,  0.3073],\n",
      "         [-0.4339, -0.6543,  0.4993,  ...,  0.1014,  0.5026, -0.8463]],\n",
      "\n",
      "        [[-0.3258, -0.2897,  0.9680,  ..., -0.4618, -2.2903,  0.3054],\n",
      "         [-1.0009, -0.5142,  1.4745,  ...,  0.3490, -1.4791, -0.8766],\n",
      "         [-1.1975, -0.2382,  1.0415,  ..., -0.5430, -1.4473,  0.0623],\n",
      "         ...,\n",
      "         [-0.6524, -0.1814, -0.2320,  ...,  0.2363, -0.1515,  0.2945],\n",
      "         [-0.6666, -0.5962, -0.2536,  ...,  0.0085, -0.4219,  0.6886],\n",
      "         [-0.5659, -1.0790,  0.3276,  ...,  0.4588,  0.4884,  0.0375]]],\n",
      "       device='cuda:0', grad_fn=<CatBackward0>), tensor([[-0.6028,  1.1241, -0.8230,  ..., -0.3846,  0.4443, -0.2226],\n",
      "        [-0.6151,  1.1422, -0.8040,  ..., -0.4303,  0.4586, -0.2441]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>))\n"
     ]
    }
   ],
   "source": [
    "recipe_encoder = recipe.RecipeEncoder(device=\"cuda\", vocab_size=total_vocab_size,\n",
    "                                      max_len=total_max)\n",
    "outputs = recipe_encoder.forward(x)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfood_torch2.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
