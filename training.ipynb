{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSBW6ryo-H9v"
   },
   "source": [
    "If using google drive please edit this line to connect to drive location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 183,
     "status": "ok",
     "timestamp": 1733633395176,
     "user": {
      "displayName": "Emily Thomas",
      "userId": "07658032212383838916"
     },
     "user_tz": 480
    },
    "id": "w6ZQ_d3W-aQq"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 944,
     "status": "ok",
     "timestamp": 1733633396238,
     "user": {
      "displayName": "Emily Thomas",
      "userId": "07658032212383838916"
     },
     "user_tz": 480
    },
    "id": "1YCGTlkW99pN",
    "outputId": "b45349e6-3d22-4452-8ee1-43ddf703acaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
      " Code\t\t\t   'Project Task List.gsheet'\t  training.ipynb\n",
      " Data\t\t\t    __pycache__\t\t\t  training_jaketesting.ipynb\n",
      "'Image Recipe.gdoc'\t    recipe_encoder.py\t\t  training_optuna.ipynb\n",
      " Main.ipynb\t\t    runner_bk.py\t\t  training_with_tokens.ipynb\n",
      " models\t\t\t    runner.py\n",
      " pictures_for_report.pptx   training_12_2_version.ipynb\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import os, sys\n",
    "\n",
    "# Mount the google colab\n",
    "drive.mount(\"/content/drive/\")\n",
    "GOOGLE_DRIVE_PATH_POST_MYDRIVE = 'DeepLearning_GroupProject/'\n",
    "GOOGLE_DRIVE_PATH = os.path.join('/content', 'drive', 'MyDrive', GOOGLE_DRIVE_PATH_POST_MYDRIVE)\n",
    "!ls {GOOGLE_DRIVE_PATH}\n",
    "sys.path.append(GOOGLE_DRIVE_PATH)\n",
    "\n",
    "# GOOGLE_DRIVE_PATH = '.'\n",
    "\n",
    "\n",
    "# relative paths\n",
    "models_dir = 'models'\n",
    "csv_path = 'Data/updated_data_with_lists.csv'\n",
    "tensors_dir = 'Data/tensor_batch_notaugmented'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "executionInfo": {
     "elapsed": 8376,
     "status": "ok",
     "timestamp": 1733633404612,
     "user": {
      "displayName": "Emily Thomas",
      "userId": "07658032212383838916"
     },
     "user_tz": 480
    },
    "id": "sGHSvkJG5fbt",
    "outputId": "c3be67d3-0826-4ed6-9539-c710e871e47d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13496\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 13496,\n  \"fields\": [\n    {\n      \"column\": \"Unnamed: 0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3896,\n        \"min\": 0,\n        \"max\": 13500,\n        \"num_unique_values\": 13496,\n        \"samples\": [\n          6173,\n          833,\n          3289\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 13305,\n        \"samples\": [\n          \"Whole Grilled Fish with Lime\",\n          \"Espresso Chocolate Sable\\u0301s\",\n          \"Turkey Shawarma With Crunchy Vegetables\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Instructions\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Image_Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 13467,\n        \"samples\": [\n          \"asparagus-green-onion-cucumber-and-herb-salad-241637\",\n          \"mushroom-and-fontina-quiche-355191\",\n          \"grill-roasted-pineapple\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Cleaned_Ingredients\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-5759533b-14a8-43a5-a1c8-b729e7402b96\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Title</th>\n",
       "      <th>Instructions</th>\n",
       "      <th>Image_Name</th>\n",
       "      <th>Cleaned_Ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Miso-Butter Roast Chicken With Acorn Squash Pa...</td>\n",
       "      <td>[Pat chicken dry with paper towels, season all...</td>\n",
       "      <td>miso-butter-roast-chicken-acorn-squash-panzanella</td>\n",
       "      <td>[1 (3½–4-lb.) whole chicken, 2¾ tsp. kosher sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Crispy Salt and Pepper Potatoes</td>\n",
       "      <td>[Preheat oven to 400°F and line a rimmed bakin...</td>\n",
       "      <td>crispy-salt-and-pepper-potatoes-dan-kluger</td>\n",
       "      <td>[2 large egg whites, 1 pound new potatoes (abo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Thanksgiving Mac and Cheese</td>\n",
       "      <td>[Place a rack in middle of oven; preheat to 40...</td>\n",
       "      <td>thanksgiving-mac-and-cheese-erick-williams</td>\n",
       "      <td>[1 cup evaporated milk, 1 cup whole milk, 1 ts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Italian Sausage and Bread Stuffing</td>\n",
       "      <td>[Preheat oven to 350°F with rack in middle. Ge...</td>\n",
       "      <td>italian-sausage-and-bread-stuffing-240559</td>\n",
       "      <td>[1 (¾- to 1-pound) round Italian loaf, cut int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Newton's Law</td>\n",
       "      <td>[Stir together brown sugar and hot water in a ...</td>\n",
       "      <td>newtons-law-apple-bourbon-cocktail</td>\n",
       "      <td>[1 teaspoon dark brown sugar, 1 teaspoon hot w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5759533b-14a8-43a5-a1c8-b729e7402b96')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-5759533b-14a8-43a5-a1c8-b729e7402b96 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-5759533b-14a8-43a5-a1c8-b729e7402b96');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-3407879c-930f-4fb3-ad40-3b54938cd680\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3407879c-930f-4fb3-ad40-3b54938cd680')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-3407879c-930f-4fb3-ad40-3b54938cd680 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "   Unnamed: 0                                              Title  \\\n",
       "0           0  Miso-Butter Roast Chicken With Acorn Squash Pa...   \n",
       "1           1                    Crispy Salt and Pepper Potatoes   \n",
       "2           2                        Thanksgiving Mac and Cheese   \n",
       "3           3                 Italian Sausage and Bread Stuffing   \n",
       "4           4                                       Newton's Law   \n",
       "\n",
       "                                        Instructions  \\\n",
       "0  [Pat chicken dry with paper towels, season all...   \n",
       "1  [Preheat oven to 400°F and line a rimmed bakin...   \n",
       "2  [Place a rack in middle of oven; preheat to 40...   \n",
       "3  [Preheat oven to 350°F with rack in middle. Ge...   \n",
       "4  [Stir together brown sugar and hot water in a ...   \n",
       "\n",
       "                                          Image_Name  \\\n",
       "0  miso-butter-roast-chicken-acorn-squash-panzanella   \n",
       "1         crispy-salt-and-pepper-potatoes-dan-kluger   \n",
       "2         thanksgiving-mac-and-cheese-erick-williams   \n",
       "3          italian-sausage-and-bread-stuffing-240559   \n",
       "4                 newtons-law-apple-bourbon-cocktail   \n",
       "\n",
       "                                 Cleaned_Ingredients  \n",
       "0  [1 (3½–4-lb.) whole chicken, 2¾ tsp. kosher sa...  \n",
       "1  [2 large egg whites, 1 pound new potatoes (abo...  \n",
       "2  [1 cup evaporated milk, 1 cup whole milk, 1 ts...  \n",
       "3  [1 (¾- to 1-pound) round Italian loaf, cut int...  \n",
       "4  [1 teaspoon dark brown sugar, 1 teaspoon hot w...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Explicitly adding models to the search path\n",
    "models_path = os.path.join(GOOGLE_DRIVE_PATH, models_dir)\n",
    "if models_path not in sys.path:\n",
    "    sys.path.insert(0, models_path)\n",
    "\n",
    "from models import recipe_encoder\n",
    "\n",
    "csv_file = os.path.join(GOOGLE_DRIVE_PATH,csv_path)\n",
    "df = pd.read_csv(csv_file, converters={\"Cleaned_Ingredients\": literal_eval, \"Instructions\": literal_eval})\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zhh--PCsDH3b"
   },
   "source": [
    "Concatenate the batches of preprocessed images into 1 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20302,
     "status": "ok",
     "timestamp": 1733633424909,
     "user": {
      "displayName": "Emily Thomas",
      "userId": "07658032212383838916"
     },
     "user_tz": 480
    },
    "id": "Wi-yY_TlDHPn",
    "outputId": "0b5095ff-fa99-4e78-8ec7-b45f013be798"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-9b4480a0d26f>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  image_tensors, image_labels = torch.load(pt_filepath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded batch_0.pt\n",
      "Loaded batch_1.pt\n",
      "Loaded batch_2.pt\n",
      "Loaded batch_3.pt\n",
      "Loaded batch_4.pt\n",
      "Number of images: 5000\n",
      "Number of labels: 5000\n"
     ]
    }
   ],
   "source": [
    "pt_files = os.listdir(os.path.join(GOOGLE_DRIVE_PATH,tensors_dir))\n",
    "all_image_tensors = []\n",
    "all_image_labels = []\n",
    "\n",
    "# Load and combine all .pt files\n",
    "for pt_file in pt_files[:5]:\n",
    "    pt_filepath = os.path.join(GOOGLE_DRIVE_PATH,tensors_dir,pt_file)\n",
    "    image_tensors, image_labels = torch.load(pt_filepath)\n",
    "    all_image_tensors.append(image_tensors)\n",
    "    all_image_labels.extend(image_labels)\n",
    "    print(f\"Loaded {pt_file}\")\n",
    "\n",
    "# Concatenate tensors\n",
    "all_image_tensors = torch.cat(all_image_tensors)\n",
    "print(f\"Number of images: {all_image_tensors.size(0)}\")\n",
    "print(f\"Number of labels: {len(all_image_labels)}\")\n",
    "assert all_image_tensors.size(0) == len(all_image_labels), \"Mismatch between images and labels!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPDFYvqaJv5w"
   },
   "source": [
    "Reorganize dataframe to be in the same order as the Image Tensors using the image_label as the matching key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2761,
     "status": "ok",
     "timestamp": 1733633427667,
     "user": {
      "displayName": "Emily Thomas",
      "userId": "07658032212383838916"
     },
     "user_tz": 480
    },
    "id": "rtFcFs4-Ji7b",
    "outputId": "f2d02d25-544c-4f06-f5cb-44ade9851eb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spanish-style-grilled-vegetables-with-breadcrumb-picada-238806', 'three-cheese-pizza-with-onion-sage-and-arugula-233543', 'speedy-skillet-ravioli-lasagna', 'summer-anchovy-salad-51108430', 'spice-roasted-cornish-hens-with-cucumber-yogurt-sauce-353418', 'stovetop-butterscotch-apples-and-cranberries', 'tarte-tatin-51196820', 'spanish-olive-and-cream-cheese-canapes-231160', 'spice-rubbed-sustainable-fish-sliders', 'spiced-pumpkin-layer-cake-240123']\n",
      "2            thanksgiving-mac-and-cheese-erick-williams\n",
      "5                  warm-comfort-tequila-chamomile-toddy\n",
      "7                     turmeric-hot-toddy-claire-sprouse\n",
      "9        spiced-lentil-and-caramelized-onion-baked-eggs\n",
      "11          spiral-ham-in-the-slow-cooker-guarnaschelli\n",
      "19    roasted-beets-with-crispy-sunchokes-and-pickle...\n",
      "24    sloppy-joe-shirred-eggs-with-spinach-vivian-ho...\n",
      "27                           spicy-coconut-pumpkin-soup\n",
      "30                                trinidad-curry-powder\n",
      "31                                  shrimp-creole-14653\n",
      "Name: Image_Name, dtype: object\n",
      "(13496, 5)\n",
      "(4968, 5) 5000\n",
      "Number of filtered tensors: 4968\n",
      "Number of filtered labels: 4968\n",
      "Number of rows in filtered_df: 4968\n",
      "0    spanish-style-grilled-vegetables-with-breadcru...\n",
      "1    three-cheese-pizza-with-onion-sage-and-arugula...\n",
      "2                       speedy-skillet-ravioli-lasagna\n",
      "3                        summer-anchovy-salad-51108430\n",
      "4    spice-roasted-cornish-hens-with-cucumber-yogur...\n",
      "5         stovetop-butterscotch-apples-and-cranberries\n",
      "6                                 tarte-tatin-51196820\n",
      "7        spanish-olive-and-cream-cheese-canapes-231160\n",
      "8                spice-rubbed-sustainable-fish-sliders\n",
      "9                     spiced-pumpkin-layer-cake-240123\n",
      "Name: Image_Name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "##Reset order of dataframe to match the image labels orders\n",
    "all_image_labels_cleaned = [label.split(\".\")[0] for label in all_image_labels] #remove the .png and .jgp\n",
    "print(all_image_labels_cleaned[:10]) #print to see if at the end the df matches\n",
    "\n",
    "filtered_df = df[df[\"Image_Name\"].isin(all_image_labels_cleaned)] #filter the df to only have values from the images\n",
    "print(filtered_df[\"Image_Name\"][:10])\n",
    "print(df.shape)\n",
    "print(filtered_df.shape, len(all_image_labels_cleaned))\n",
    "\n",
    "valid_labels = set(filtered_df['Image_Name'])\n",
    "\n",
    "# Filter labels and tensors\n",
    "filtered_labels_and_tensors = [\n",
    "    (tensor, label) for tensor, label in zip(all_image_tensors, all_image_labels_cleaned) if label in valid_labels\n",
    "]\n",
    "\n",
    "# Unpack the filtered data\n",
    "filtered_tensors, filtered_labels = zip(*filtered_labels_and_tensors)\n",
    "\n",
    "# Convert back to tensors\n",
    "filtered_tensors = torch.stack(filtered_tensors)\n",
    "filtered_labels = list(filtered_labels)\n",
    "\n",
    "# Verify alignment\n",
    "print(f\"Number of filtered tensors: {len(filtered_tensors)}\")\n",
    "print(f\"Number of filtered labels: {len(filtered_labels)}\")\n",
    "print(f\"Number of rows in filtered_df: {len(filtered_df)}\")\n",
    "\n",
    "# Finally reorganize the df to be in the same order as the image tensors\n",
    "filtered_df = filtered_df.set_index(\"Image_Name\").loc[filtered_labels].reset_index()\n",
    "print(filtered_df[\"Image_Name\"][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1733633427669,
     "user": {
      "displayName": "Emily Thomas",
      "userId": "07658032212383838916"
     },
     "user_tz": 480
    },
    "id": "GeAOZnJ6A5mR",
    "outputId": "6f082a89-48ca-4f5e-dcf4-a95b97049cf4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"filtered_df\",\n  \"rows\": 4968,\n  \"fields\": [\n    {\n      \"column\": \"Image_Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4968,\n        \"samples\": [\n          \"twice-cooked-french-fries-241100\",\n          \"raspberry-white-chocolate-and-almond-trifle-233432\",\n          \"simple-is-best-dressing-51124210\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Unnamed: 0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3864,\n        \"min\": 2,\n        \"max\": 13499,\n        \"num_unique_values\": 4968,\n        \"samples\": [\n          10259,\n          12156,\n          5957\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4908,\n        \"samples\": [\n          \"Pimiento Cheese Crackers\",\n          \"Scallop Rice Bowls With Crunchy Spice Oil\",\n          \"Yodel B\\u00fbche de No\\u00ebl\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Instructions\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Cleaned_Ingredients\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "filtered_df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-35e80d4d-03b7-4017-98dc-592608a37b02\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image_Name</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Title</th>\n",
       "      <th>Instructions</th>\n",
       "      <th>Cleaned_Ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spanish-style-grilled-vegetables-with-breadcru...</td>\n",
       "      <td>10787</td>\n",
       "      <td>Spanish-Style Grilled Vegetables with Breadcru...</td>\n",
       "      <td>[Prepare barbecue (medium heat). Arrange veget...</td>\n",
       "      <td>[3 large red bell peppers (about 1 1/2 pounds)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>three-cheese-pizza-with-onion-sage-and-arugula...</td>\n",
       "      <td>12145</td>\n",
       "      <td>Three-Cheese Pizza with Onion, Sage, and Arugula</td>\n",
       "      <td>[Place pizza stone on floor of gas oven or on ...</td>\n",
       "      <td>[1 (1/4-oz) package active dry yeast (2 1/4 te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>speedy-skillet-ravioli-lasagna</td>\n",
       "      <td>551</td>\n",
       "      <td>Speedy Skillet Ravioli Lasagna</td>\n",
       "      <td>[Preheat the oven to 450°F with a rack in the ...</td>\n",
       "      <td>[2 tbsp. extra-virgin olive oil, 2 large garli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>summer-anchovy-salad-51108430</td>\n",
       "      <td>6117</td>\n",
       "      <td>Summer Anchovy Salad</td>\n",
       "      <td>[Cut tomatoes into fat wedges. Drizzle with ol...</td>\n",
       "      <td>[Tomatoes, Olive oil, Splash of vinegar, Crumb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spice-roasted-cornish-hens-with-cucumber-yogur...</td>\n",
       "      <td>8769</td>\n",
       "      <td>Spice-Roasted Cornish Hens with Cucumber-Yogur...</td>\n",
       "      <td>[Position rack in top third of oven; preheat t...</td>\n",
       "      <td>[3 1 1/4 to 1 1/2-pound Cornish game hens, spl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-35e80d4d-03b7-4017-98dc-592608a37b02')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-35e80d4d-03b7-4017-98dc-592608a37b02 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-35e80d4d-03b7-4017-98dc-592608a37b02');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-40f9ccb1-11b6-4b9e-96e6-a5dafca32c55\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-40f9ccb1-11b6-4b9e-96e6-a5dafca32c55')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-40f9ccb1-11b6-4b9e-96e6-a5dafca32c55 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                          Image_Name  Unnamed: 0  \\\n",
       "0  spanish-style-grilled-vegetables-with-breadcru...       10787   \n",
       "1  three-cheese-pizza-with-onion-sage-and-arugula...       12145   \n",
       "2                     speedy-skillet-ravioli-lasagna         551   \n",
       "3                      summer-anchovy-salad-51108430        6117   \n",
       "4  spice-roasted-cornish-hens-with-cucumber-yogur...        8769   \n",
       "\n",
       "                                               Title  \\\n",
       "0  Spanish-Style Grilled Vegetables with Breadcru...   \n",
       "1   Three-Cheese Pizza with Onion, Sage, and Arugula   \n",
       "2                     Speedy Skillet Ravioli Lasagna   \n",
       "3                               Summer Anchovy Salad   \n",
       "4  Spice-Roasted Cornish Hens with Cucumber-Yogur...   \n",
       "\n",
       "                                        Instructions  \\\n",
       "0  [Prepare barbecue (medium heat). Arrange veget...   \n",
       "1  [Place pizza stone on floor of gas oven or on ...   \n",
       "2  [Preheat the oven to 450°F with a rack in the ...   \n",
       "3  [Cut tomatoes into fat wedges. Drizzle with ol...   \n",
       "4  [Position rack in top third of oven; preheat t...   \n",
       "\n",
       "                                 Cleaned_Ingredients  \n",
       "0  [3 large red bell peppers (about 1 1/2 pounds)...  \n",
       "1  [1 (1/4-oz) package active dry yeast (2 1/4 te...  \n",
       "2  [2 tbsp. extra-virgin olive oil, 2 large garli...  \n",
       "3  [Tomatoes, Olive oil, Splash of vinegar, Crumb...  \n",
       "4  [3 1 1/4 to 1 1/2-pound Cornish game hens, spl...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFbLiKRFJxmj"
   },
   "source": [
    "Reformat Ingredients, Recipes, and Image titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XgK81ynndVtG"
   },
   "source": [
    "Tokenize Recipes, Ingredients, and Image Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39365,
     "status": "ok",
     "timestamp": 1733633467024,
     "user": {
      "displayName": "Emily Thomas",
      "userId": "07658032212383838916"
     },
     "user_tz": 480
    },
    "id": "wyHgDU2OmUO0",
    "outputId": "d5dd2a70-f500-416d-ad05-ad16f8011ac9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[[101, 1017, 2312, 2417, 4330, 23582, 1006, 2055, 1015, 1015, 1013, 1016, 7038, 1007, 1010, 27674, 1010, 13916, 1010, 4284, 2098, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1018, 2312, 2887, 8288, 24759, 11390, 1006, 2055, 1015, 1015, 1013, 1018, 7038, 1007, 1010, 21920, 1010, 3013, 3091, 14244, 2046, 1017, 25609, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1018, 5396, 2665, 2030, 3756, 16950, 25955, 3490, 1006, 9544, 8231, 1016, 1997, 2169, 1025, 2055, 1015, 9044, 1007, 1010, 21920, 1010, 3013, 3091, 14244, 2046, 1015, 1013, 1017, 1011, 4960, 1011, 4317, 25609, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 4469, 1011, 6261, 9724, 3514, 1006, 2005, 18651, 2075, 1007, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1020, 7251, 24667, 3619, 4469, 1011, 6261, 9724, 3514, 1010, 4055, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1016, 20548, 18856, 21818, 2015, 1010, 22126, 24881, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1015, 1013, 1016, 5572, 13102, 7828, 9550, 10560, 2417, 11565, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1015, 1013, 1016, 2452, 6090, 3683, 1006, 2887, 7852, 26775, 25438, 2015, 1007, 1008, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1016, 7251, 24667, 3619, 22268, 4511, 29387, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1015, 1013, 1018, 2452, 24881, 4840, 3059, 11968, 8002, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1016, 7251, 24667, 3619, 24881, 4840, 10848, 29451, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1008, 2800, 1999, 1996, 4004, 9440, 2930, 1997, 2070, 26676, 1998, 2012, 4004, 6089, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[101, 7374, 26375, 1006, 5396, 3684, 1007, 1012, 13621, 11546, 2006, 21522, 8697, 1012, 8248, 2007, 3514, 1025, 11867, 6657, 19099, 2007, 5474, 1998, 11565, 1012, 18651, 23582, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 3096, 2217, 2091, 1998, 2302, 3810, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2127, 25788, 1998, 1038, 9863, 6850, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 3048, 5681, 2005, 2130, 8434, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2055, 2184, 2781, 1012, 4372, 20464, 9232, 1999, 6081, 4524, 1012, 2292, 3233, 2127, 21049, 29476, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2055, 2382, 2781, 1012, 18651, 8288, 24759, 11390, 1998, 16950, 25955, 3490, 2127, 29030, 1998, 8616, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 3810, 1998, 4373, 24388, 2075, 2005, 2130, 18778, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1019, 2000, 1020, 2781, 1012, 2173, 2006, 17910, 18194, 21522, 7123, 1012, 14113, 23582, 1012, 4651, 2000, 7123, 2007, 8288, 24759, 11390, 1998, 16950, 25955, 3490, 1012, 3684, 1017, 7251, 24667, 3619, 9724, 3514, 1999, 5396, 8066, 3388, 2058, 5396, 3684, 1012, 5587, 20548, 1998, 10560, 2417, 11565, 1025, 16130, 2127, 25312, 18980, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2055, 2382, 3823, 1012, 5587, 7852, 26775, 25438, 2015, 1025, 16130, 2127, 3585, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2055, 1017, 2781, 1012, 2161, 7852, 26775, 25438, 27263, 8447, 2000, 5510, 2007, 5474, 1025, 26988, 2046, 2235, 4605, 1012, 2173, 29387, 1999, 2178, 2235, 4605, 1025, 1059, 24158, 2243, 1999, 1017, 7251, 24667, 3619, 3514, 1012, 4666, 1999, 11968, 8002, 1998, 10848, 29451, 1012, 2161, 2000, 5510, 2007, 5474, 1012, 13621, 11546, 2006, 28005, 2121, 1012, 15642, 12810, 11225, 2058, 1025, 11867, 6657, 19099, 2007, 7852, 26775, 25438, 2015, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "total_max = 128\n",
    "# Initialize the tokenizer\n",
    "tokenizer_recipes = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_nested_list(nested_list, max_length=128):\n",
    "    \"\"\"\n",
    "    Tokenizes a nested list of strings (list of ingredients per recipe).\n",
    "    Each inner list is tokenized into a list of token IDs.\n",
    "    \"\"\"\n",
    "    tokenized_list = []\n",
    "    for sublist in nested_list:\n",
    "        # Join the inner list into a string\n",
    "        # text = \" \".join(sublist)\n",
    "        text = str(sublist)\n",
    "        # Tokenize the string\n",
    "        tokens = tokenizer_recipes(\n",
    "            text,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        # Append tokenized input_ids to the result list\n",
    "        tokenized_list.append(tokens[\"input_ids\"].squeeze(0).tolist())\n",
    "    return tokenized_list\n",
    "\n",
    "filtered_df['Title_List'] = df['Title'].apply(lambda x: [x])\n",
    "filtered_df['tokenized_ingredients'] = filtered_df['Cleaned_Ingredients'].apply(lambda x: tokenize_nested_list(x, total_max))\n",
    "filtered_df['tokenized_instructions'] = filtered_df['Instructions'].apply(lambda x: tokenize_nested_list(x, total_max))\n",
    "filtered_df['tokenized_titles'] = filtered_df['Title_List'].apply(lambda x: tokenize_nested_list(x, total_max))\n",
    "\n",
    "\n",
    "# # Store tokenized data in a dictionary for your DataLoader\n",
    "# filtered_df[\"tokenized_titles\"] = title_tokens[\"input_ids\"]\n",
    "# filtered_df[\"title_attention_mask\"] = title_tokens[\"attention_mask\"]\n",
    "\n",
    "# filtered_df[\"tokenized_ingredients\"] = ingredients_tokens[\"input_ids\"]\n",
    "# filtered_df[\"ingredients_attention_mask\"] = ingredients_tokens[\"attention_mask\"]\n",
    "\n",
    "# filtered_df[\"tokenized_instructions\"] = instructions_tokens[\"input_ids\"]\n",
    "# filtered_df[\"instructions_attention_mask\"] = instructions_tokens[\"attention_mask\"]\n",
    "print(len(filtered_df[\"tokenized_titles\"][0]))\n",
    "print(filtered_df[\"tokenized_ingredients\"][0])\n",
    "print(filtered_df[\"tokenized_instructions\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 406,
     "status": "ok",
     "timestamp": 1733633467409,
     "user": {
      "displayName": "Emily Thomas",
      "userId": "07658032212383838916"
     },
     "user_tz": 480
    },
    "id": "52oSgq6k7Ldn",
    "outputId": "41d3392b-719b-4726-8d7a-377d7539e8ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 23 1\n",
      "42\n",
      "[[101, 1015, 1013, 1018, 2452, 9724, 3514, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1016, 3009, 24444, 1010, 20956, 1998, 4857, 2135, 15920, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1016, 2312, 18856, 21818, 2015, 20548, 1010, 20956, 1998, 24881, 1010, 4606, 1018, 18856, 21818, 2015, 2878, 20548, 1010, 20956, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1015, 2452, 4318, 2317, 4511, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1016, 3016, 3727, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1020, 7251, 24667, 3619, 4895, 12002, 3064, 12136, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 10869, 1997, 1016, 14380, 2015, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1016, 1006, 1018, 1011, 9044, 1007, 2878, 2417, 10245, 7347, 1010, 12176, 1010, 2007, 2132, 1998, 5725, 10109, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1015, 2235, 20856, 1010, 15920, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1016, 10335, 8292, 3917, 2100, 1010, 11085, 7178, 2892, 14244, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 10869, 1997, 2184, 3145, 14123, 2015, 2030, 2235, 4857, 1011, 19937, 14123, 2015, 1010, 4606, 1018, 14123, 2015, 3013, 2046, 7728, 2005, 3529, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1015, 1013, 1016, 2452, 20392, 2135, 24881, 4840, 25022, 5802, 13181, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1015, 1006, 2260, 1011, 2011, 2324, 1011, 4960, 2030, 3469, 1007, 25043, 2075, 6090, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "def pad_or_truncate_nested(sublist, target_length, max_length, pad_token=0):\n",
    "        \"\"\"\n",
    "            Pad or truncate the outer list of a nested list to match the target_length.\n",
    "            Each inner list remains untouched.\n",
    "        \"\"\"\n",
    "        # Pad with [pad_token] or truncate the outer list\n",
    "        if len(sublist) < target_length:\n",
    "            sublist.extend([[pad_token]* max_length] * (target_length - len(sublist)))\n",
    "        else:\n",
    "            sublist = sublist[:target_length]\n",
    "        return sublist\n",
    "\n",
    "max_length_ing = filtered_df['tokenized_ingredients'].apply(len).max()\n",
    "max_length_inst = filtered_df['tokenized_instructions'].apply(len).max()//4\n",
    "max_length_title = filtered_df['tokenized_titles'].apply(len).max()\n",
    "filtered_df['tokenized_ingredients'] = filtered_df['tokenized_ingredients'].apply(\n",
    "    lambda ing: pad_or_truncate_nested(ing, max_length_ing,total_max))\n",
    "\n",
    "filtered_df['tokenized_instructions'] = filtered_df['tokenized_instructions'].apply(\n",
    "    lambda inst: pad_or_truncate_nested(inst, max_length_inst, total_max))\n",
    "# new_token_ing = [pad_or_truncate_nested(ing, max_length_title) for ing in tokenized_ingredients] #titles were all list length of 1\n",
    "max_length_ing = filtered_df['tokenized_ingredients'].apply(len).max()\n",
    "max_length_inst = filtered_df['tokenized_instructions'].apply(len).max()\n",
    "max_length_title = filtered_df['tokenized_titles'].apply(len).max()\n",
    "print(max_length_ing, max_length_inst, max_length_title)\n",
    "print(len(filtered_df['tokenized_ingredients'][5]))\n",
    "print(filtered_df['tokenized_ingredients'][100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1jZ1lhBw2DnV"
   },
   "source": [
    "Tokenize the Image Labels for the Image Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 7887,
     "status": "ok",
     "timestamp": 1733633475294,
     "user": {
      "displayName": "Emily Thomas",
      "userId": "07658032212383838916"
     },
     "user_tz": 480
    },
    "id": "w8EKVqNG2CGJ"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import CLIPModel\n",
    "model_name = \"openai/clip-vit-base-patch16\"\n",
    "clip_model = CLIPModel.from_pretrained(model_name)\n",
    "tokenizer_images = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenized_labels = tokenizer_images(\n",
    "    filtered_labels,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=tokenizer_images.model_max_length,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1733633475294,
     "user": {
      "displayName": "Emily Thomas",
      "userId": "07658032212383838916"
     },
     "user_tz": 480
    },
    "id": "ilxU3rfNFI8j",
    "outputId": "e34a9b2c-cee6-4548-f840-afe53fe2be29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "%reload_ext autoreload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1733633475295,
     "user": {
      "displayName": "Emily Thomas",
      "userId": "07658032212383838916"
     },
     "user_tz": 480
    },
    "id": "cOR7eQshjPKL"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class RecipeDecoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size):\n",
    "        super(RecipeDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, input_tokens):\n",
    "        embedded = self.embedding(input_tokens)\n",
    "        outputs, hidden_state = self.rnn(embedded, hidden_state)\n",
    "        predictions = self.fc_out(outputs)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 176,
     "status": "ok",
     "timestamp": 1733633475449,
     "user": {
      "displayName": "Emily Thomas",
      "userId": "07658032212383838916"
     },
     "user_tz": 480
    },
    "id": "N_UNJWI5GMCg"
   },
   "outputs": [],
   "source": [
    "def calc_recall(top, image_features, recipe_embeddings, image_labels):\n",
    "  true_pos = 0\n",
    "  false_neg = 0\n",
    "  tmp_batches = image_features.shape[0]\n",
    "  print(image_features.shape, recipe_embeddings.shape)\n",
    "  for i in range(tmp_batches):\n",
    "    cosine_similarity = torch.nn.CosineSimilarity(dim=1)\n",
    "    similarities = cosine_similarity(image_features[i, :].unsqueeze(0), recipe_embeddings)\n",
    "    print(similarities)\n",
    "    #top = amount of top results to retrieve\n",
    "    top_results = top\n",
    "    top_k_values, top_k_indices = torch.topk(similarities, top_results, largest=True)\n",
    "    print(top_k_values, top_k_indices)\n",
    "    top_images = [(filtered_df['Image_Name'][i]) for i in top_k_indices]\n",
    "    if image_labels[i] in [image[0] for image in top_images]:\n",
    "      true_pos += 1\n",
    "    else:\n",
    "      false_neg += 1\n",
    "\n",
    "  return (true_pos / tmp_batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1733633475449,
     "user": {
      "displayName": "Emily Thomas",
      "userId": "07658032212383838916"
     },
     "user_tz": 480
    },
    "id": "nc71r5-Uyouz"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch\n",
    "from models.image_2_recipe import Image2Recipe\n",
    "from models.image_encoder import Image_Encoder\n",
    "from models.recipe_encoder import RecipeEncoder\n",
    "from models.MMR import MMR\n",
    "from models.MMR import MMR_losses\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class Data_Loading(Dataset):\n",
    "    \"\"\"\n",
    "    Class to combine the Images, Labels, Recipes together to be used in combination when inputted into Model\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenized_ingredients, tokenized_instructions, tokenized_titles, image_tensors, image_labels):\n",
    "        self.ingredients = torch.tensor(tokenized_ingredients, dtype=torch.int16)\n",
    "        self.instructions = torch.tensor(tokenized_instructions, dtype=torch.int16)\n",
    "        self.titles = torch.tensor(tokenized_titles, dtype=torch.int16)\n",
    "        self.images = image_tensors\n",
    "        self.image_labels = image_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"ingredients\": self.ingredients[idx],\n",
    "            \"instructions\": self.instructions[idx],\n",
    "            \"titles\": self.titles[idx],\n",
    "            \"images\": self.images[idx],\n",
    "            \"image_labels\": self.image_labels[idx]\n",
    "            # \"tokenized_labels\": {\n",
    "            #     \"input_ids\": self.tokenized_labels['input_ids'][idx].to(dtype=torch.long),\n",
    "            #     \"attention_mask\": self.tokenized_labels['attention_mask'][idx].to(dtype=torch.uint8)\n",
    "            # }\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "class Trainer(object):\n",
    "    \"\"\"\n",
    "    Class designed to run ViT (train, evaluate, plot)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize ViT\n",
    "        \"\"\"\n",
    "        self.epochs = kwargs['epochs']\n",
    "        self.optimizer_name = kwargs['optimizer']\n",
    "        self.device = kwargs['device']\n",
    "        self.batch_size = kwargs['batch_size']\n",
    "        self.lr = kwargs['learning_rate']\n",
    "\n",
    "        self.tokenized_ingredients = kwargs['ingredient_tokens']\n",
    "        self.tokenized_instructions = kwargs['instruction_tokens']\n",
    "        self.tokenized_title = kwargs['title_tokens']\n",
    "        self.image_tensor = kwargs['image_tensors']\n",
    "        self.image_labels = kwargs['image_labels']\n",
    "        self.clip_model = kwargs['clip_model']\n",
    "        self.vocab_size = kwargs['vocab_size']\n",
    "        self.max_len = kwargs['max_len']\n",
    "        self.instance_weight = kwargs['instance_weight']\n",
    "        self.sem_weight = kwargs['sem_weight']\n",
    "        self.itm_weight = kwargs['itm_weight']\n",
    "        self.best_model_parameters = kwargs['best_model_parameters_path']\n",
    "        # self.margin = kwargs['margin']\n",
    "        self.initial_margin = kwargs['initial_margin']\n",
    "        self.margin_step = kwargs['margin_step']\n",
    "        self.max_margin = kwargs['max_margin']\n",
    "        self.topk = kwargs['topk']\n",
    "        # self.patience = kwargs['patience']\n",
    "        self.patience = 5\n",
    "        # Pending variable margin calc\n",
    "        # self.loss_calcs = MMR_losses(margin=1.0, instance_weight=self.instance_weight, sem_weight=self.sem_weight, itm_weight=self.itm_weight)\n",
    "        self.loss_calcs = MMR_losses(instance_weight=self.instance_weight, sem_weight=self.sem_weight, itm_weight=self.itm_weight)\n",
    "        # num_classes = len(set(self.image_labels['input_ids']))\n",
    "        num_classes = len(set(self.image_labels))\n",
    "        # print(num_classes)\n",
    "\n",
    "\n",
    "        self.image_encoder = Image_Encoder(self.device, self.clip_model, num_classes).to(self.device)\n",
    "        self.recipe_encoder = RecipeEncoder(self.device, self.vocab_size, self.max_len).to(self.device)\n",
    "        self.mmr = MMR(hidden_dim=self.image_encoder.clip_model.config.projection_dim).to(self.device)\n",
    "        # self.recipe_decoder = RecipeDecoder(512,512,self.vocab_size).to(self.device)\n",
    "        # MMR varaibles: num_heads=num_heads, ITEM_lyrs=ITEM_lyrs, MTD_lyrs=MTD_lyrs, projection_dim=projection_dim\n",
    "        self.model = Image2Recipe(self.image_encoder, self.recipe_encoder, self.mmr).to(self.device)\n",
    "\n",
    "\n",
    "        ##DO we want to tune each of these learning rates for each model?\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        # self.optimizer2 = torch.optim.Adam(self.recipe_decoder.parameters(), lr=self.lr)\n",
    "        # self.optimizer = torch.optim.AdamW([\n",
    "        #     {\"params\": self.model.image_encoder.parent_model.parameters(), \"lr\": 1e-6},\n",
    "        #     {\"params\": self.model.recipe_encoder.parameters(), \"lr\": 1e-5},\n",
    "        #     {\"params\": self.model.image_encoder.fc1.parameters(), \"lr\": 1e-5},\n",
    "        #     {\"params\": self.model.recipe_encoder.ll_e.parameters(), \"lr\": 1e-5},\n",
    "        # ])\n",
    "\n",
    "\n",
    "        #Combine Images, Recipes, Instructions in training and eval datasets\n",
    "        self.data_total = Data_Loading(\n",
    "            self.tokenized_ingredients,\n",
    "            self.tokenized_instructions,\n",
    "            self.tokenized_title,\n",
    "            self.image_tensor,\n",
    "            self.image_labels\n",
    "        )\n",
    "        training_perc = .9\n",
    "        train_size = int(training_perc * len(self.data_total))\n",
    "        eval_size = len(self.data_total) - train_size\n",
    "        train_dataset, eval_dataset = random_split(self.data_total, [train_size, eval_size])\n",
    "        self.dataloader = {}\n",
    "        self.dataloader['train'] = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, drop_last=True)\n",
    "        self.dataloader['eval'] = DataLoader(eval_dataset, batch_size=self.batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "        #Lists to fill up during training and plotted later for learning curves\n",
    "        self.train_loss_list = []\n",
    "        self.eval_loss_list = []\n",
    "        self.eval_acc_list = []\n",
    "        self.eval_acc_list = []\n",
    "        self.best_score = 0\n",
    "        self.best_loss = 1000\n",
    "        print(\"finished initializing\")\n",
    "\n",
    "        # self.CELoss = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "        # self.decoder_lambda = kwargs['decoder_lambda']\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train ViT, image encoder, recipe encoder, MMR\n",
    "        \"\"\"\n",
    "        # Set initial margin\n",
    "        self.margin = self.initial_margin\n",
    "        patience_count = 0\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "\n",
    "            for phase in ['train', 'eval']:\n",
    "                total_loss = 0\n",
    "                total_accuracy = 0\n",
    "                total_eval_loss = 0\n",
    "                if phase == 'train':\n",
    "                    self.model.train()\n",
    "                else:\n",
    "                    self.model.eval()\n",
    "                for i, batch_data in enumerate(tqdm(self.dataloader[phase],position=0, leave=True)):\n",
    "                    #Looping through batches of training data then eval data each epoch\n",
    "                    #TODO: Add how the recipe, instructions, and titles will be tokenized\n",
    "                    ingredients, instructions, titles, images, image_labels = (\n",
    "                        batch_data['ingredients'].to(self.device),\n",
    "                        batch_data['instructions'].to(self.device),\n",
    "                        batch_data['titles'].to(self.device),\n",
    "                        batch_data['images'].to(self.device),\n",
    "                        batch_data['image_labels']\n",
    "                    )\n",
    "\n",
    "                    recipe_enc_src = [titles, ingredients, instructions]\n",
    "                    self.optimizer.zero_grad()\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        output = self.model(images, image_labels, recipe_enc_src)\n",
    "                        ##Combine the Recipe Encoder Losses and Image Encoder Losses based on TFOOD\n",
    "                        mmr_logits = output[\"mmr_logits\"]\n",
    "                        image_logits = output[\"image_logits\"]\n",
    "                        image_embeddings_proj = output[\"image_embeddings_proj\"]\n",
    "                        recipe_embeddings_proj = output[\"recipe_embeddings_proj\"]\n",
    "                        # image_embeddings = output[\"image_embeddings\"]\n",
    "                        # recipe_embeddings = output[\"recipe_embeddings\"]\n",
    "\n",
    "                        # #Train the Recipe Decoder so it can output a generative (Title, Ingredient, Instructions)\n",
    "                        # title_logits = self.recipe_decoder(image_embeddings, titles)\n",
    "                        # ingredients_logits = self.recipe_decoder(image_embeddings, ingredients)\n",
    "                        # instructions_logits = self.recipe_decoder(image_embeddings, instructions)\n",
    "\n",
    "                        # #Compute Reconstruction Loss\n",
    "                        # title_loss = self.CELoss(title_logits.transpose(1, 2), titles)\n",
    "                        # ingredients_loss = self.CELoss(ingredients_logits.transpose(1, 2), ingredients)\n",
    "                        # instructions_loss = self.CELoss(instructions_logits.transpose(1, 2), instructions)\n",
    "                        # decoder_loss = (title_loss + ingredients_loss + instructions_loss) *self.decoder_lambda\n",
    "\n",
    "                        loss = self.loss_calcs.total_loss(image_logits, image_embeddings_proj,\n",
    "                                                          recipe_embeddings_proj, mmr_logits, self.margin)\n",
    "\n",
    "                        print(f'training loss for step: {loss.item()}')\n",
    "                        # recall_score = calc_recall(self.topk, image_embeddings_proj, recipe_embeddings_proj, image_labels)\n",
    "                        # print(recall_score)\n",
    "                        loss.backward()\n",
    "                        self.optimizer.step()\n",
    "                        # self.optimizer2.step() #decoder optimizer\n",
    "\n",
    "                    else: ##Eval mode\n",
    "                        with torch.no_grad():\n",
    "                            output = self.model(images, image_labels, recipe_enc_src)\n",
    "                            mmr_logits = output[\"mmr_logits\"]\n",
    "                            image_logits = output[\"image_logits\"]\n",
    "                            image_embeddings_proj = output[\"image_embeddings_proj\"]\n",
    "                            recipe_embeddings_proj = output[\"recipe_embeddings_proj\"]\n",
    "                            # #Train the Recipe Decoder so it can output a generative (Title, Ingredient, Instructions)\n",
    "                            # title_logits = self.recipe_decoder(image_embeddings, titles)\n",
    "                            # ingredients_logits = self.recipe_decoder(image_embeddings, ingredients)\n",
    "                            # instructions_logits = self.recipe_decoder(image_embeddings, instructions)\n",
    "\n",
    "                            # #Compute Reconstruction Loss\n",
    "                            # title_loss = self.CELoss(title_logits.transpose(1, 2), titles)\n",
    "                            # ingredients_loss = self.CELoss(ingredients_logits.transpose(1, 2), ingredients)\n",
    "                            # instructions_loss = self.CELoss(instructions_logits.transpose(1, 2), instructions)\n",
    "                            # decoder_loss = (title_loss + ingredients_loss + instructions_loss) * self.decoder_lambda\n",
    "                            eval_loss = self.loss_calcs.total_eval_loss(image_logits, image_embeddings_proj,\n",
    "                                                                   recipe_embeddings_proj)# + decoder_loss\n",
    "                            total_eval_loss += eval_loss.item()\n",
    "                            # print(f'eval loss for step: {loss}')\n",
    "                            # self.eval_loss_list.append(loss.item())\n",
    "\n",
    "                            # recall_score = calc_recall(self.topk, image_embeddings_proj, recipe_embeddings_proj, image_labels)\n",
    "                            # print(recall_score)\n",
    "\n",
    "\n",
    "                    # del unused_tensor\n",
    "                    torch.cuda.empty_cache() #clear cache after each batch\n",
    "                    # print(i)\n",
    "                    # print(output)\n",
    "                    total_loss += loss.item()\n",
    "\n",
    "\n",
    "                # print(f\"{phase}: Epoch {epoch+1}, Loss: {total_loss / len(self.dataloader[phase])}\")\n",
    "\n",
    "                # Update loss margin\n",
    "                if self.margin <= (self.max_margin - self.margin_step):\n",
    "                    self.margin += self.margin_step\n",
    "                else:\n",
    "                    self.margin = self.max_margin\n",
    "                if phase == \"train\":\n",
    "                  self.train_loss_list.append(total_loss/len(self.dataloader[phase]))\n",
    "                else:\n",
    "                  self.eval_loss_list.append(total_eval_loss/len(self.dataloader[phase]))\n",
    "                  if total_eval_loss < self.best_loss:\n",
    "                    patience_count = 0\n",
    "                    self.best_loss = total_eval_loss\n",
    "                    torch.save(self.model.state_dict(), self.best_model_parameters)\n",
    "                    print(\"Saving best model\")\n",
    "                    # torch.save()\n",
    "                  else:\n",
    "                    patience_count += 1\n",
    "                    if patience_count >= self.patience:\n",
    "                      print(\"Early stopping\")\n",
    "                      break\n",
    "\n",
    "    ##Waiting on training code to finish\n",
    "    def plot_learning_loss_curves(self):\n",
    "        \"\"\"\n",
    "        Plot accuracy and loss curves for training and eval accuracy/loss lists (item/epoch)\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.train_loss_list, label='Training Loss')\n",
    "        plt.plot(self.eval_loss_list, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Loss Curve')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # plt.figure(figsize=(10, 5))\n",
    "        # plt.plot(self.train_acc_list, label='Training Accuracy')\n",
    "        # plt.plot(self.eval_acc_list, label='Validation Accuracy')\n",
    "        # plt.xlabel('Epoch')\n",
    "        # plt.ylabel('Accuracy')\n",
    "        # plt.title('Accuracy Curve')\n",
    "        # plt.legend()\n",
    "        # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 165,
     "status": "ok",
     "timestamp": 1733633475611,
     "user": {
      "displayName": "Emily Thomas",
      "userId": "07658032212383838916"
     },
     "user_tz": 480
    },
    "id": "oCpaIgTRca8s"
   },
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1733633475611,
     "user": {
      "displayName": "Emily Thomas",
      "userId": "07658032212383838916"
     },
     "user_tz": 480
    },
    "id": "jXA0oBF70OCN",
    "outputId": "48d0542d-8e0a-44bd-af9f-aa354ebe0225"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "128\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "print(len(filtered_df['tokenized_ingredients'][1][0]))\n",
    "print(len(filtered_df['tokenized_instructions'][1][0]))\n",
    "print(len(filtered_df['tokenized_titles'][1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 213,
     "status": "ok",
     "timestamp": 1733633475822,
     "user": {
      "displayName": "Emily Thomas",
      "userId": "07658032212383838916"
     },
     "user_tz": 480
    },
    "id": "4lOE7ew3Czjr",
    "outputId": "d017c944-cebb-455b-8820-3bbb6cd2f11d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "DL-FetOiVKeT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n",
      "True\n",
      "finished initializing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e6e4f6565de4a8aa1e3bd8223e96e5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/447 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss for step: 4.678544998168945\n",
      "training loss for step: 6.80489444732666\n",
      "training loss for step: 223.24327087402344\n",
      "training loss for step: 1942.703857421875\n",
      "training loss for step: 173.8151092529297\n",
      "training loss for step: 1102.8529052734375\n",
      "training loss for step: 1049.574462890625\n",
      "training loss for step: 29.945636749267578\n",
      "training loss for step: 3035.361572265625\n",
      "training loss for step: 2564.353759765625\n",
      "training loss for step: 1213.419677734375\n",
      "training loss for step: 28.982295989990234\n",
      "training loss for step: 307.5826110839844\n",
      "training loss for step: 211.1154327392578\n",
      "training loss for step: 516.989501953125\n",
      "training loss for step: 577.34912109375\n",
      "training loss for step: 120.47570037841797\n",
      "training loss for step: 269.15814208984375\n",
      "training loss for step: 174.69615173339844\n",
      "training loss for step: 283.94818115234375\n",
      "training loss for step: 11.576366424560547\n",
      "training loss for step: 127.49201202392578\n",
      "training loss for step: 250.23414611816406\n",
      "training loss for step: 337.8952331542969\n",
      "training loss for step: 237.00189208984375\n",
      "training loss for step: 40.15675354003906\n",
      "training loss for step: 73.53287506103516\n",
      "training loss for step: 9.277381896972656\n",
      "training loss for step: 367.2287292480469\n",
      "training loss for step: 190.97483825683594\n",
      "training loss for step: 204.9501190185547\n",
      "training loss for step: 346.84832763671875\n",
      "training loss for step: 361.74298095703125\n",
      "training loss for step: 278.4011535644531\n",
      "training loss for step: 128.60140991210938\n",
      "training loss for step: 129.84637451171875\n",
      "training loss for step: 184.8361358642578\n",
      "training loss for step: 21.091827392578125\n",
      "training loss for step: 149.6687774658203\n",
      "training loss for step: 231.61679077148438\n",
      "training loss for step: 256.3209228515625\n",
      "training loss for step: 237.8235321044922\n",
      "training loss for step: 190.31642150878906\n",
      "training loss for step: 127.00252532958984\n",
      "training loss for step: 58.68724822998047\n",
      "training loss for step: 27.56057357788086\n",
      "training loss for step: 66.34981536865234\n",
      "training loss for step: 38.35938262939453\n",
      "training loss for step: 21.81806755065918\n",
      "training loss for step: 42.85011672973633\n",
      "training loss for step: 51.96854019165039\n",
      "training loss for step: 50.62179946899414\n",
      "training loss for step: 40.717411041259766\n",
      "training loss for step: 24.57996368408203\n",
      "training loss for step: 4.305491924285889\n",
      "training loss for step: 49.098628997802734\n",
      "training loss for step: 58.486263275146484\n",
      "training loss for step: 39.102867126464844\n",
      "training loss for step: 4.4468536376953125\n",
      "training loss for step: 14.593539237976074\n",
      "training loss for step: 18.39673614501953\n",
      "training loss for step: 17.18141746520996\n",
      "training loss for step: 12.074431419372559\n",
      "training loss for step: 4.170072555541992\n",
      "training loss for step: 18.85471534729004\n",
      "training loss for step: 18.929731369018555\n",
      "training loss for step: 6.661040782928467\n",
      "training loss for step: 12.828187942504883\n",
      "training loss for step: 18.516498565673828\n",
      "training loss for step: 20.325590133666992\n",
      "training loss for step: 18.919246673583984\n",
      "training loss for step: 14.972908973693848\n",
      "training loss for step: 9.136767387390137\n",
      "training loss for step: 8.515754699707031\n",
      "training loss for step: 12.063876152038574\n",
      "training loss for step: 6.381946086883545\n",
      "training loss for step: 7.954628944396973\n",
      "training loss for step: 10.375733375549316\n",
      "training loss for step: 10.926039695739746\n",
      "training loss for step: 10.0639066696167\n",
      "training loss for step: 8.089561462402344\n",
      "training loss for step: 5.172688961029053\n",
      "training loss for step: 9.618815422058105\n",
      "training loss for step: 11.551431655883789\n",
      "training loss for step: 7.988037109375\n",
      "training loss for step: 5.013248920440674\n",
      "training loss for step: 6.383993625640869\n",
      "training loss for step: 6.722381591796875\n",
      "training loss for step: 6.32227087020874\n",
      "training loss for step: 5.318302631378174\n",
      "training loss for step: 4.616742134094238\n",
      "training loss for step: 5.4988508224487305\n",
      "training loss for step: 4.171131134033203\n",
      "training loss for step: 4.630218029022217\n",
      "training loss for step: 4.620539665222168\n",
      "training loss for step: 4.184570789337158\n",
      "training loss for step: 5.125219345092773\n",
      "training loss for step: 4.261290550231934\n",
      "training loss for step: 4.8438944816589355\n",
      "training loss for step: 5.203957557678223\n",
      "training loss for step: 5.165724277496338\n",
      "training loss for step: 4.833583831787109\n",
      "training loss for step: 4.279308795928955\n",
      "training loss for step: 5.011832237243652\n",
      "training loss for step: 5.038245677947998\n",
      "training loss for step: 4.17458963394165\n",
      "training loss for step: 4.452983856201172\n",
      "training loss for step: 4.613268852233887\n",
      "training loss for step: 4.594559669494629\n",
      "training loss for step: 4.4596638679504395\n",
      "training loss for step: 4.240420341491699\n",
      "training loss for step: 4.2718682289123535\n",
      "training loss for step: 4.475015640258789\n",
      "training loss for step: 4.261444568634033\n",
      "training loss for step: 4.188962459564209\n",
      "training loss for step: 4.277458190917969\n",
      "training loss for step: 4.318017482757568\n",
      "training loss for step: 4.296966552734375\n",
      "training loss for step: 4.236131191253662\n",
      "training loss for step: 4.174446105957031\n",
      "training loss for step: 4.191218852996826\n",
      "training loss for step: 4.254167556762695\n",
      "training loss for step: 4.237302303314209\n",
      "training loss for step: 4.182950496673584\n",
      "training loss for step: 4.1771721839904785\n",
      "training loss for step: 4.196657180786133\n",
      "training loss for step: 4.208651065826416\n",
      "training loss for step: 4.206084251403809\n",
      "training loss for step: 4.187400817871094\n",
      "training loss for step: 4.173701763153076\n",
      "training loss for step: 4.169687271118164\n",
      "training loss for step: 4.1766791343688965\n",
      "training loss for step: 4.188564300537109\n",
      "training loss for step: 4.189892292022705\n",
      "training loss for step: 4.182497501373291\n",
      "training loss for step: 4.169900894165039\n",
      "training loss for step: 4.166391372680664\n",
      "training loss for step: 4.168190956115723\n",
      "training loss for step: 4.170146465301514\n",
      "training loss for step: 4.170909404754639\n",
      "training loss for step: 4.170370578765869\n",
      "training loss for step: 4.169908046722412\n",
      "training loss for step: 4.1688995361328125\n",
      "training loss for step: 4.169338226318359\n",
      "training loss for step: 4.164484024047852\n",
      "training loss for step: 4.164955139160156\n",
      "training loss for step: 4.16429328918457\n",
      "training loss for step: 4.169016361236572\n",
      "training loss for step: 4.173036575317383\n",
      "training loss for step: 4.173876762390137\n",
      "training loss for step: 4.17272424697876\n",
      "training loss for step: 4.169307231903076\n",
      "training loss for step: 4.16464376449585\n",
      "training loss for step: 4.164213180541992\n",
      "training loss for step: 4.166256904602051\n",
      "training loss for step: 4.164053916931152\n",
      "training loss for step: 4.164968013763428\n",
      "training loss for step: 4.164693832397461\n",
      "training loss for step: 4.164498329162598\n",
      "training loss for step: 4.165261268615723\n",
      "training loss for step: 4.167885780334473\n",
      "training loss for step: 4.170186996459961\n",
      "training loss for step: 4.168348789215088\n",
      "training loss for step: 4.167099475860596\n",
      "training loss for step: 4.165426731109619\n",
      "training loss for step: 4.165971755981445\n",
      "training loss for step: 4.1676836013793945\n",
      "training loss for step: 4.168918609619141\n",
      "training loss for step: 4.1684393882751465\n",
      "training loss for step: 4.164198875427246\n",
      "training loss for step: 4.165778160095215\n",
      "training loss for step: 4.16803503036499\n",
      "training loss for step: 4.165694236755371\n",
      "training loss for step: 4.163805961608887\n",
      "training loss for step: 4.164464950561523\n",
      "training loss for step: 4.164949893951416\n",
      "training loss for step: 4.16574764251709\n",
      "training loss for step: 4.166971683502197\n",
      "training loss for step: 4.1643524169921875\n",
      "training loss for step: 4.1638970375061035\n",
      "training loss for step: 4.163702964782715\n",
      "training loss for step: 4.164333343505859\n",
      "training loss for step: 4.165219783782959\n",
      "training loss for step: 4.164812088012695\n",
      "training loss for step: 4.166314125061035\n",
      "training loss for step: 4.1635003089904785\n",
      "training loss for step: 4.1639251708984375\n",
      "training loss for step: 4.1643829345703125\n",
      "training loss for step: 4.165440082550049\n",
      "training loss for step: 4.1650776863098145\n",
      "training loss for step: 4.166756629943848\n",
      "training loss for step: 4.163854598999023\n",
      "training loss for step: 4.164566516876221\n",
      "training loss for step: 4.164514541625977\n",
      "training loss for step: 4.164385795593262\n",
      "training loss for step: 4.163653373718262\n",
      "training loss for step: 4.164815425872803\n",
      "training loss for step: 4.165360450744629\n",
      "training loss for step: 4.164286136627197\n",
      "training loss for step: 4.165585994720459\n",
      "training loss for step: 4.166327953338623\n",
      "training loss for step: 4.165857791900635\n",
      "training loss for step: 4.164911270141602\n",
      "training loss for step: 4.164541721343994\n",
      "training loss for step: 4.165131568908691\n",
      "training loss for step: 4.165774822235107\n",
      "training loss for step: 4.1643171310424805\n",
      "training loss for step: 4.166565418243408\n",
      "training loss for step: 4.16689920425415\n",
      "training loss for step: 4.165918350219727\n",
      "training loss for step: 4.166231632232666\n",
      "training loss for step: 4.164281845092773\n",
      "training loss for step: 4.165185928344727\n",
      "training loss for step: 4.1658220291137695\n",
      "training loss for step: 4.163669109344482\n",
      "training loss for step: 4.165398597717285\n",
      "training loss for step: 4.16622257232666\n",
      "training loss for step: 4.1669206619262695\n",
      "training loss for step: 4.164982318878174\n",
      "training loss for step: 4.165313243865967\n",
      "training loss for step: 4.164391040802002\n",
      "training loss for step: 4.1641693115234375\n",
      "training loss for step: 4.1649394035339355\n",
      "training loss for step: 4.166205406188965\n",
      "training loss for step: 4.166070461273193\n",
      "training loss for step: 4.1655473709106445\n",
      "training loss for step: 4.166121006011963\n",
      "training loss for step: 4.163980007171631\n",
      "training loss for step: 4.164205074310303\n",
      "training loss for step: 4.163926601409912\n",
      "training loss for step: 4.164889335632324\n",
      "training loss for step: 4.168473720550537\n",
      "training loss for step: 4.167304515838623\n",
      "training loss for step: 4.165755271911621\n",
      "training loss for step: 4.164525508880615\n",
      "training loss for step: 4.165771484375\n",
      "training loss for step: 4.163855075836182\n",
      "training loss for step: 4.165471076965332\n",
      "training loss for step: 4.164609909057617\n",
      "training loss for step: 4.165356636047363\n",
      "training loss for step: 4.1638641357421875\n",
      "training loss for step: 4.169116020202637\n",
      "training loss for step: 4.1666059494018555\n",
      "training loss for step: 4.165103912353516\n",
      "training loss for step: 4.165702819824219\n",
      "training loss for step: 4.165655612945557\n",
      "training loss for step: 4.164109706878662\n",
      "training loss for step: 4.167413234710693\n",
      "training loss for step: 4.164957523345947\n",
      "training loss for step: 4.164944648742676\n",
      "training loss for step: 4.166311264038086\n",
      "training loss for step: 4.1668195724487305\n",
      "training loss for step: 4.164124011993408\n",
      "training loss for step: 4.163467884063721\n",
      "training loss for step: 4.164888858795166\n",
      "training loss for step: 4.163608074188232\n",
      "training loss for step: 4.1653594970703125\n",
      "training loss for step: 4.163658618927002\n",
      "training loss for step: 4.163782119750977\n",
      "training loss for step: 4.163540840148926\n",
      "training loss for step: 4.164766788482666\n",
      "training loss for step: 4.164374351501465\n",
      "training loss for step: 4.16351842880249\n",
      "training loss for step: 4.164462566375732\n",
      "training loss for step: 4.163800239562988\n",
      "training loss for step: 4.164508819580078\n",
      "training loss for step: 4.163563251495361\n",
      "training loss for step: 4.16461181640625\n",
      "training loss for step: 4.164305686950684\n",
      "training loss for step: 4.164541244506836\n",
      "training loss for step: 4.167014122009277\n",
      "training loss for step: 4.165835857391357\n",
      "training loss for step: 4.164310455322266\n",
      "training loss for step: 4.164834022521973\n",
      "training loss for step: 4.16694450378418\n",
      "training loss for step: 4.1668620109558105\n",
      "training loss for step: 4.16520881652832\n",
      "training loss for step: 4.164577960968018\n",
      "training loss for step: 4.164994716644287\n",
      "training loss for step: 4.16395902633667\n",
      "training loss for step: 4.164138317108154\n",
      "training loss for step: 4.166813850402832\n",
      "training loss for step: 4.167696952819824\n",
      "training loss for step: 4.168053150177002\n",
      "training loss for step: 4.1661810874938965\n",
      "training loss for step: 4.1646199226379395\n",
      "training loss for step: 4.165807723999023\n",
      "training loss for step: 4.168840408325195\n",
      "training loss for step: 4.169064521789551\n",
      "training loss for step: 4.16741418838501\n",
      "training loss for step: 4.16403865814209\n",
      "training loss for step: 4.167130470275879\n",
      "training loss for step: 4.163527488708496\n",
      "training loss for step: 4.164984703063965\n",
      "training loss for step: 4.164332866668701\n",
      "training loss for step: 4.16547155380249\n",
      "training loss for step: 4.164466857910156\n",
      "training loss for step: 4.1635637283325195\n",
      "training loss for step: 4.16433572769165\n",
      "training loss for step: 4.168861389160156\n",
      "training loss for step: 4.1644086837768555\n",
      "training loss for step: 4.164488792419434\n",
      "training loss for step: 4.164400100708008\n",
      "training loss for step: 4.165048599243164\n",
      "training loss for step: 4.164400577545166\n",
      "training loss for step: 4.163708209991455\n",
      "training loss for step: 4.164780139923096\n",
      "training loss for step: 4.164233207702637\n",
      "training loss for step: 4.1641130447387695\n",
      "training loss for step: 4.1641526222229\n",
      "training loss for step: 4.164035320281982\n",
      "training loss for step: 4.165859699249268\n",
      "training loss for step: 4.165430545806885\n",
      "training loss for step: 4.167239665985107\n",
      "training loss for step: 4.1648173332214355\n",
      "training loss for step: 4.164974212646484\n",
      "training loss for step: 4.1689043045043945\n",
      "training loss for step: 4.166092872619629\n",
      "training loss for step: 4.1638665199279785\n",
      "training loss for step: 4.165449619293213\n",
      "training loss for step: 4.164810657501221\n",
      "training loss for step: 4.164626598358154\n",
      "training loss for step: 4.164328575134277\n",
      "training loss for step: 4.1661224365234375\n",
      "training loss for step: 4.166353225708008\n",
      "training loss for step: 4.166123390197754\n",
      "training loss for step: 4.166963577270508\n",
      "training loss for step: 4.164085865020752\n",
      "training loss for step: 4.166818141937256\n",
      "training loss for step: 4.168075084686279\n",
      "training loss for step: 4.170456409454346\n",
      "training loss for step: 4.169673919677734\n",
      "training loss for step: 4.16694974899292\n",
      "training loss for step: 4.1635966300964355\n",
      "training loss for step: 4.166946887969971\n",
      "training loss for step: 4.1693501472473145\n",
      "training loss for step: 4.16970157623291\n",
      "training loss for step: 4.16848087310791\n",
      "training loss for step: 4.166116237640381\n",
      "training loss for step: 4.163572311401367\n",
      "training loss for step: 4.165879249572754\n",
      "training loss for step: 4.165350914001465\n",
      "training loss for step: 4.167052745819092\n",
      "training loss for step: 4.165338516235352\n",
      "training loss for step: 4.166505336761475\n",
      "training loss for step: 4.1657490730285645\n",
      "training loss for step: 4.1649699211120605\n",
      "training loss for step: 4.164590358734131\n",
      "training loss for step: 4.164129257202148\n",
      "training loss for step: 4.165355682373047\n",
      "training loss for step: 4.166443824768066\n",
      "training loss for step: 4.165609836578369\n",
      "training loss for step: 4.163646221160889\n",
      "training loss for step: 4.1653900146484375\n",
      "training loss for step: 4.164184093475342\n",
      "training loss for step: 4.1639790534973145\n",
      "training loss for step: 4.164086818695068\n",
      "training loss for step: 4.166066646575928\n",
      "training loss for step: 4.1644792556762695\n",
      "training loss for step: 4.16440486907959\n",
      "training loss for step: 4.166491508483887\n",
      "training loss for step: 4.164555549621582\n",
      "training loss for step: 4.1639084815979\n",
      "training loss for step: 4.164792537689209\n",
      "training loss for step: 4.163665294647217\n",
      "training loss for step: 4.164896011352539\n",
      "training loss for step: 4.163736343383789\n",
      "training loss for step: 4.163853168487549\n",
      "training loss for step: 4.165687561035156\n",
      "training loss for step: 4.164969444274902\n",
      "training loss for step: 4.164402484893799\n",
      "training loss for step: 4.165490627288818\n",
      "training loss for step: 4.1666178703308105\n",
      "training loss for step: 4.165564060211182\n",
      "training loss for step: 4.164374351501465\n",
      "training loss for step: 4.164219379425049\n",
      "training loss for step: 4.164969444274902\n",
      "training loss for step: 4.16400671005249\n",
      "training loss for step: 4.163753986358643\n",
      "training loss for step: 4.166502475738525\n",
      "training loss for step: 4.166573524475098\n",
      "training loss for step: 4.1664958000183105\n",
      "training loss for step: 4.165659427642822\n",
      "training loss for step: 4.16349458694458\n",
      "training loss for step: 4.165950298309326\n",
      "training loss for step: 4.167673587799072\n",
      "training loss for step: 4.164492607116699\n",
      "training loss for step: 4.16541051864624\n",
      "training loss for step: 4.16387414932251\n",
      "training loss for step: 4.1653008460998535\n",
      "training loss for step: 4.165703296661377\n",
      "training loss for step: 4.168344020843506\n",
      "training loss for step: 4.166265487670898\n",
      "training loss for step: 4.164629936218262\n",
      "training loss for step: 4.1638336181640625\n",
      "training loss for step: 4.1653313636779785\n",
      "training loss for step: 4.165201663970947\n",
      "training loss for step: 4.163504123687744\n",
      "training loss for step: 4.165344715118408\n",
      "training loss for step: 4.1639251708984375\n",
      "training loss for step: 4.163679122924805\n",
      "training loss for step: 4.163957118988037\n",
      "training loss for step: 4.166175365447998\n",
      "training loss for step: 4.164705753326416\n",
      "training loss for step: 4.167595386505127\n",
      "training loss for step: 4.164338111877441\n",
      "training loss for step: 4.1640400886535645\n",
      "training loss for step: 4.163822174072266\n",
      "training loss for step: 4.165740489959717\n",
      "training loss for step: 4.1643476486206055\n",
      "training loss for step: 4.165437698364258\n",
      "training loss for step: 4.165225028991699\n",
      "training loss for step: 4.164308547973633\n",
      "training loss for step: 4.1645355224609375\n",
      "training loss for step: 4.165484428405762\n",
      "training loss for step: 4.1653571128845215\n",
      "training loss for step: 4.163778305053711\n",
      "training loss for step: 4.1646833419799805\n",
      "training loss for step: 4.164214611053467\n",
      "training loss for step: 4.165441513061523\n",
      "training loss for step: 4.164804458618164\n",
      "training loss for step: 4.165354251861572\n",
      "training loss for step: 4.1639862060546875\n",
      "training loss for step: 4.164787769317627\n",
      "training loss for step: 4.1668524742126465\n",
      "training loss for step: 4.164158344268799\n",
      "training loss for step: 4.165279388427734\n",
      "training loss for step: 4.168642044067383\n",
      "training loss for step: 4.16578483581543\n",
      "training loss for step: 4.163668155670166\n",
      "training loss for step: 4.165900230407715\n",
      "training loss for step: 4.166147708892822\n",
      "training loss for step: 4.164007663726807\n",
      "training loss for step: 4.164984703063965\n",
      "training loss for step: 4.165844440460205\n",
      "training loss for step: 4.165063858032227\n",
      "training loss for step: 4.165624141693115\n",
      "training loss for step: 4.163541793823242\n",
      "training loss for step: 4.1669511795043945\n",
      "training loss for step: 4.166016101837158\n",
      "training loss for step: 4.164631366729736\n",
      "training loss for step: 4.16503381729126\n",
      "training loss for step: 4.164984703063965\n",
      "training loss for step: 4.166452884674072\n",
      "training loss for step: 4.167671203613281\n",
      "training loss for step: 4.16574764251709\n",
      "training loss for step: 4.164078712463379\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f07ecbd4204b628aa8296e82adfdd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd6b2b9972db4979b7ffad15b5456ea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/447 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss for step: 4.264461517333984\n",
      "training loss for step: 4.263628005981445\n",
      "training loss for step: 4.2634758949279785\n",
      "training loss for step: 4.26606559753418\n",
      "training loss for step: 4.264707565307617\n",
      "training loss for step: 4.263925552368164\n",
      "training loss for step: 4.2642693519592285\n",
      "training loss for step: 4.264338493347168\n",
      "training loss for step: 4.26696252822876\n",
      "training loss for step: 4.265182971954346\n",
      "training loss for step: 4.263852119445801\n",
      "training loss for step: 4.263572692871094\n",
      "training loss for step: 4.265926361083984\n",
      "training loss for step: 4.26668119430542\n",
      "training loss for step: 4.268243789672852\n",
      "training loss for step: 4.266839027404785\n",
      "training loss for step: 4.264459609985352\n",
      "training loss for step: 4.265568256378174\n",
      "training loss for step: 4.2673020362854\n",
      "training loss for step: 4.266392707824707\n",
      "training loss for step: 4.264991760253906\n",
      "training loss for step: 4.264284133911133\n",
      "training loss for step: 4.2648725509643555\n",
      "training loss for step: 4.266934394836426\n",
      "training loss for step: 4.265641212463379\n",
      "training loss for step: 4.264854907989502\n",
      "training loss for step: 4.264272689819336\n",
      "training loss for step: 4.26378870010376\n",
      "training loss for step: 4.263548851013184\n",
      "training loss for step: 4.2637715339660645\n",
      "training loss for step: 4.263767242431641\n",
      "training loss for step: 4.265578269958496\n",
      "training loss for step: 4.264909744262695\n",
      "training loss for step: 4.266031265258789\n",
      "training loss for step: 4.264010906219482\n",
      "training loss for step: 4.266301155090332\n",
      "training loss for step: 4.265593528747559\n",
      "training loss for step: 4.264105319976807\n",
      "training loss for step: 4.265294075012207\n",
      "training loss for step: 4.26743745803833\n",
      "training loss for step: 4.2658843994140625\n",
      "training loss for step: 4.267699241638184\n",
      "training loss for step: 4.266336441040039\n",
      "training loss for step: 4.266119956970215\n",
      "training loss for step: 4.263642311096191\n",
      "training loss for step: 4.267337799072266\n",
      "training loss for step: 4.269792079925537\n",
      "training loss for step: 4.269705295562744\n",
      "training loss for step: 4.269989967346191\n",
      "training loss for step: 4.2690815925598145\n",
      "training loss for step: 4.267086982727051\n",
      "training loss for step: 4.2647175788879395\n",
      "training loss for step: 4.266473770141602\n",
      "training loss for step: 4.266928195953369\n",
      "training loss for step: 4.269560813903809\n",
      "training loss for step: 4.267304420471191\n",
      "training loss for step: 4.268022060394287\n",
      "training loss for step: 4.26537561416626\n",
      "training loss for step: 4.263686180114746\n",
      "training loss for step: 4.266663551330566\n",
      "training loss for step: 4.2685227394104\n",
      "training loss for step: 4.269448280334473\n",
      "training loss for step: 4.2698493003845215\n",
      "training loss for step: 4.2684550285339355\n",
      "training loss for step: 4.267538547515869\n",
      "training loss for step: 4.264140605926514\n",
      "training loss for step: 4.267213344573975\n",
      "training loss for step: 4.2677531242370605\n",
      "training loss for step: 4.268855094909668\n",
      "training loss for step: 4.268898010253906\n",
      "training loss for step: 4.26713228225708\n",
      "training loss for step: 4.266259670257568\n",
      "training loss for step: 4.263829231262207\n",
      "training loss for step: 4.2646942138671875\n",
      "training loss for step: 4.265039443969727\n",
      "training loss for step: 4.2646660804748535\n",
      "training loss for step: 4.263729095458984\n",
      "training loss for step: 4.265912055969238\n",
      "training loss for step: 4.263746738433838\n",
      "training loss for step: 4.263996601104736\n",
      "training loss for step: 4.2656636238098145\n",
      "training loss for step: 4.265382766723633\n",
      "training loss for step: 4.264542579650879\n",
      "training loss for step: 4.264144420623779\n",
      "training loss for step: 4.266364574432373\n",
      "training loss for step: 4.26570463180542\n",
      "training loss for step: 4.264176845550537\n",
      "training loss for step: 4.264601230621338\n",
      "training loss for step: 4.2663116455078125\n",
      "training loss for step: 4.265146732330322\n",
      "training loss for step: 4.2662529945373535\n",
      "training loss for step: 4.265995025634766\n",
      "training loss for step: 4.264842510223389\n",
      "training loss for step: 4.264889240264893\n",
      "training loss for step: 4.2663373947143555\n",
      "training loss for step: 4.266352653503418\n",
      "training loss for step: 4.265327453613281\n",
      "training loss for step: 4.264464378356934\n",
      "training loss for step: 4.265028953552246\n",
      "training loss for step: 4.26701021194458\n",
      "training loss for step: 4.266096591949463\n",
      "training loss for step: 4.266244888305664\n",
      "training loss for step: 4.264007091522217\n",
      "training loss for step: 4.264142990112305\n",
      "training loss for step: 4.263759613037109\n",
      "training loss for step: 4.263625621795654\n",
      "training loss for step: 4.267261505126953\n",
      "training loss for step: 4.269455909729004\n",
      "training loss for step: 4.267704010009766\n",
      "training loss for step: 4.268282413482666\n",
      "training loss for step: 4.265458583831787\n",
      "training loss for step: 4.263474941253662\n",
      "training loss for step: 4.26563024520874\n",
      "training loss for step: 4.266468524932861\n",
      "training loss for step: 4.266843795776367\n",
      "training loss for step: 4.2652812004089355\n",
      "training loss for step: 4.263837814331055\n",
      "training loss for step: 4.264688014984131\n",
      "training loss for step: 4.266083240509033\n",
      "training loss for step: 4.266575336456299\n",
      "training loss for step: 4.266825199127197\n",
      "training loss for step: 4.2660698890686035\n",
      "training loss for step: 4.263827800750732\n",
      "training loss for step: 4.264451026916504\n",
      "training loss for step: 4.263482570648193\n",
      "training loss for step: 4.265251159667969\n",
      "training loss for step: 4.264360427856445\n",
      "training loss for step: 4.2638044357299805\n",
      "training loss for step: 4.264421463012695\n",
      "training loss for step: 4.263651371002197\n",
      "training loss for step: 4.265189170837402\n",
      "training loss for step: 4.265538215637207\n",
      "training loss for step: 4.264024257659912\n",
      "training loss for step: 4.2636542320251465\n",
      "training loss for step: 4.266435623168945\n",
      "training loss for step: 4.265731334686279\n",
      "training loss for step: 4.266012191772461\n",
      "training loss for step: 4.266202449798584\n",
      "training loss for step: 4.263956069946289\n",
      "training loss for step: 4.265944480895996\n",
      "training loss for step: 4.26603364944458\n",
      "training loss for step: 4.266696929931641\n",
      "training loss for step: 4.267051696777344\n",
      "training loss for step: 4.264548301696777\n",
      "training loss for step: 4.264650821685791\n",
      "training loss for step: 4.26448392868042\n",
      "training loss for step: 4.265819072723389\n",
      "training loss for step: 4.264021396636963\n",
      "training loss for step: 4.263566017150879\n",
      "training loss for step: 4.264971733093262\n",
      "training loss for step: 4.264227390289307\n",
      "training loss for step: 4.2637248039245605\n",
      "training loss for step: 4.263993263244629\n",
      "training loss for step: 4.264097213745117\n",
      "training loss for step: 4.264126300811768\n",
      "training loss for step: 4.263751983642578\n",
      "training loss for step: 4.264439582824707\n",
      "training loss for step: 4.264052391052246\n",
      "training loss for step: 4.2642598152160645\n",
      "training loss for step: 4.264438629150391\n",
      "training loss for step: 4.264335632324219\n",
      "training loss for step: 4.264007568359375\n",
      "training loss for step: 4.264538764953613\n",
      "training loss for step: 4.263812065124512\n",
      "training loss for step: 4.265022277832031\n",
      "training loss for step: 4.263697624206543\n",
      "training loss for step: 4.265893936157227\n",
      "training loss for step: 4.266426086425781\n",
      "training loss for step: 4.266570091247559\n",
      "training loss for step: 4.265382289886475\n",
      "training loss for step: 4.26354455947876\n",
      "training loss for step: 4.26483678817749\n",
      "training loss for step: 4.266384124755859\n",
      "training loss for step: 4.266967296600342\n",
      "training loss for step: 4.2661004066467285\n",
      "training loss for step: 4.264345645904541\n",
      "training loss for step: 4.263603687286377\n",
      "training loss for step: 4.264950752258301\n",
      "training loss for step: 4.265385150909424\n",
      "training loss for step: 4.2639241218566895\n",
      "training loss for step: 4.263654708862305\n",
      "training loss for step: 4.2638044357299805\n",
      "training loss for step: 4.263898849487305\n",
      "training loss for step: 4.264340877532959\n",
      "training loss for step: 4.264501571655273\n",
      "training loss for step: 4.263660907745361\n",
      "training loss for step: 4.26387357711792\n",
      "training loss for step: 4.265857219696045\n",
      "training loss for step: 4.26584529876709\n",
      "training loss for step: 4.263859272003174\n",
      "training loss for step: 4.26396369934082\n",
      "training loss for step: 4.265115261077881\n",
      "training loss for step: 4.265081882476807\n",
      "training loss for step: 4.264523506164551\n",
      "training loss for step: 4.263561725616455\n",
      "training loss for step: 4.265602111816406\n",
      "training loss for step: 4.268052101135254\n",
      "training loss for step: 4.266721248626709\n",
      "training loss for step: 4.266269207000732\n",
      "training loss for step: 4.264593601226807\n",
      "training loss for step: 4.263486385345459\n",
      "training loss for step: 4.265023708343506\n",
      "training loss for step: 4.265958309173584\n",
      "training loss for step: 4.265473365783691\n",
      "training loss for step: 4.263594627380371\n",
      "training loss for step: 4.264556884765625\n",
      "training loss for step: 4.265615940093994\n",
      "training loss for step: 4.264736652374268\n",
      "training loss for step: 4.267168998718262\n",
      "training loss for step: 4.2660346031188965\n",
      "training loss for step: 4.263561725616455\n",
      "training loss for step: 4.263901233673096\n",
      "training loss for step: 4.265385150909424\n",
      "training loss for step: 4.264195919036865\n",
      "training loss for step: 4.264883041381836\n",
      "training loss for step: 4.265867233276367\n",
      "training loss for step: 4.264835357666016\n",
      "training loss for step: 4.263489723205566\n",
      "training loss for step: 4.264917850494385\n",
      "training loss for step: 4.2661285400390625\n",
      "training loss for step: 4.264917850494385\n",
      "training loss for step: 4.2667155265808105\n",
      "training loss for step: 4.265969276428223\n",
      "training loss for step: 4.266515254974365\n",
      "training loss for step: 4.2661848068237305\n",
      "training loss for step: 4.26752233505249\n",
      "training loss for step: 4.265246868133545\n",
      "training loss for step: 4.265630722045898\n",
      "training loss for step: 4.263875484466553\n",
      "training loss for step: 4.26465368270874\n",
      "training loss for step: 4.265926361083984\n",
      "training loss for step: 4.264958381652832\n",
      "training loss for step: 4.263781547546387\n",
      "training loss for step: 4.2661967277526855\n",
      "training loss for step: 4.265398979187012\n",
      "training loss for step: 4.266043186187744\n",
      "training loss for step: 4.265952110290527\n",
      "training loss for step: 4.264808177947998\n",
      "training loss for step: 4.264675617218018\n",
      "training loss for step: 4.264932155609131\n",
      "training loss for step: 4.265570640563965\n",
      "training loss for step: 4.263892650604248\n",
      "training loss for step: 4.263890266418457\n",
      "training loss for step: 4.265205383300781\n",
      "training loss for step: 4.265593528747559\n",
      "training loss for step: 4.265356063842773\n",
      "training loss for step: 4.266164779663086\n",
      "training loss for step: 4.264649868011475\n",
      "training loss for step: 4.263981819152832\n",
      "training loss for step: 4.2661027908325195\n",
      "training loss for step: 4.265712261199951\n",
      "training loss for step: 4.265048980712891\n",
      "training loss for step: 4.264790058135986\n",
      "training loss for step: 4.2659687995910645\n",
      "training loss for step: 4.267262935638428\n",
      "training loss for step: 4.266992092132568\n",
      "training loss for step: 4.266241550445557\n",
      "training loss for step: 4.264988899230957\n",
      "training loss for step: 4.263463973999023\n",
      "training loss for step: 4.265822887420654\n",
      "training loss for step: 4.266612529754639\n",
      "training loss for step: 4.2649617195129395\n",
      "training loss for step: 4.264827251434326\n",
      "training loss for step: 4.26413631439209\n",
      "training loss for step: 4.264730453491211\n",
      "training loss for step: 4.263890266418457\n",
      "training loss for step: 4.263498306274414\n",
      "training loss for step: 4.26472806930542\n",
      "training loss for step: 4.264073848724365\n",
      "training loss for step: 4.265519142150879\n",
      "training loss for step: 4.2660136222839355\n",
      "training loss for step: 4.2663679122924805\n",
      "training loss for step: 4.266254901885986\n",
      "training loss for step: 4.263575077056885\n",
      "training loss for step: 4.264073371887207\n",
      "training loss for step: 4.264784812927246\n",
      "training loss for step: 4.263582706451416\n",
      "training loss for step: 4.2650322914123535\n",
      "training loss for step: 4.266739368438721\n",
      "training loss for step: 4.264896392822266\n",
      "training loss for step: 4.264698505401611\n",
      "training loss for step: 4.264815330505371\n",
      "training loss for step: 4.263929843902588\n",
      "training loss for step: 4.265050411224365\n",
      "training loss for step: 4.264133930206299\n",
      "training loss for step: 4.264854431152344\n",
      "training loss for step: 4.264612197875977\n",
      "training loss for step: 4.265335559844971\n",
      "training loss for step: 4.264642238616943\n",
      "training loss for step: 4.2642669677734375\n",
      "training loss for step: 4.265507221221924\n",
      "training loss for step: 4.264401912689209\n",
      "training loss for step: 4.263864517211914\n",
      "training loss for step: 4.264667987823486\n",
      "training loss for step: 4.264443397521973\n",
      "training loss for step: 4.264578819274902\n",
      "training loss for step: 4.264540195465088\n",
      "training loss for step: 4.26372766494751\n",
      "training loss for step: 4.264657974243164\n",
      "training loss for step: 4.265227794647217\n",
      "training loss for step: 4.264648914337158\n",
      "training loss for step: 4.265661239624023\n",
      "training loss for step: 4.26667594909668\n",
      "training loss for step: 4.2665276527404785\n",
      "training loss for step: 4.263563632965088\n",
      "training loss for step: 4.263869762420654\n",
      "training loss for step: 4.265160083770752\n",
      "training loss for step: 4.265226364135742\n",
      "training loss for step: 4.263744831085205\n",
      "training loss for step: 4.2634735107421875\n",
      "training loss for step: 4.264466762542725\n",
      "training loss for step: 4.2650227546691895\n",
      "training loss for step: 4.263523101806641\n",
      "training loss for step: 4.263561725616455\n",
      "training loss for step: 4.264333248138428\n",
      "training loss for step: 4.264896392822266\n",
      "training loss for step: 4.263481616973877\n",
      "training loss for step: 4.264133453369141\n",
      "training loss for step: 4.263892650604248\n",
      "training loss for step: 4.264392375946045\n",
      "training loss for step: 4.265230178833008\n",
      "training loss for step: 4.265192985534668\n",
      "training loss for step: 4.263739585876465\n",
      "training loss for step: 4.264708995819092\n",
      "training loss for step: 4.264857769012451\n",
      "training loss for step: 4.26523494720459\n",
      "training loss for step: 4.263962268829346\n",
      "training loss for step: 4.264300346374512\n",
      "training loss for step: 4.264333248138428\n",
      "training loss for step: 4.263902187347412\n",
      "training loss for step: 4.264026165008545\n",
      "training loss for step: 4.263736248016357\n",
      "training loss for step: 4.264241695404053\n",
      "training loss for step: 4.263896465301514\n",
      "training loss for step: 4.2640380859375\n",
      "training loss for step: 4.26347017288208\n",
      "training loss for step: 4.263518333435059\n",
      "training loss for step: 4.2645039558410645\n",
      "training loss for step: 4.265091896057129\n",
      "training loss for step: 4.263570785522461\n",
      "training loss for step: 4.264037609100342\n",
      "training loss for step: 4.263513088226318\n",
      "training loss for step: 4.263692378997803\n",
      "training loss for step: 4.264808177947998\n",
      "training loss for step: 4.2643656730651855\n",
      "training loss for step: 4.264098167419434\n",
      "training loss for step: 4.265834331512451\n",
      "training loss for step: 4.265904903411865\n",
      "training loss for step: 4.265074729919434\n",
      "training loss for step: 4.265598297119141\n",
      "training loss for step: 4.2641167640686035\n",
      "training loss for step: 4.264925479888916\n",
      "training loss for step: 4.265666484832764\n",
      "training loss for step: 4.267492294311523\n",
      "training loss for step: 4.266440391540527\n",
      "training loss for step: 4.264626979827881\n",
      "training loss for step: 4.264034271240234\n",
      "training loss for step: 4.265077590942383\n",
      "training loss for step: 4.26602840423584\n",
      "training loss for step: 4.263925075531006\n",
      "training loss for step: 4.264503479003906\n",
      "training loss for step: 4.266386985778809\n",
      "training loss for step: 4.266312599182129\n",
      "training loss for step: 4.267076015472412\n",
      "training loss for step: 4.2658538818359375\n",
      "training loss for step: 4.264549255371094\n",
      "training loss for step: 4.263871192932129\n",
      "training loss for step: 4.264072418212891\n",
      "training loss for step: 4.2656965255737305\n",
      "training loss for step: 4.26401424407959\n",
      "training loss for step: 4.263581275939941\n",
      "training loss for step: 4.264190673828125\n",
      "training loss for step: 4.264281272888184\n",
      "training loss for step: 4.2644267082214355\n",
      "training loss for step: 4.263635158538818\n",
      "training loss for step: 4.2639994621276855\n",
      "training loss for step: 4.263878345489502\n",
      "training loss for step: 4.264181137084961\n",
      "training loss for step: 4.2635016441345215\n",
      "training loss for step: 4.264138221740723\n",
      "training loss for step: 4.263962745666504\n",
      "training loss for step: 4.263926029205322\n",
      "training loss for step: 4.265710830688477\n",
      "training loss for step: 4.265555381774902\n",
      "training loss for step: 4.264489650726318\n",
      "training loss for step: 4.263767242431641\n",
      "training loss for step: 4.263632774353027\n",
      "training loss for step: 4.264629364013672\n",
      "training loss for step: 4.265721321105957\n",
      "training loss for step: 4.264652729034424\n",
      "training loss for step: 4.264283657073975\n",
      "training loss for step: 4.264545917510986\n",
      "training loss for step: 4.264297008514404\n",
      "training loss for step: 4.264301300048828\n",
      "training loss for step: 4.264167308807373\n",
      "training loss for step: 4.263643741607666\n",
      "training loss for step: 4.2640509605407715\n",
      "training loss for step: 4.264406681060791\n",
      "training loss for step: 4.26422643661499\n",
      "training loss for step: 4.263483047485352\n",
      "training loss for step: 4.2655415534973145\n",
      "training loss for step: 4.265276908874512\n",
      "training loss for step: 4.263889789581299\n",
      "training loss for step: 4.264064788818359\n",
      "training loss for step: 4.263553142547607\n",
      "training loss for step: 4.2651824951171875\n",
      "training loss for step: 4.263606548309326\n",
      "training loss for step: 4.264501094818115\n",
      "training loss for step: 4.263831615447998\n",
      "training loss for step: 4.264090538024902\n",
      "training loss for step: 4.264023303985596\n",
      "training loss for step: 4.2637939453125\n",
      "training loss for step: 4.263612747192383\n",
      "training loss for step: 4.264181137084961\n",
      "training loss for step: 4.264536380767822\n",
      "training loss for step: 4.264471054077148\n",
      "training loss for step: 4.263669490814209\n",
      "training loss for step: 4.264926433563232\n",
      "training loss for step: 4.264646530151367\n",
      "training loss for step: 4.264406204223633\n",
      "training loss for step: 4.2641706466674805\n",
      "training loss for step: 4.264636516571045\n",
      "training loss for step: 4.265411376953125\n",
      "training loss for step: 4.266456127166748\n",
      "training loss for step: 4.264438629150391\n",
      "training loss for step: 4.263473987579346\n",
      "training loss for step: 4.264317512512207\n",
      "training loss for step: 4.265591144561768\n",
      "training loss for step: 4.2644171714782715\n",
      "training loss for step: 4.26408576965332\n",
      "training loss for step: 4.263985633850098\n",
      "training loss for step: 4.265108108520508\n",
      "training loss for step: 4.264231204986572\n",
      "training loss for step: 4.26389741897583\n",
      "training loss for step: 4.264472007751465\n",
      "training loss for step: 4.265416622161865\n",
      "training loss for step: 4.2645955085754395\n",
      "training loss for step: 4.264326572418213\n",
      "training loss for step: 4.264123916625977\n",
      "training loss for step: 4.264437198638916\n",
      "training loss for step: 4.264422416687012\n",
      "training loss for step: 4.264457702636719\n",
      "training loss for step: 4.265100479125977\n",
      "training loss for step: 4.265683174133301\n",
      "training loss for step: 4.26488733291626\n",
      "training loss for step: 4.263527870178223\n",
      "training loss for step: 4.2635111808776855\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "311937e002954762a0467c3e3bf2eb9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0220ffb3e36e4c75b6ed3e55c24e4c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/447 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss for step: 4.363918304443359\n",
      "training loss for step: 4.364914417266846\n",
      "training loss for step: 4.363515853881836\n",
      "training loss for step: 4.364448547363281\n",
      "training loss for step: 4.363704204559326\n",
      "training loss for step: 4.364945888519287\n",
      "training loss for step: 4.365971565246582\n",
      "training loss for step: 4.364068031311035\n",
      "training loss for step: 4.364679336547852\n",
      "training loss for step: 4.36538028717041\n",
      "training loss for step: 4.365172386169434\n",
      "training loss for step: 4.365016937255859\n",
      "training loss for step: 4.364113807678223\n",
      "training loss for step: 4.363622665405273\n",
      "training loss for step: 4.36552095413208\n",
      "training loss for step: 4.3666229248046875\n",
      "training loss for step: 4.367058753967285\n",
      "training loss for step: 4.366591453552246\n",
      "training loss for step: 4.366384029388428\n",
      "training loss for step: 4.364079475402832\n",
      "training loss for step: 4.3650221824646\n",
      "training loss for step: 4.36622953414917\n",
      "training loss for step: 4.367237567901611\n",
      "training loss for step: 4.368281841278076\n",
      "training loss for step: 4.366166591644287\n",
      "training loss for step: 4.364648342132568\n",
      "training loss for step: 4.36396598815918\n",
      "training loss for step: 4.36494255065918\n",
      "training loss for step: 4.366150856018066\n",
      "training loss for step: 4.364274501800537\n",
      "training loss for step: 4.365139484405518\n",
      "training loss for step: 4.36357307434082\n",
      "training loss for step: 4.366488933563232\n",
      "training loss for step: 4.367844104766846\n",
      "training loss for step: 4.368376731872559\n",
      "training loss for step: 4.367130756378174\n",
      "training loss for step: 4.366101264953613\n",
      "training loss for step: 4.366415977478027\n",
      "training loss for step: 4.36478328704834\n",
      "training loss for step: 4.364953517913818\n",
      "training loss for step: 4.365283489227295\n",
      "training loss for step: 4.365246295928955\n",
      "training loss for step: 4.3642706871032715\n",
      "training loss for step: 4.363801956176758\n",
      "training loss for step: 4.363921165466309\n",
      "training loss for step: 4.364546775817871\n",
      "training loss for step: 4.363885879516602\n",
      "training loss for step: 4.364351272583008\n",
      "training loss for step: 4.365705490112305\n",
      "training loss for step: 4.36529541015625\n",
      "training loss for step: 4.364343643188477\n",
      "training loss for step: 4.365637302398682\n",
      "training loss for step: 4.365191459655762\n",
      "training loss for step: 4.365058422088623\n",
      "training loss for step: 4.364712715148926\n",
      "training loss for step: 4.363565444946289\n",
      "training loss for step: 4.364306449890137\n",
      "training loss for step: 4.365371227264404\n",
      "training loss for step: 4.365541458129883\n",
      "training loss for step: 4.363513469696045\n",
      "training loss for step: 4.364343166351318\n",
      "training loss for step: 4.364365100860596\n",
      "training loss for step: 4.364985466003418\n",
      "training loss for step: 4.364666938781738\n",
      "training loss for step: 4.365792751312256\n",
      "training loss for step: 4.363956928253174\n",
      "training loss for step: 4.364821910858154\n",
      "training loss for step: 4.364292621612549\n",
      "training loss for step: 4.364784240722656\n",
      "training loss for step: 4.363944053649902\n",
      "training loss for step: 4.363580703735352\n",
      "training loss for step: 4.364475250244141\n",
      "training loss for step: 4.363789081573486\n",
      "training loss for step: 4.363816261291504\n",
      "training loss for step: 4.364661693572998\n",
      "training loss for step: 4.363838195800781\n",
      "training loss for step: 4.364243507385254\n",
      "training loss for step: 4.364136219024658\n",
      "training loss for step: 4.365964889526367\n",
      "training loss for step: 4.364663124084473\n",
      "training loss for step: 4.36350154876709\n",
      "training loss for step: 4.363731384277344\n",
      "training loss for step: 4.363742351531982\n",
      "training loss for step: 4.3649001121521\n",
      "training loss for step: 4.364379405975342\n",
      "training loss for step: 4.363933086395264\n",
      "training loss for step: 4.364377498626709\n",
      "training loss for step: 4.364422798156738\n",
      "training loss for step: 4.365020275115967\n",
      "training loss for step: 4.365574836730957\n",
      "training loss for step: 4.36417293548584\n",
      "training loss for step: 4.363955020904541\n",
      "training loss for step: 4.363724231719971\n",
      "training loss for step: 4.363831520080566\n",
      "training loss for step: 4.364140510559082\n",
      "training loss for step: 4.363891124725342\n",
      "training loss for step: 4.364134311676025\n",
      "training loss for step: 4.363973140716553\n",
      "training loss for step: 4.3640522956848145\n",
      "training loss for step: 4.363877296447754\n",
      "training loss for step: 4.364260196685791\n",
      "training loss for step: 4.364513874053955\n",
      "training loss for step: 4.363565921783447\n",
      "training loss for step: 4.364116668701172\n",
      "training loss for step: 4.363870620727539\n",
      "training loss for step: 4.364297389984131\n",
      "training loss for step: 4.365482807159424\n",
      "training loss for step: 4.3654937744140625\n",
      "training loss for step: 4.363987922668457\n",
      "training loss for step: 4.364039897918701\n",
      "training loss for step: 4.365466117858887\n",
      "training loss for step: 4.366138935089111\n",
      "training loss for step: 4.3645100593566895\n",
      "training loss for step: 4.363623142242432\n",
      "training loss for step: 4.364058494567871\n",
      "training loss for step: 4.367420673370361\n",
      "training loss for step: 4.367215633392334\n",
      "training loss for step: 4.364875316619873\n",
      "training loss for step: 4.364775657653809\n",
      "training loss for step: 4.364259243011475\n",
      "training loss for step: 4.365505218505859\n",
      "training loss for step: 4.367883682250977\n",
      "training loss for step: 4.3680500984191895\n",
      "training loss for step: 4.366526126861572\n",
      "training loss for step: 4.366291522979736\n",
      "training loss for step: 4.3646416664123535\n",
      "training loss for step: 4.363594055175781\n",
      "training loss for step: 4.366702556610107\n",
      "training loss for step: 4.367604732513428\n",
      "training loss for step: 4.368567943572998\n",
      "training loss for step: 4.368101119995117\n",
      "training loss for step: 4.368259906768799\n",
      "training loss for step: 4.3664655685424805\n",
      "training loss for step: 4.363791465759277\n",
      "training loss for step: 4.364227294921875\n",
      "training loss for step: 4.365504741668701\n",
      "training loss for step: 4.366317272186279\n",
      "training loss for step: 4.366533279418945\n",
      "training loss for step: 4.3650221824646\n",
      "training loss for step: 4.365543365478516\n",
      "training loss for step: 4.36533784866333\n",
      "training loss for step: 4.3649582862854\n",
      "training loss for step: 4.365501403808594\n",
      "training loss for step: 4.3653340339660645\n",
      "training loss for step: 4.364287853240967\n",
      "training loss for step: 4.3638715744018555\n",
      "training loss for step: 4.365388870239258\n",
      "training loss for step: 4.364233493804932\n",
      "training loss for step: 4.364378929138184\n",
      "training loss for step: 4.363659858703613\n",
      "training loss for step: 4.364089488983154\n",
      "training loss for step: 4.364062786102295\n",
      "training loss for step: 4.363842010498047\n",
      "training loss for step: 4.363953590393066\n",
      "training loss for step: 4.364009380340576\n",
      "training loss for step: 4.363555431365967\n",
      "training loss for step: 4.364038467407227\n",
      "training loss for step: 4.364327907562256\n",
      "training loss for step: 4.363952159881592\n",
      "training loss for step: 4.363597393035889\n",
      "training loss for step: 4.365784168243408\n",
      "training loss for step: 4.366584300994873\n",
      "training loss for step: 4.366790294647217\n",
      "training loss for step: 4.367136001586914\n",
      "training loss for step: 4.365055561065674\n",
      "training loss for step: 4.363521099090576\n",
      "training loss for step: 4.365301132202148\n",
      "training loss for step: 4.366091251373291\n",
      "training loss for step: 4.366257667541504\n",
      "training loss for step: 4.366522312164307\n",
      "training loss for step: 4.365250587463379\n",
      "training loss for step: 4.364137172698975\n",
      "training loss for step: 4.36473274230957\n",
      "training loss for step: 4.365493297576904\n",
      "training loss for step: 4.364778518676758\n",
      "training loss for step: 4.365506172180176\n",
      "training loss for step: 4.364731788635254\n",
      "training loss for step: 4.364128589630127\n",
      "training loss for step: 4.366037845611572\n",
      "training loss for step: 4.364684104919434\n",
      "training loss for step: 4.364370346069336\n",
      "training loss for step: 4.3636040687561035\n",
      "training loss for step: 4.365713596343994\n",
      "training loss for step: 4.366151809692383\n",
      "training loss for step: 4.365380764007568\n",
      "training loss for step: 4.3658447265625\n",
      "training loss for step: 4.365758895874023\n",
      "training loss for step: 4.363666534423828\n",
      "training loss for step: 4.365821361541748\n",
      "training loss for step: 4.363886833190918\n",
      "training loss for step: 4.363894939422607\n",
      "training loss for step: 4.363487720489502\n",
      "training loss for step: 4.36525821685791\n",
      "training loss for step: 4.3641252517700195\n",
      "training loss for step: 4.364726543426514\n",
      "training loss for step: 4.364920139312744\n",
      "training loss for step: 4.364744663238525\n",
      "training loss for step: 4.365761756896973\n",
      "training loss for step: 4.365095138549805\n",
      "training loss for step: 4.3644609451293945\n",
      "training loss for step: 4.364288330078125\n",
      "training loss for step: 4.364572525024414\n",
      "training loss for step: 4.363802433013916\n",
      "training loss for step: 4.363793849945068\n",
      "training loss for step: 4.365105628967285\n",
      "training loss for step: 4.365251064300537\n",
      "training loss for step: 4.364236354827881\n",
      "training loss for step: 4.364783763885498\n",
      "training loss for step: 4.365328788757324\n",
      "training loss for step: 4.36441707611084\n",
      "training loss for step: 4.363615036010742\n",
      "training loss for step: 4.364089012145996\n",
      "training loss for step: 4.365203857421875\n",
      "training loss for step: 4.363813877105713\n",
      "training loss for step: 4.364353179931641\n",
      "training loss for step: 4.3643646240234375\n",
      "training loss for step: 4.365072250366211\n",
      "training loss for step: 4.363968372344971\n",
      "training loss for step: 4.363980770111084\n",
      "training loss for step: 4.363816738128662\n",
      "training loss for step: 4.364770889282227\n",
      "training loss for step: 4.364748477935791\n",
      "training loss for step: 4.363839149475098\n",
      "training loss for step: 4.364479064941406\n",
      "training loss for step: 4.364049434661865\n",
      "training loss for step: 4.364172458648682\n",
      "training loss for step: 4.36424446105957\n",
      "training loss for step: 4.364902496337891\n",
      "training loss for step: 4.364908218383789\n",
      "training loss for step: 4.364624977111816\n",
      "training loss for step: 4.364531993865967\n",
      "training loss for step: 4.364207744598389\n",
      "training loss for step: 4.365124225616455\n",
      "training loss for step: 4.3643903732299805\n",
      "training loss for step: 4.363839626312256\n",
      "training loss for step: 4.363478183746338\n",
      "training loss for step: 4.364317893981934\n",
      "training loss for step: 4.36368465423584\n",
      "training loss for step: 4.364015102386475\n",
      "training loss for step: 4.364614486694336\n",
      "training loss for step: 4.364965438842773\n",
      "training loss for step: 4.364788055419922\n",
      "training loss for step: 4.364558219909668\n",
      "training loss for step: 4.364293575286865\n",
      "training loss for step: 4.3646368980407715\n",
      "training loss for step: 4.364531517028809\n",
      "training loss for step: 4.363511085510254\n",
      "training loss for step: 4.364567756652832\n",
      "training loss for step: 4.363945960998535\n",
      "training loss for step: 4.365518569946289\n",
      "training loss for step: 4.364687442779541\n",
      "training loss for step: 4.365550518035889\n",
      "training loss for step: 4.36476469039917\n",
      "training loss for step: 4.365250110626221\n",
      "training loss for step: 4.364266872406006\n",
      "training loss for step: 4.364641189575195\n",
      "training loss for step: 4.3640546798706055\n",
      "training loss for step: 4.364363193511963\n",
      "training loss for step: 4.364326477050781\n",
      "training loss for step: 4.365177631378174\n",
      "training loss for step: 4.3634934425354\n",
      "training loss for step: 4.363637924194336\n",
      "training loss for step: 4.363558769226074\n",
      "training loss for step: 4.365001201629639\n",
      "training loss for step: 4.365573406219482\n",
      "training loss for step: 4.364730358123779\n",
      "training loss for step: 4.364163875579834\n",
      "training loss for step: 4.363583564758301\n",
      "training loss for step: 4.365461349487305\n",
      "training loss for step: 4.366317272186279\n",
      "training loss for step: 4.3670477867126465\n",
      "training loss for step: 4.3671555519104\n",
      "training loss for step: 4.365382194519043\n",
      "training loss for step: 4.364175796508789\n",
      "training loss for step: 4.364326477050781\n",
      "training loss for step: 4.364841938018799\n",
      "training loss for step: 4.365168571472168\n",
      "training loss for step: 4.366034984588623\n",
      "training loss for step: 4.364445686340332\n",
      "training loss for step: 4.363650798797607\n",
      "training loss for step: 4.364939212799072\n",
      "training loss for step: 4.364069938659668\n",
      "training loss for step: 4.364020824432373\n",
      "training loss for step: 4.364025592803955\n",
      "training loss for step: 4.365330696105957\n",
      "training loss for step: 4.365141868591309\n",
      "training loss for step: 4.364312171936035\n",
      "training loss for step: 4.363912582397461\n",
      "training loss for step: 4.364853382110596\n",
      "training loss for step: 4.364616870880127\n",
      "training loss for step: 4.363741397857666\n",
      "training loss for step: 4.363860607147217\n",
      "training loss for step: 4.364592552185059\n",
      "training loss for step: 4.364343166351318\n",
      "training loss for step: 4.363998889923096\n",
      "training loss for step: 4.364760398864746\n",
      "training loss for step: 4.363591194152832\n",
      "training loss for step: 4.364693641662598\n",
      "training loss for step: 4.363653182983398\n",
      "training loss for step: 4.364378929138184\n",
      "training loss for step: 4.365192890167236\n",
      "training loss for step: 4.364360332489014\n",
      "training loss for step: 4.364962100982666\n",
      "training loss for step: 4.364179611206055\n",
      "training loss for step: 4.36464786529541\n",
      "training loss for step: 4.364058017730713\n",
      "training loss for step: 4.363884925842285\n",
      "training loss for step: 4.364707946777344\n",
      "training loss for step: 4.364789009094238\n",
      "training loss for step: 4.36466121673584\n",
      "training loss for step: 4.3651580810546875\n",
      "training loss for step: 4.364231109619141\n",
      "training loss for step: 4.364222526550293\n",
      "training loss for step: 4.363472938537598\n",
      "training loss for step: 4.36422061920166\n",
      "training loss for step: 4.363575458526611\n",
      "training loss for step: 4.3636674880981445\n",
      "training loss for step: 4.363918304443359\n",
      "training loss for step: 4.3639817237854\n",
      "training loss for step: 4.36408805847168\n",
      "training loss for step: 4.364469528198242\n",
      "training loss for step: 4.363912582397461\n",
      "training loss for step: 4.3659563064575195\n",
      "training loss for step: 4.366677284240723\n",
      "training loss for step: 4.365464210510254\n",
      "training loss for step: 4.364350318908691\n",
      "training loss for step: 4.364219665527344\n",
      "training loss for step: 4.36362361907959\n",
      "training loss for step: 4.36350154876709\n",
      "training loss for step: 4.3634867668151855\n",
      "training loss for step: 4.364706993103027\n",
      "training loss for step: 4.364587306976318\n",
      "training loss for step: 4.363525867462158\n",
      "training loss for step: 4.364154815673828\n",
      "training loss for step: 4.363475322723389\n",
      "training loss for step: 4.364489555358887\n",
      "training loss for step: 4.364182472229004\n",
      "training loss for step: 4.363579273223877\n",
      "training loss for step: 4.364260673522949\n",
      "training loss for step: 4.364006042480469\n",
      "training loss for step: 4.364253044128418\n",
      "training loss for step: 4.364850997924805\n",
      "training loss for step: 4.3644328117370605\n",
      "training loss for step: 4.363839149475098\n",
      "training loss for step: 4.363720893859863\n",
      "training loss for step: 4.3634772300720215\n",
      "training loss for step: 4.363969326019287\n",
      "training loss for step: 4.363526821136475\n",
      "training loss for step: 4.363790035247803\n",
      "training loss for step: 4.3639960289001465\n",
      "training loss for step: 4.3636908531188965\n",
      "training loss for step: 4.364843845367432\n",
      "training loss for step: 4.364398002624512\n",
      "training loss for step: 4.363930702209473\n",
      "training loss for step: 4.363992214202881\n",
      "training loss for step: 4.364450454711914\n",
      "training loss for step: 4.364882946014404\n",
      "training loss for step: 4.364181041717529\n",
      "training loss for step: 4.363847255706787\n",
      "training loss for step: 4.363821029663086\n",
      "training loss for step: 4.3639068603515625\n",
      "training loss for step: 4.36393404006958\n",
      "training loss for step: 4.363561153411865\n",
      "training loss for step: 4.36485481262207\n",
      "training loss for step: 4.364873886108398\n",
      "training loss for step: 4.364602088928223\n",
      "training loss for step: 4.364626884460449\n",
      "training loss for step: 4.363671779632568\n",
      "training loss for step: 4.364533424377441\n",
      "training loss for step: 4.3653717041015625\n",
      "training loss for step: 4.365013122558594\n",
      "training loss for step: 4.363611698150635\n",
      "training loss for step: 4.3640546798706055\n",
      "training loss for step: 4.363918304443359\n",
      "training loss for step: 4.364207744598389\n",
      "training loss for step: 4.365025997161865\n",
      "training loss for step: 4.3646979331970215\n",
      "training loss for step: 4.3635172843933105\n",
      "training loss for step: 4.363987445831299\n",
      "training loss for step: 4.365225315093994\n",
      "training loss for step: 4.365267276763916\n",
      "training loss for step: 4.364394664764404\n",
      "training loss for step: 4.363475322723389\n",
      "training loss for step: 4.365567207336426\n",
      "training loss for step: 4.365861892700195\n",
      "training loss for step: 4.365933895111084\n",
      "training loss for step: 4.365295886993408\n",
      "training loss for step: 4.363983631134033\n",
      "training loss for step: 4.36458158493042\n",
      "training loss for step: 4.365237712860107\n",
      "training loss for step: 4.364903450012207\n",
      "training loss for step: 4.365307807922363\n",
      "training loss for step: 4.36383056640625\n",
      "training loss for step: 4.363508224487305\n",
      "training loss for step: 4.363846302032471\n",
      "training loss for step: 4.3634514808654785\n",
      "training loss for step: 4.363697052001953\n",
      "training loss for step: 4.36517858505249\n",
      "training loss for step: 4.364572048187256\n",
      "training loss for step: 4.363958358764648\n",
      "training loss for step: 4.363569259643555\n",
      "training loss for step: 4.364426612854004\n",
      "training loss for step: 4.36366605758667\n",
      "training loss for step: 4.3638200759887695\n",
      "training loss for step: 4.364530086517334\n",
      "training loss for step: 4.36443567276001\n",
      "training loss for step: 4.364737033843994\n",
      "training loss for step: 4.364555358886719\n",
      "training loss for step: 4.365301132202148\n",
      "training loss for step: 4.363880634307861\n",
      "training loss for step: 4.36468505859375\n",
      "training loss for step: 4.3658928871154785\n",
      "training loss for step: 4.365824222564697\n",
      "training loss for step: 4.365209579467773\n",
      "training loss for step: 4.363870143890381\n",
      "training loss for step: 4.364670276641846\n",
      "training loss for step: 4.365405082702637\n",
      "training loss for step: 4.366692543029785\n",
      "training loss for step: 4.364855766296387\n",
      "training loss for step: 4.364541053771973\n",
      "training loss for step: 4.364165782928467\n",
      "training loss for step: 4.364774703979492\n",
      "training loss for step: 4.364201545715332\n",
      "training loss for step: 4.36435079574585\n",
      "training loss for step: 4.363553047180176\n",
      "training loss for step: 4.364912986755371\n",
      "training loss for step: 4.366046905517578\n",
      "training loss for step: 4.366855621337891\n",
      "training loss for step: 4.365357875823975\n",
      "training loss for step: 4.364164352416992\n",
      "training loss for step: 4.364398956298828\n",
      "training loss for step: 4.36472225189209\n",
      "training loss for step: 4.3643035888671875\n",
      "training loss for step: 4.36408805847168\n",
      "training loss for step: 4.3638081550598145\n",
      "training loss for step: 4.363542079925537\n",
      "training loss for step: 4.3657073974609375\n",
      "training loss for step: 4.365725994110107\n",
      "training loss for step: 4.364442825317383\n",
      "training loss for step: 4.364150047302246\n",
      "training loss for step: 4.363988399505615\n",
      "training loss for step: 4.365675926208496\n",
      "training loss for step: 4.36533260345459\n",
      "training loss for step: 4.364530086517334\n",
      "training loss for step: 4.363915920257568\n",
      "training loss for step: 4.363550662994385\n",
      "training loss for step: 4.363654136657715\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76dc981d640a4937b4e6fc83ec79c2c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8913c40a38b9461680bdc540d2aafd0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/447 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss for step: 4.463846683502197\n",
      "training loss for step: 4.465566158294678\n",
      "training loss for step: 4.466290473937988\n",
      "training loss for step: 4.465339660644531\n",
      "training loss for step: 4.464564323425293\n",
      "training loss for step: 4.464362144470215\n",
      "training loss for step: 4.464877128601074\n",
      "training loss for step: 4.46472692489624\n",
      "training loss for step: 4.465371131896973\n",
      "training loss for step: 4.46522855758667\n",
      "training loss for step: 4.4637017250061035\n",
      "training loss for step: 4.464170932769775\n",
      "training loss for step: 4.464823246002197\n",
      "training loss for step: 4.465484619140625\n",
      "training loss for step: 4.465307712554932\n",
      "training loss for step: 4.463644027709961\n",
      "training loss for step: 4.46492338180542\n",
      "training loss for step: 4.465344429016113\n",
      "training loss for step: 4.465826034545898\n",
      "training loss for step: 4.465312480926514\n",
      "training loss for step: 4.4647603034973145\n",
      "training loss for step: 4.463799476623535\n",
      "training loss for step: 4.465112209320068\n",
      "training loss for step: 4.464245319366455\n",
      "training loss for step: 4.464212894439697\n",
      "training loss for step: 4.463648796081543\n",
      "training loss for step: 4.465488433837891\n",
      "training loss for step: 4.466436386108398\n",
      "training loss for step: 4.466007709503174\n",
      "training loss for step: 4.465918064117432\n",
      "training loss for step: 4.464147567749023\n",
      "training loss for step: 4.464301109313965\n",
      "training loss for step: 4.464767932891846\n",
      "training loss for step: 4.465473175048828\n",
      "training loss for step: 4.463958740234375\n",
      "training loss for step: 4.463741779327393\n",
      "training loss for step: 4.463786602020264\n",
      "training loss for step: 4.46411657333374\n",
      "training loss for step: 4.463758945465088\n",
      "training loss for step: 4.4647626876831055\n",
      "training loss for step: 4.46368932723999\n",
      "training loss for step: 4.463956832885742\n",
      "training loss for step: 4.463981628417969\n",
      "training loss for step: 4.4635090827941895\n",
      "training loss for step: 4.463796615600586\n",
      "training loss for step: 4.464211463928223\n",
      "training loss for step: 4.463484764099121\n",
      "training loss for step: 4.464478969573975\n",
      "training loss for step: 4.464450836181641\n",
      "training loss for step: 4.4642653465271\n",
      "training loss for step: 4.463656425476074\n",
      "training loss for step: 4.463956356048584\n",
      "training loss for step: 4.463568210601807\n",
      "training loss for step: 4.464040756225586\n",
      "training loss for step: 4.463520526885986\n",
      "training loss for step: 4.464087963104248\n",
      "training loss for step: 4.464798450469971\n",
      "training loss for step: 4.463979721069336\n",
      "training loss for step: 4.463634490966797\n",
      "training loss for step: 4.464883327484131\n",
      "training loss for step: 4.465594291687012\n",
      "training loss for step: 4.465616226196289\n",
      "training loss for step: 4.464207172393799\n",
      "training loss for step: 4.46403694152832\n",
      "training loss for step: 4.4646077156066895\n",
      "training loss for step: 4.465363502502441\n",
      "training loss for step: 4.46531867980957\n",
      "training loss for step: 4.465484619140625\n",
      "training loss for step: 4.46378755569458\n",
      "training loss for step: 4.464555263519287\n",
      "training loss for step: 4.464902400970459\n",
      "training loss for step: 4.465017795562744\n",
      "training loss for step: 4.464493274688721\n",
      "training loss for step: 4.463603973388672\n",
      "training loss for step: 4.46399450302124\n",
      "training loss for step: 4.463820457458496\n",
      "training loss for step: 4.464529991149902\n",
      "training loss for step: 4.464208126068115\n",
      "training loss for step: 4.463625907897949\n",
      "training loss for step: 4.463613033294678\n",
      "training loss for step: 4.464016437530518\n",
      "training loss for step: 4.463756084442139\n",
      "training loss for step: 4.4638471603393555\n",
      "training loss for step: 4.463663578033447\n",
      "training loss for step: 4.463486194610596\n",
      "training loss for step: 4.463757038116455\n",
      "training loss for step: 4.464698314666748\n",
      "training loss for step: 4.464386463165283\n",
      "training loss for step: 4.463988304138184\n",
      "training loss for step: 4.46350622177124\n",
      "training loss for step: 4.463770866394043\n",
      "training loss for step: 4.4635186195373535\n",
      "training loss for step: 4.463828086853027\n",
      "training loss for step: 4.464323043823242\n",
      "training loss for step: 4.46370267868042\n",
      "training loss for step: 4.463537216186523\n",
      "training loss for step: 4.464841365814209\n",
      "training loss for step: 4.464240550994873\n",
      "training loss for step: 4.463871002197266\n",
      "training loss for step: 4.463496685028076\n",
      "training loss for step: 4.464869976043701\n",
      "training loss for step: 4.46509313583374\n",
      "training loss for step: 4.464015007019043\n",
      "training loss for step: 4.463624000549316\n",
      "training loss for step: 4.463975429534912\n",
      "training loss for step: 4.4637041091918945\n",
      "training loss for step: 4.463582515716553\n",
      "training loss for step: 4.463962554931641\n",
      "training loss for step: 4.464221477508545\n",
      "training loss for step: 4.464407444000244\n",
      "training loss for step: 4.463566780090332\n",
      "training loss for step: 4.463696002960205\n",
      "training loss for step: 4.463651657104492\n",
      "training loss for step: 4.463764667510986\n",
      "training loss for step: 4.463510990142822\n",
      "training loss for step: 4.4635419845581055\n",
      "training loss for step: 4.464285850524902\n",
      "training loss for step: 4.464774131774902\n",
      "training loss for step: 4.464581489562988\n",
      "training loss for step: 4.463490009307861\n",
      "training loss for step: 4.4640374183654785\n",
      "training loss for step: 4.463580131530762\n",
      "training loss for step: 4.464634895324707\n",
      "training loss for step: 4.46403694152832\n",
      "training loss for step: 4.463451385498047\n",
      "training loss for step: 4.464471817016602\n",
      "training loss for step: 4.4637274742126465\n",
      "training loss for step: 4.464073657989502\n",
      "training loss for step: 4.463830471038818\n",
      "training loss for step: 4.464348793029785\n",
      "training loss for step: 4.4644856452941895\n",
      "training loss for step: 4.464835166931152\n",
      "training loss for step: 4.4637603759765625\n",
      "training loss for step: 4.4638495445251465\n",
      "training loss for step: 4.464544773101807\n",
      "training loss for step: 4.465134143829346\n",
      "training loss for step: 4.4644975662231445\n",
      "training loss for step: 4.463546276092529\n",
      "training loss for step: 4.465278148651123\n",
      "training loss for step: 4.4657979011535645\n",
      "training loss for step: 4.466469764709473\n",
      "training loss for step: 4.465890407562256\n",
      "training loss for step: 4.464719772338867\n",
      "training loss for step: 4.463531017303467\n",
      "training loss for step: 4.464117527008057\n",
      "training loss for step: 4.463535785675049\n",
      "training loss for step: 4.464814186096191\n",
      "training loss for step: 4.4649763107299805\n",
      "training loss for step: 4.464015483856201\n",
      "training loss for step: 4.463945388793945\n",
      "training loss for step: 4.464508056640625\n",
      "training loss for step: 4.463790416717529\n",
      "training loss for step: 4.463821887969971\n",
      "training loss for step: 4.464107036590576\n",
      "training loss for step: 4.463885307312012\n",
      "training loss for step: 4.463940620422363\n",
      "training loss for step: 4.463461399078369\n",
      "training loss for step: 4.463733196258545\n",
      "training loss for step: 4.464199542999268\n",
      "training loss for step: 4.463999271392822\n",
      "training loss for step: 4.464375019073486\n",
      "training loss for step: 4.464437484741211\n",
      "training loss for step: 4.464211463928223\n",
      "training loss for step: 4.46383810043335\n",
      "training loss for step: 4.463632583618164\n",
      "training loss for step: 4.464189052581787\n",
      "training loss for step: 4.464325428009033\n",
      "training loss for step: 4.463623046875\n",
      "training loss for step: 4.464433670043945\n",
      "training loss for step: 4.463728427886963\n",
      "training loss for step: 4.463678359985352\n",
      "training loss for step: 4.463499069213867\n",
      "training loss for step: 4.463708400726318\n",
      "training loss for step: 4.463891983032227\n",
      "training loss for step: 4.465304851531982\n",
      "training loss for step: 4.464160919189453\n",
      "training loss for step: 4.463615894317627\n",
      "training loss for step: 4.4644341468811035\n",
      "training loss for step: 4.4635467529296875\n",
      "training loss for step: 4.463761329650879\n",
      "training loss for step: 4.46453857421875\n",
      "training loss for step: 4.4644317626953125\n",
      "training loss for step: 4.463515758514404\n",
      "training loss for step: 4.464698791503906\n",
      "training loss for step: 4.46612548828125\n",
      "training loss for step: 4.466519355773926\n",
      "training loss for step: 4.465725898742676\n",
      "training loss for step: 4.464132785797119\n",
      "training loss for step: 4.4637885093688965\n",
      "training loss for step: 4.464571475982666\n",
      "training loss for step: 4.4649224281311035\n",
      "training loss for step: 4.465078830718994\n",
      "training loss for step: 4.4638471603393555\n",
      "training loss for step: 4.464437007904053\n",
      "training loss for step: 4.4653849601745605\n",
      "training loss for step: 4.4652276039123535\n",
      "training loss for step: 4.4647674560546875\n",
      "training loss for step: 4.464120388031006\n",
      "training loss for step: 4.463773250579834\n",
      "training loss for step: 4.465384483337402\n",
      "training loss for step: 4.46561336517334\n",
      "training loss for step: 4.465062618255615\n",
      "training loss for step: 4.464111804962158\n",
      "training loss for step: 4.464557647705078\n",
      "training loss for step: 4.465732097625732\n",
      "training loss for step: 4.465479850769043\n",
      "training loss for step: 4.465198993682861\n",
      "training loss for step: 4.4635186195373535\n",
      "training loss for step: 4.464557647705078\n",
      "training loss for step: 4.465183258056641\n",
      "training loss for step: 4.465109348297119\n",
      "training loss for step: 4.464693546295166\n",
      "training loss for step: 4.46397590637207\n",
      "training loss for step: 4.464693069458008\n",
      "training loss for step: 4.4653449058532715\n",
      "training loss for step: 4.465086936950684\n",
      "training loss for step: 4.465353965759277\n",
      "training loss for step: 4.463659286499023\n",
      "training loss for step: 4.463460445404053\n",
      "training loss for step: 4.463913440704346\n",
      "training loss for step: 4.463472843170166\n",
      "training loss for step: 4.464652061462402\n",
      "training loss for step: 4.4637274742126465\n",
      "training loss for step: 4.463596820831299\n",
      "training loss for step: 4.464893817901611\n",
      "training loss for step: 4.465320587158203\n",
      "training loss for step: 4.465569019317627\n",
      "training loss for step: 4.464052200317383\n",
      "training loss for step: 4.464953899383545\n",
      "training loss for step: 4.464409351348877\n",
      "training loss for step: 4.464725017547607\n",
      "training loss for step: 4.464670658111572\n",
      "training loss for step: 4.46457052230835\n",
      "training loss for step: 4.464712619781494\n",
      "training loss for step: 4.464529514312744\n",
      "training loss for step: 4.463965892791748\n",
      "training loss for step: 4.464440822601318\n",
      "training loss for step: 4.463776111602783\n",
      "training loss for step: 4.464618682861328\n",
      "training loss for step: 4.4637451171875\n",
      "training loss for step: 4.46352481842041\n",
      "training loss for step: 4.46397066116333\n",
      "training loss for step: 4.464028358459473\n",
      "training loss for step: 4.464070796966553\n",
      "training loss for step: 4.463743209838867\n",
      "training loss for step: 4.464710235595703\n",
      "training loss for step: 4.464216709136963\n",
      "training loss for step: 4.4639763832092285\n",
      "training loss for step: 4.464639186859131\n",
      "training loss for step: 4.464979648590088\n",
      "training loss for step: 4.4647979736328125\n",
      "training loss for step: 4.464241981506348\n",
      "training loss for step: 4.463925361633301\n",
      "training loss for step: 4.464066505432129\n",
      "training loss for step: 4.463938236236572\n",
      "training loss for step: 4.463730335235596\n",
      "training loss for step: 4.464210033416748\n",
      "training loss for step: 4.46483039855957\n",
      "training loss for step: 4.464653015136719\n",
      "training loss for step: 4.463984489440918\n",
      "training loss for step: 4.464227676391602\n",
      "training loss for step: 4.464280605316162\n",
      "training loss for step: 4.464116096496582\n",
      "training loss for step: 4.46368932723999\n",
      "training loss for step: 4.464288711547852\n",
      "training loss for step: 4.465353012084961\n",
      "training loss for step: 4.464913368225098\n",
      "training loss for step: 4.463668346405029\n",
      "training loss for step: 4.464198589324951\n",
      "training loss for step: 4.464466571807861\n",
      "training loss for step: 4.46382999420166\n",
      "training loss for step: 4.463756561279297\n",
      "training loss for step: 4.463933944702148\n",
      "training loss for step: 4.46471643447876\n",
      "training loss for step: 4.464285850524902\n",
      "training loss for step: 4.463925838470459\n",
      "training loss for step: 4.463612079620361\n",
      "training loss for step: 4.464271545410156\n",
      "training loss for step: 4.4645538330078125\n",
      "training loss for step: 4.463508129119873\n",
      "training loss for step: 4.465254306793213\n",
      "training loss for step: 4.464931488037109\n",
      "training loss for step: 4.465672492980957\n",
      "training loss for step: 4.463813304901123\n",
      "training loss for step: 4.463625431060791\n",
      "training loss for step: 4.464057922363281\n",
      "training loss for step: 4.464334011077881\n",
      "training loss for step: 4.4638495445251465\n",
      "training loss for step: 4.463810920715332\n",
      "training loss for step: 4.463907718658447\n",
      "training loss for step: 4.4639410972595215\n",
      "training loss for step: 4.464122295379639\n",
      "training loss for step: 4.465137004852295\n",
      "training loss for step: 4.4645771980285645\n",
      "training loss for step: 4.465746879577637\n",
      "training loss for step: 4.465098857879639\n",
      "training loss for step: 4.464533805847168\n",
      "training loss for step: 4.464273452758789\n",
      "training loss for step: 4.463942527770996\n",
      "training loss for step: 4.463716983795166\n",
      "training loss for step: 4.4644622802734375\n",
      "training loss for step: 4.4644551277160645\n",
      "training loss for step: 4.463499069213867\n",
      "training loss for step: 4.46449089050293\n",
      "training loss for step: 4.463823318481445\n",
      "training loss for step: 4.4637451171875\n",
      "training loss for step: 4.464503765106201\n",
      "training loss for step: 4.4640583992004395\n",
      "training loss for step: 4.464651584625244\n",
      "training loss for step: 4.4637298583984375\n",
      "training loss for step: 4.46425724029541\n",
      "training loss for step: 4.464104652404785\n",
      "training loss for step: 4.464890956878662\n",
      "training loss for step: 4.464123249053955\n",
      "training loss for step: 4.463698387145996\n",
      "training loss for step: 4.465118885040283\n",
      "training loss for step: 4.464512825012207\n",
      "training loss for step: 4.463531017303467\n",
      "training loss for step: 4.464506149291992\n",
      "training loss for step: 4.463540554046631\n",
      "training loss for step: 4.46446418762207\n",
      "training loss for step: 4.4650092124938965\n",
      "training loss for step: 4.465212345123291\n",
      "training loss for step: 4.464198112487793\n",
      "training loss for step: 4.463832855224609\n",
      "training loss for step: 4.464268207550049\n",
      "training loss for step: 4.464329242706299\n",
      "training loss for step: 4.463635444641113\n",
      "training loss for step: 4.463522911071777\n",
      "training loss for step: 4.464576721191406\n",
      "training loss for step: 4.463808536529541\n",
      "training loss for step: 4.464366912841797\n",
      "training loss for step: 4.4644455909729\n",
      "training loss for step: 4.4645490646362305\n",
      "training loss for step: 4.46475076675415\n",
      "training loss for step: 4.4639482498168945\n",
      "training loss for step: 4.464193344116211\n",
      "training loss for step: 4.464426517486572\n",
      "training loss for step: 4.464684009552002\n",
      "training loss for step: 4.463632583618164\n",
      "training loss for step: 4.463788986206055\n",
      "training loss for step: 4.464329719543457\n",
      "training loss for step: 4.463773250579834\n",
      "training loss for step: 4.463470458984375\n",
      "training loss for step: 4.4642653465271\n",
      "training loss for step: 4.46478271484375\n",
      "training loss for step: 4.46443510055542\n",
      "training loss for step: 4.463455677032471\n",
      "training loss for step: 4.464060306549072\n",
      "training loss for step: 4.464793682098389\n",
      "training loss for step: 4.464877128601074\n",
      "training loss for step: 4.463492393493652\n",
      "training loss for step: 4.46392822265625\n",
      "training loss for step: 4.463572978973389\n",
      "training loss for step: 4.4644856452941895\n",
      "training loss for step: 4.464388370513916\n",
      "training loss for step: 4.464491367340088\n",
      "training loss for step: 4.463931560516357\n",
      "training loss for step: 4.464578628540039\n",
      "training loss for step: 4.464456558227539\n",
      "training loss for step: 4.465188503265381\n",
      "training loss for step: 4.464531421661377\n",
      "training loss for step: 4.463578701019287\n",
      "training loss for step: 4.463891983032227\n",
      "training loss for step: 4.464331150054932\n",
      "training loss for step: 4.463642120361328\n",
      "training loss for step: 4.463455677032471\n",
      "training loss for step: 4.463789463043213\n",
      "training loss for step: 4.463475704193115\n",
      "training loss for step: 4.464248180389404\n",
      "training loss for step: 4.464726448059082\n",
      "training loss for step: 4.464422225952148\n",
      "training loss for step: 4.463951587677002\n",
      "training loss for step: 4.464751720428467\n",
      "training loss for step: 4.46533203125\n",
      "training loss for step: 4.465052127838135\n",
      "training loss for step: 4.464623928070068\n",
      "training loss for step: 4.463744640350342\n",
      "training loss for step: 4.464363098144531\n",
      "training loss for step: 4.464998722076416\n",
      "training loss for step: 4.465587615966797\n",
      "training loss for step: 4.464750289916992\n",
      "training loss for step: 4.464237689971924\n",
      "training loss for step: 4.464257717132568\n",
      "training loss for step: 4.464231491088867\n",
      "training loss for step: 4.464936256408691\n",
      "training loss for step: 4.4637041091918945\n",
      "training loss for step: 4.463503837585449\n",
      "training loss for step: 4.46561861038208\n",
      "training loss for step: 4.466170310974121\n",
      "training loss for step: 4.4660162925720215\n",
      "training loss for step: 4.466550827026367\n",
      "training loss for step: 4.464410305023193\n",
      "training loss for step: 4.465147495269775\n",
      "training loss for step: 4.465826511383057\n",
      "training loss for step: 4.465938568115234\n",
      "training loss for step: 4.466424465179443\n",
      "training loss for step: 4.46696138381958\n",
      "training loss for step: 4.465714931488037\n",
      "training loss for step: 4.465474605560303\n",
      "training loss for step: 4.4635419845581055\n",
      "training loss for step: 4.4659833908081055\n",
      "training loss for step: 4.467721462249756\n",
      "training loss for step: 4.467789173126221\n",
      "training loss for step: 4.467567443847656\n",
      "training loss for step: 4.4667487144470215\n",
      "training loss for step: 4.4650468826293945\n",
      "training loss for step: 4.464335918426514\n",
      "training loss for step: 4.464552879333496\n",
      "training loss for step: 4.4653449058532715\n",
      "training loss for step: 4.466332912445068\n",
      "training loss for step: 4.464760780334473\n",
      "training loss for step: 4.465672492980957\n",
      "training loss for step: 4.464739799499512\n",
      "training loss for step: 4.464962959289551\n",
      "training loss for step: 4.465535640716553\n",
      "training loss for step: 4.465683937072754\n",
      "training loss for step: 4.465648651123047\n",
      "training loss for step: 4.464249610900879\n",
      "training loss for step: 4.464597225189209\n",
      "training loss for step: 4.464474201202393\n",
      "training loss for step: 4.464946746826172\n",
      "training loss for step: 4.464730262756348\n",
      "training loss for step: 4.463759899139404\n",
      "training loss for step: 4.464241981506348\n",
      "training loss for step: 4.464414596557617\n",
      "training loss for step: 4.463881969451904\n",
      "training loss for step: 4.464089393615723\n",
      "training loss for step: 4.463501930236816\n",
      "training loss for step: 4.464354991912842\n",
      "training loss for step: 4.464560031890869\n",
      "training loss for step: 4.464715480804443\n",
      "training loss for step: 4.463460922241211\n",
      "training loss for step: 4.464043140411377\n",
      "training loss for step: 4.464393138885498\n",
      "training loss for step: 4.4637980461120605\n",
      "training loss for step: 4.463751792907715\n",
      "training loss for step: 4.464536190032959\n",
      "training loss for step: 4.465108394622803\n",
      "training loss for step: 4.465206146240234\n",
      "training loss for step: 4.464921474456787\n",
      "training loss for step: 4.463915824890137\n",
      "training loss for step: 4.464087963104248\n",
      "training loss for step: 4.465656280517578\n",
      "training loss for step: 4.46618127822876\n",
      "training loss for step: 4.465088844299316\n",
      "training loss for step: 4.463961124420166\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75eec9d64a6f4f0ebe22bc2818a7690a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4275881eb91d454285f4970c53bebff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/447 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss for step: 4.563502788543701\n",
      "training loss for step: 4.565033435821533\n",
      "training loss for step: 4.566665172576904\n",
      "training loss for step: 4.56677770614624\n",
      "training loss for step: 4.565945148468018\n",
      "training loss for step: 4.566217422485352\n",
      "training loss for step: 4.564444065093994\n",
      "training loss for step: 4.563746452331543\n",
      "training loss for step: 4.564207077026367\n",
      "training loss for step: 4.564621925354004\n",
      "training loss for step: 4.565174102783203\n",
      "training loss for step: 4.56365442276001\n",
      "training loss for step: 4.56486701965332\n",
      "training loss for step: 4.565186977386475\n",
      "training loss for step: 4.565232753753662\n",
      "training loss for step: 4.564877986907959\n",
      "training loss for step: 4.5640869140625\n",
      "training loss for step: 4.564820289611816\n",
      "training loss for step: 4.565004348754883\n",
      "training loss for step: 4.565308570861816\n",
      "training loss for step: 4.5656585693359375\n",
      "training loss for step: 4.564907550811768\n",
      "training loss for step: 4.563992500305176\n",
      "training loss for step: 4.563553333282471\n",
      "training loss for step: 4.564489841461182\n",
      "training loss for step: 4.564059257507324\n",
      "training loss for step: 4.564440727233887\n",
      "training loss for step: 4.5650200843811035\n",
      "training loss for step: 4.566022872924805\n",
      "training loss for step: 4.56614351272583\n",
      "training loss for step: 4.564440727233887\n",
      "training loss for step: 4.564121246337891\n",
      "training loss for step: 4.563972473144531\n",
      "training loss for step: 4.5666961669921875\n",
      "training loss for step: 4.566498279571533\n",
      "training loss for step: 4.566224098205566\n",
      "training loss for step: 4.566887855529785\n",
      "training loss for step: 4.56696891784668\n",
      "training loss for step: 4.56533670425415\n",
      "training loss for step: 4.5639472007751465\n",
      "training loss for step: 4.564211845397949\n",
      "training loss for step: 4.564085483551025\n",
      "training loss for step: 4.563960075378418\n",
      "training loss for step: 4.563462734222412\n",
      "training loss for step: 4.56345272064209\n",
      "training loss for step: 4.563752174377441\n",
      "training loss for step: 4.5642008781433105\n",
      "training loss for step: 4.564669609069824\n",
      "training loss for step: 4.564204692840576\n",
      "training loss for step: 4.563684940338135\n",
      "training loss for step: 4.56513786315918\n",
      "training loss for step: 4.5657172203063965\n",
      "training loss for step: 4.565323352813721\n",
      "training loss for step: 4.5648193359375\n",
      "training loss for step: 4.56345796585083\n",
      "training loss for step: 4.564883232116699\n",
      "training loss for step: 4.56504487991333\n",
      "training loss for step: 4.565587520599365\n",
      "training loss for step: 4.565499305725098\n",
      "training loss for step: 4.564419746398926\n",
      "training loss for step: 4.564353942871094\n",
      "training loss for step: 4.564560890197754\n",
      "training loss for step: 4.564857482910156\n",
      "training loss for step: 4.564204692840576\n",
      "training loss for step: 4.563565731048584\n",
      "training loss for step: 4.563618183135986\n",
      "training loss for step: 4.563956260681152\n",
      "training loss for step: 4.563737869262695\n",
      "training loss for step: 4.5635881423950195\n",
      "training loss for step: 4.563838481903076\n",
      "training loss for step: 4.5634942054748535\n",
      "training loss for step: 4.564628601074219\n",
      "training loss for step: 4.56385612487793\n",
      "training loss for step: 4.564063549041748\n",
      "training loss for step: 4.563761234283447\n",
      "training loss for step: 4.5638298988342285\n",
      "training loss for step: 4.563612937927246\n",
      "training loss for step: 4.564175605773926\n",
      "training loss for step: 4.563566207885742\n",
      "training loss for step: 4.563827991485596\n",
      "training loss for step: 4.564356327056885\n",
      "training loss for step: 4.5638508796691895\n",
      "training loss for step: 4.5642008781433105\n",
      "training loss for step: 4.564215660095215\n",
      "training loss for step: 4.564197063446045\n",
      "training loss for step: 4.563981533050537\n",
      "training loss for step: 4.564104080200195\n",
      "training loss for step: 4.563666820526123\n",
      "training loss for step: 4.564275741577148\n",
      "training loss for step: 4.5641632080078125\n",
      "training loss for step: 4.564131259918213\n",
      "training loss for step: 4.56402587890625\n",
      "training loss for step: 4.56389856338501\n",
      "training loss for step: 4.563677787780762\n",
      "training loss for step: 4.564149856567383\n",
      "training loss for step: 4.563994407653809\n",
      "training loss for step: 4.564109802246094\n",
      "training loss for step: 4.564047336578369\n",
      "training loss for step: 4.563755512237549\n",
      "training loss for step: 4.56350040435791\n",
      "training loss for step: 4.563666820526123\n",
      "training loss for step: 4.563516616821289\n",
      "training loss for step: 4.563472747802734\n",
      "training loss for step: 4.5643720626831055\n",
      "training loss for step: 4.5643391609191895\n",
      "training loss for step: 4.564150810241699\n",
      "training loss for step: 4.563852787017822\n",
      "training loss for step: 4.564215183258057\n",
      "training loss for step: 4.563632965087891\n",
      "training loss for step: 4.564072608947754\n",
      "training loss for step: 4.564117431640625\n",
      "training loss for step: 4.563867568969727\n",
      "training loss for step: 4.563912868499756\n",
      "training loss for step: 4.563984394073486\n",
      "training loss for step: 4.563517093658447\n",
      "training loss for step: 4.563937664031982\n",
      "training loss for step: 4.563601016998291\n",
      "training loss for step: 4.56386137008667\n",
      "training loss for step: 4.563981533050537\n",
      "training loss for step: 4.563471794128418\n",
      "training loss for step: 4.563451766967773\n",
      "training loss for step: 4.5655131340026855\n",
      "training loss for step: 4.5651960372924805\n",
      "training loss for step: 4.564891338348389\n",
      "training loss for step: 4.564478874206543\n",
      "training loss for step: 4.5637993812561035\n",
      "training loss for step: 4.564221382141113\n",
      "training loss for step: 4.5639543533325195\n",
      "training loss for step: 4.5639328956604\n",
      "training loss for step: 4.564242362976074\n",
      "training loss for step: 4.564886569976807\n",
      "training loss for step: 4.564563274383545\n",
      "training loss for step: 4.563622951507568\n",
      "training loss for step: 4.5640482902526855\n",
      "training loss for step: 4.564416885375977\n",
      "training loss for step: 4.564444065093994\n",
      "training loss for step: 4.563492298126221\n",
      "training loss for step: 4.564233779907227\n",
      "training loss for step: 4.564279079437256\n",
      "training loss for step: 4.5646867752075195\n",
      "training loss for step: 4.5637335777282715\n",
      "training loss for step: 4.564634323120117\n",
      "training loss for step: 4.56518030166626\n",
      "training loss for step: 4.564664363861084\n",
      "training loss for step: 4.563742160797119\n",
      "training loss for step: 4.5644659996032715\n",
      "training loss for step: 4.564244270324707\n",
      "training loss for step: 4.563477516174316\n",
      "training loss for step: 4.563727378845215\n",
      "training loss for step: 4.56396484375\n",
      "training loss for step: 4.565047264099121\n",
      "training loss for step: 4.564175128936768\n",
      "training loss for step: 4.563891410827637\n",
      "training loss for step: 4.563902854919434\n",
      "training loss for step: 4.564689636230469\n",
      "training loss for step: 4.564251899719238\n",
      "training loss for step: 4.563780307769775\n",
      "training loss for step: 4.563790798187256\n",
      "training loss for step: 4.564793109893799\n",
      "training loss for step: 4.564869403839111\n",
      "training loss for step: 4.5638628005981445\n",
      "training loss for step: 4.564118385314941\n",
      "training loss for step: 4.564480304718018\n",
      "training loss for step: 4.564200401306152\n",
      "training loss for step: 4.563448905944824\n",
      "training loss for step: 4.563727855682373\n",
      "training loss for step: 4.563695430755615\n",
      "training loss for step: 4.563547611236572\n",
      "training loss for step: 4.564464569091797\n",
      "training loss for step: 4.564720630645752\n",
      "training loss for step: 4.564780235290527\n",
      "training loss for step: 4.563971042633057\n",
      "training loss for step: 4.564030647277832\n",
      "training loss for step: 4.564752101898193\n",
      "training loss for step: 4.564490795135498\n",
      "training loss for step: 4.56392765045166\n",
      "training loss for step: 4.563715934753418\n",
      "training loss for step: 4.564203262329102\n",
      "training loss for step: 4.564059257507324\n",
      "training loss for step: 4.5636515617370605\n",
      "training loss for step: 4.563868045806885\n",
      "training loss for step: 4.563746452331543\n",
      "training loss for step: 4.563532829284668\n",
      "training loss for step: 4.564352512359619\n",
      "training loss for step: 4.56417179107666\n",
      "training loss for step: 4.563752174377441\n",
      "training loss for step: 4.563561916351318\n",
      "training loss for step: 4.563601493835449\n",
      "training loss for step: 4.56363582611084\n",
      "training loss for step: 4.563940048217773\n",
      "training loss for step: 4.5636115074157715\n",
      "training loss for step: 4.5635247230529785\n",
      "training loss for step: 4.564006805419922\n",
      "training loss for step: 4.565006732940674\n",
      "training loss for step: 4.564044952392578\n",
      "training loss for step: 4.563871383666992\n",
      "training loss for step: 4.564369201660156\n",
      "training loss for step: 4.565122604370117\n",
      "training loss for step: 4.564199924468994\n",
      "training loss for step: 4.5641937255859375\n",
      "training loss for step: 4.5636444091796875\n",
      "training loss for step: 4.56547212600708\n",
      "training loss for step: 4.566572189331055\n",
      "training loss for step: 4.566589832305908\n",
      "training loss for step: 4.565695762634277\n",
      "training loss for step: 4.5644941329956055\n",
      "training loss for step: 4.564584732055664\n",
      "training loss for step: 4.565268039703369\n",
      "training loss for step: 4.566215515136719\n",
      "training loss for step: 4.566317081451416\n",
      "training loss for step: 4.566220760345459\n",
      "training loss for step: 4.566133499145508\n",
      "training loss for step: 4.564395904541016\n",
      "training loss for step: 4.564278602600098\n",
      "training loss for step: 4.564741134643555\n",
      "training loss for step: 4.566035747528076\n",
      "training loss for step: 4.565306186676025\n",
      "training loss for step: 4.563477993011475\n",
      "training loss for step: 4.564395427703857\n",
      "training loss for step: 4.564855575561523\n",
      "training loss for step: 4.564632415771484\n",
      "training loss for step: 4.563735008239746\n",
      "training loss for step: 4.5638322830200195\n",
      "training loss for step: 4.564502239227295\n",
      "training loss for step: 4.564047336578369\n",
      "training loss for step: 4.563684940338135\n",
      "training loss for step: 4.563899993896484\n",
      "training loss for step: 4.5636444091796875\n",
      "training loss for step: 4.563456058502197\n",
      "training loss for step: 4.563818454742432\n",
      "training loss for step: 4.5637922286987305\n",
      "training loss for step: 4.564112663269043\n",
      "training loss for step: 4.564431190490723\n",
      "training loss for step: 4.563477516174316\n",
      "training loss for step: 4.563458442687988\n",
      "training loss for step: 4.563845157623291\n",
      "training loss for step: 4.563639163970947\n",
      "training loss for step: 4.564263820648193\n",
      "training loss for step: 4.564234256744385\n",
      "training loss for step: 4.56346321105957\n",
      "training loss for step: 4.564687252044678\n",
      "training loss for step: 4.564393997192383\n",
      "training loss for step: 4.564185619354248\n",
      "training loss for step: 4.563541889190674\n",
      "training loss for step: 4.563788414001465\n",
      "training loss for step: 4.563675403594971\n",
      "training loss for step: 4.563589572906494\n",
      "training loss for step: 4.56411600112915\n",
      "training loss for step: 4.563627243041992\n",
      "training loss for step: 4.563619136810303\n",
      "training loss for step: 4.564351558685303\n",
      "training loss for step: 4.564393997192383\n",
      "training loss for step: 4.564188003540039\n",
      "training loss for step: 4.5636138916015625\n",
      "training loss for step: 4.5649871826171875\n",
      "training loss for step: 4.564903736114502\n",
      "training loss for step: 4.565264701843262\n",
      "training loss for step: 4.564855575561523\n",
      "training loss for step: 4.563716888427734\n",
      "training loss for step: 4.565045356750488\n",
      "training loss for step: 4.5654826164245605\n",
      "training loss for step: 4.565066814422607\n",
      "training loss for step: 4.565144062042236\n",
      "training loss for step: 4.564416408538818\n",
      "training loss for step: 4.564019680023193\n",
      "training loss for step: 4.564698219299316\n",
      "training loss for step: 4.5647382736206055\n",
      "training loss for step: 4.5638532638549805\n",
      "training loss for step: 4.56367301940918\n",
      "training loss for step: 4.563468933105469\n",
      "training loss for step: 4.564319610595703\n",
      "training loss for step: 4.563727855682373\n",
      "training loss for step: 4.563453674316406\n",
      "training loss for step: 4.563488006591797\n",
      "training loss for step: 4.564116477966309\n",
      "training loss for step: 4.563940525054932\n",
      "training loss for step: 4.563500881195068\n",
      "training loss for step: 4.564123153686523\n",
      "training loss for step: 4.563507080078125\n",
      "training loss for step: 4.563692569732666\n",
      "training loss for step: 4.563998699188232\n",
      "training loss for step: 4.563730716705322\n",
      "training loss for step: 4.563874244689941\n",
      "training loss for step: 4.563980579376221\n",
      "training loss for step: 4.563553333282471\n",
      "training loss for step: 4.564167499542236\n",
      "training loss for step: 4.564347267150879\n",
      "training loss for step: 4.563956260681152\n",
      "training loss for step: 4.563740253448486\n",
      "training loss for step: 4.564397811889648\n",
      "training loss for step: 4.564707279205322\n",
      "training loss for step: 4.564471244812012\n",
      "training loss for step: 4.564399242401123\n",
      "training loss for step: 4.5637946128845215\n",
      "training loss for step: 4.565487384796143\n",
      "training loss for step: 4.566305160522461\n",
      "training loss for step: 4.566476821899414\n",
      "training loss for step: 4.565799713134766\n",
      "training loss for step: 4.56458854675293\n",
      "training loss for step: 4.563599109649658\n",
      "training loss for step: 4.565709114074707\n",
      "training loss for step: 4.565975666046143\n",
      "training loss for step: 4.566433429718018\n",
      "training loss for step: 4.565938949584961\n",
      "training loss for step: 4.565609931945801\n",
      "training loss for step: 4.564520359039307\n",
      "training loss for step: 4.564159870147705\n",
      "training loss for step: 4.564387321472168\n",
      "training loss for step: 4.5648980140686035\n",
      "training loss for step: 4.565169811248779\n",
      "training loss for step: 4.563897609710693\n",
      "training loss for step: 4.564587593078613\n",
      "training loss for step: 4.564606666564941\n",
      "training loss for step: 4.564791202545166\n",
      "training loss for step: 4.564181327819824\n",
      "training loss for step: 4.563655376434326\n",
      "training loss for step: 4.563918590545654\n",
      "training loss for step: 4.5639543533325195\n",
      "training loss for step: 4.56374979019165\n",
      "training loss for step: 4.563758850097656\n",
      "training loss for step: 4.56355619430542\n",
      "training loss for step: 4.56364631652832\n",
      "training loss for step: 4.563544273376465\n",
      "training loss for step: 4.563502311706543\n",
      "training loss for step: 4.56419038772583\n",
      "training loss for step: 4.563955307006836\n",
      "training loss for step: 4.563520431518555\n",
      "training loss for step: 4.56411600112915\n",
      "training loss for step: 4.564199924468994\n",
      "training loss for step: 4.5637712478637695\n",
      "training loss for step: 4.563584804534912\n",
      "training loss for step: 4.563490390777588\n",
      "training loss for step: 4.564484119415283\n",
      "training loss for step: 4.564615726470947\n",
      "training loss for step: 4.563838481903076\n",
      "training loss for step: 4.563948154449463\n",
      "training loss for step: 4.564395427703857\n",
      "training loss for step: 4.563596725463867\n",
      "training loss for step: 4.5635905265808105\n",
      "training loss for step: 4.563632488250732\n",
      "training loss for step: 4.5636491775512695\n",
      "training loss for step: 4.564362049102783\n",
      "training loss for step: 4.564154624938965\n",
      "training loss for step: 4.56459379196167\n",
      "training loss for step: 4.563459873199463\n",
      "training loss for step: 4.5634894371032715\n",
      "training loss for step: 4.563665866851807\n",
      "training loss for step: 4.563521385192871\n",
      "training loss for step: 4.564032077789307\n",
      "training loss for step: 4.563848495483398\n",
      "training loss for step: 4.563945770263672\n",
      "training loss for step: 4.563704013824463\n",
      "training loss for step: 4.563589096069336\n",
      "training loss for step: 4.564321994781494\n",
      "training loss for step: 4.564496040344238\n",
      "training loss for step: 4.564367294311523\n",
      "training loss for step: 4.563538551330566\n",
      "training loss for step: 4.563446521759033\n",
      "training loss for step: 4.5637102127075195\n",
      "training loss for step: 4.5635175704956055\n",
      "training loss for step: 4.563668251037598\n",
      "training loss for step: 4.5635986328125\n",
      "training loss for step: 4.563823223114014\n",
      "training loss for step: 4.563877105712891\n",
      "training loss for step: 4.563863277435303\n",
      "training loss for step: 4.5638532638549805\n",
      "training loss for step: 4.563846588134766\n",
      "training loss for step: 4.564502716064453\n",
      "training loss for step: 4.564233779907227\n",
      "training loss for step: 4.563587665557861\n",
      "training loss for step: 4.564070701599121\n",
      "training loss for step: 4.563450336456299\n",
      "training loss for step: 4.563702583312988\n",
      "training loss for step: 4.563755035400391\n",
      "training loss for step: 4.563519477844238\n",
      "training loss for step: 4.563517093658447\n",
      "training loss for step: 4.563671112060547\n",
      "training loss for step: 4.563724994659424\n",
      "training loss for step: 4.563721656799316\n",
      "training loss for step: 4.563596725463867\n",
      "training loss for step: 4.5638041496276855\n",
      "training loss for step: 4.563682556152344\n",
      "training loss for step: 4.563628673553467\n",
      "training loss for step: 4.563802242279053\n",
      "training loss for step: 4.563545227050781\n",
      "training loss for step: 4.56351375579834\n",
      "training loss for step: 4.564619541168213\n",
      "training loss for step: 4.564753532409668\n",
      "training loss for step: 4.5651068687438965\n",
      "training loss for step: 4.564214706420898\n",
      "training loss for step: 4.563506126403809\n",
      "training loss for step: 4.564159870147705\n",
      "training loss for step: 4.564368724822998\n",
      "training loss for step: 4.563477993011475\n",
      "training loss for step: 4.56395149230957\n",
      "training loss for step: 4.564236640930176\n",
      "training loss for step: 4.564000129699707\n",
      "training loss for step: 4.563670635223389\n",
      "training loss for step: 4.563612461090088\n",
      "training loss for step: 4.563586235046387\n",
      "training loss for step: 4.564522743225098\n",
      "training loss for step: 4.565090179443359\n",
      "training loss for step: 4.5647196769714355\n",
      "training loss for step: 4.5642571449279785\n",
      "training loss for step: 4.563828468322754\n",
      "training loss for step: 4.564037322998047\n",
      "training loss for step: 4.564676761627197\n",
      "training loss for step: 4.563629150390625\n",
      "training loss for step: 4.564609050750732\n",
      "training loss for step: 4.564548015594482\n",
      "training loss for step: 4.564843654632568\n",
      "training loss for step: 4.563941478729248\n",
      "training loss for step: 4.564249038696289\n",
      "training loss for step: 4.5642852783203125\n",
      "training loss for step: 4.564847469329834\n",
      "training loss for step: 4.563754081726074\n",
      "training loss for step: 4.563676357269287\n",
      "training loss for step: 4.564650058746338\n",
      "training loss for step: 4.563990592956543\n",
      "training loss for step: 4.563607215881348\n",
      "training loss for step: 4.5644073486328125\n",
      "training loss for step: 4.565159320831299\n",
      "training loss for step: 4.565101146697998\n",
      "training loss for step: 4.564352512359619\n",
      "training loss for step: 4.563849925994873\n",
      "training loss for step: 4.564249515533447\n",
      "training loss for step: 4.564157962799072\n",
      "training loss for step: 4.563835144042969\n",
      "training loss for step: 4.564280986785889\n",
      "training loss for step: 4.5647711753845215\n",
      "training loss for step: 4.564643859863281\n",
      "training loss for step: 4.564677715301514\n",
      "training loss for step: 4.563986778259277\n",
      "training loss for step: 4.564281463623047\n",
      "training loss for step: 4.563858985900879\n",
      "training loss for step: 4.563977241516113\n",
      "training loss for step: 4.563661575317383\n",
      "training loss for step: 4.563757419586182\n",
      "training loss for step: 4.563791751861572\n",
      "training loss for step: 4.563756942749023\n",
      "training loss for step: 4.5649027824401855\n",
      "training loss for step: 4.565136432647705\n",
      "training loss for step: 4.565196990966797\n",
      "training loss for step: 4.564424991607666\n",
      "training loss for step: 4.563634395599365\n",
      "training loss for step: 4.564924240112305\n",
      "training loss for step: 4.566004753112793\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28aa881cc09f43b99e3fabc6b978aded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec807b4b75b3494284bbc1397d9a749d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/447 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss for step: 4.666414737701416\n",
      "training loss for step: 4.665642261505127\n",
      "training loss for step: 4.665065765380859\n",
      "training loss for step: 4.663801670074463\n",
      "training loss for step: 4.664345741271973\n",
      "training loss for step: 4.66555118560791\n",
      "training loss for step: 4.665445327758789\n",
      "training loss for step: 4.665823459625244\n",
      "training loss for step: 4.664846420288086\n",
      "training loss for step: 4.6637468338012695\n",
      "training loss for step: 4.664606094360352\n",
      "training loss for step: 4.665704250335693\n",
      "training loss for step: 4.665897369384766\n",
      "training loss for step: 4.665445804595947\n",
      "training loss for step: 4.664493083953857\n",
      "training loss for step: 4.663508892059326\n",
      "training loss for step: 4.665024280548096\n",
      "training loss for step: 4.665921211242676\n",
      "training loss for step: 4.665785312652588\n",
      "training loss for step: 4.665822982788086\n",
      "training loss for step: 4.664666652679443\n",
      "training loss for step: 4.663928985595703\n",
      "training loss for step: 4.664853096008301\n",
      "training loss for step: 4.665245056152344\n",
      "training loss for step: 4.665548324584961\n",
      "training loss for step: 4.66567325592041\n",
      "training loss for step: 4.665031909942627\n",
      "training loss for step: 4.664291858673096\n",
      "training loss for step: 4.664915084838867\n",
      "training loss for step: 4.6656084060668945\n",
      "training loss for step: 4.666266441345215\n",
      "training loss for step: 4.665761947631836\n",
      "training loss for step: 4.6650776863098145\n",
      "training loss for step: 4.663527488708496\n",
      "training loss for step: 4.66477108001709\n",
      "training loss for step: 4.665491580963135\n",
      "training loss for step: 4.665731906890869\n",
      "training loss for step: 4.6656670570373535\n",
      "training loss for step: 4.665040493011475\n",
      "training loss for step: 4.663841724395752\n",
      "training loss for step: 4.664849281311035\n",
      "training loss for step: 4.665755271911621\n",
      "training loss for step: 4.665595531463623\n",
      "training loss for step: 4.665520191192627\n",
      "training loss for step: 4.664860248565674\n",
      "training loss for step: 4.6638264656066895\n",
      "training loss for step: 4.66494083404541\n",
      "training loss for step: 4.6654486656188965\n",
      "training loss for step: 4.666110038757324\n",
      "training loss for step: 4.66538667678833\n",
      "training loss for step: 4.665256500244141\n",
      "training loss for step: 4.664089202880859\n",
      "training loss for step: 4.664459705352783\n",
      "training loss for step: 4.665470123291016\n",
      "training loss for step: 4.665640830993652\n",
      "training loss for step: 4.665563583374023\n",
      "training loss for step: 4.664402484893799\n",
      "training loss for step: 4.663761138916016\n",
      "training loss for step: 4.6647491455078125\n",
      "training loss for step: 4.665253639221191\n",
      "training loss for step: 4.666159152984619\n",
      "training loss for step: 4.6654253005981445\n",
      "training loss for step: 4.664637088775635\n",
      "training loss for step: 4.664144515991211\n",
      "training loss for step: 4.665212631225586\n",
      "training loss for step: 4.666599750518799\n",
      "training loss for step: 4.665735721588135\n",
      "training loss for step: 4.665435791015625\n",
      "training loss for step: 4.665173053741455\n",
      "training loss for step: 4.663541793823242\n",
      "training loss for step: 4.663484573364258\n",
      "training loss for step: 4.663670539855957\n",
      "training loss for step: 4.664464473724365\n",
      "training loss for step: 4.663942337036133\n",
      "training loss for step: 4.663601875305176\n",
      "training loss for step: 4.663887023925781\n",
      "training loss for step: 4.6638875007629395\n",
      "training loss for step: 4.663676738739014\n",
      "training loss for step: 4.663846015930176\n",
      "training loss for step: 4.663878917694092\n",
      "training loss for step: 4.663640975952148\n",
      "training loss for step: 4.6634721755981445\n",
      "training loss for step: 4.6636962890625\n",
      "training loss for step: 4.663839817047119\n",
      "training loss for step: 4.663695812225342\n",
      "training loss for step: 4.66344690322876\n",
      "training loss for step: 4.66401481628418\n",
      "training loss for step: 4.663893699645996\n",
      "training loss for step: 4.663547992706299\n",
      "training loss for step: 4.663532257080078\n",
      "training loss for step: 4.663486957550049\n",
      "training loss for step: 4.663715839385986\n",
      "training loss for step: 4.663675308227539\n",
      "training loss for step: 4.663731575012207\n",
      "training loss for step: 4.663506984710693\n",
      "training loss for step: 4.66457462310791\n",
      "training loss for step: 4.6646575927734375\n",
      "training loss for step: 4.664493560791016\n",
      "training loss for step: 4.663969039916992\n",
      "training loss for step: 4.6637725830078125\n",
      "training loss for step: 4.664353847503662\n",
      "training loss for step: 4.664596080780029\n",
      "training loss for step: 4.6639885902404785\n",
      "training loss for step: 4.66426420211792\n",
      "training loss for step: 4.664572715759277\n",
      "training loss for step: 4.664619445800781\n",
      "training loss for step: 4.66358757019043\n",
      "training loss for step: 4.664218902587891\n",
      "training loss for step: 4.664692401885986\n",
      "training loss for step: 4.664529323577881\n",
      "training loss for step: 4.6639485359191895\n",
      "training loss for step: 4.663745880126953\n",
      "training loss for step: 4.6641926765441895\n",
      "training loss for step: 4.6640472412109375\n",
      "training loss for step: 4.663542747497559\n",
      "training loss for step: 4.664124488830566\n",
      "training loss for step: 4.66492223739624\n",
      "training loss for step: 4.664670467376709\n",
      "training loss for step: 4.664216041564941\n",
      "training loss for step: 4.663635730743408\n",
      "training loss for step: 4.664148807525635\n",
      "training loss for step: 4.663877010345459\n",
      "training loss for step: 4.6635918617248535\n",
      "training loss for step: 4.663668155670166\n",
      "training loss for step: 4.663815975189209\n",
      "training loss for step: 4.663808822631836\n",
      "training loss for step: 4.66380500793457\n",
      "training loss for step: 4.6636199951171875\n",
      "training loss for step: 4.663511753082275\n",
      "training loss for step: 4.663707733154297\n",
      "training loss for step: 4.6635236740112305\n",
      "training loss for step: 4.6650285720825195\n",
      "training loss for step: 4.665437698364258\n",
      "training loss for step: 4.665297985076904\n",
      "training loss for step: 4.664485454559326\n",
      "training loss for step: 4.663460731506348\n",
      "training loss for step: 4.664112567901611\n",
      "training loss for step: 4.663662910461426\n",
      "training loss for step: 4.66364049911499\n",
      "training loss for step: 4.664001941680908\n",
      "training loss for step: 4.663549423217773\n",
      "training loss for step: 4.664132595062256\n",
      "training loss for step: 4.665079116821289\n",
      "training loss for step: 4.66477108001709\n",
      "training loss for step: 4.664116382598877\n",
      "training loss for step: 4.664211750030518\n",
      "training loss for step: 4.664485454559326\n",
      "training loss for step: 4.664276123046875\n",
      "training loss for step: 4.663806438446045\n",
      "training loss for step: 4.664188385009766\n",
      "training loss for step: 4.664055824279785\n",
      "training loss for step: 4.6641435623168945\n",
      "training loss for step: 4.663475036621094\n",
      "training loss for step: 4.664702892303467\n",
      "training loss for step: 4.665060043334961\n",
      "training loss for step: 4.664716720581055\n",
      "training loss for step: 4.664292335510254\n",
      "training loss for step: 4.663511753082275\n",
      "training loss for step: 4.664097785949707\n",
      "training loss for step: 4.664022445678711\n",
      "training loss for step: 4.663771629333496\n",
      "training loss for step: 4.6636962890625\n",
      "training loss for step: 4.663775444030762\n",
      "training loss for step: 4.663520336151123\n",
      "training loss for step: 4.663757801055908\n",
      "training loss for step: 4.6639299392700195\n",
      "training loss for step: 4.663510799407959\n",
      "training loss for step: 4.663489818572998\n",
      "training loss for step: 4.664443492889404\n",
      "training loss for step: 4.663807392120361\n",
      "training loss for step: 4.663537979125977\n",
      "training loss for step: 4.664218902587891\n",
      "training loss for step: 4.664392948150635\n",
      "training loss for step: 4.664326190948486\n",
      "training loss for step: 4.663924694061279\n",
      "training loss for step: 4.6645283699035645\n",
      "training loss for step: 4.664834499359131\n",
      "training loss for step: 4.664791107177734\n",
      "training loss for step: 4.664061069488525\n",
      "training loss for step: 4.6635942459106445\n",
      "training loss for step: 4.665273189544678\n",
      "training loss for step: 4.666003704071045\n",
      "training loss for step: 4.665942192077637\n",
      "training loss for step: 4.665420055389404\n",
      "training loss for step: 4.664907455444336\n",
      "training loss for step: 4.663510799407959\n",
      "training loss for step: 4.663975715637207\n",
      "training loss for step: 4.663879871368408\n",
      "training loss for step: 4.663729190826416\n",
      "training loss for step: 4.663560390472412\n",
      "training loss for step: 4.664555072784424\n",
      "training loss for step: 4.664121150970459\n",
      "training loss for step: 4.663476467132568\n",
      "training loss for step: 4.663511276245117\n",
      "training loss for step: 4.66371488571167\n",
      "training loss for step: 4.663558483123779\n",
      "training loss for step: 4.663855075836182\n",
      "training loss for step: 4.663502216339111\n",
      "training loss for step: 4.663736820220947\n",
      "training loss for step: 4.6635332107543945\n",
      "training loss for step: 4.663863658905029\n",
      "training loss for step: 4.664297103881836\n",
      "training loss for step: 4.663694381713867\n",
      "training loss for step: 4.664137840270996\n",
      "training loss for step: 4.6647467613220215\n",
      "training loss for step: 4.664478302001953\n",
      "training loss for step: 4.663686275482178\n",
      "training loss for step: 4.663965225219727\n",
      "training loss for step: 4.664798259735107\n",
      "training loss for step: 4.6647772789001465\n",
      "training loss for step: 4.663949012756348\n",
      "training loss for step: 4.664247989654541\n",
      "training loss for step: 4.663939952850342\n",
      "training loss for step: 4.6642746925354\n",
      "training loss for step: 4.663599967956543\n",
      "training loss for step: 4.6634697914123535\n",
      "training loss for step: 4.663872718811035\n",
      "training loss for step: 4.663517475128174\n",
      "training loss for step: 4.663801670074463\n",
      "training loss for step: 4.6637468338012695\n",
      "training loss for step: 4.663449764251709\n",
      "training loss for step: 4.663938045501709\n",
      "training loss for step: 4.664090156555176\n",
      "training loss for step: 4.663862705230713\n",
      "training loss for step: 4.6639018058776855\n",
      "training loss for step: 4.664504528045654\n",
      "training loss for step: 4.664381504058838\n",
      "training loss for step: 4.663570404052734\n",
      "training loss for step: 4.663739204406738\n",
      "training loss for step: 4.663463592529297\n",
      "training loss for step: 4.663474082946777\n",
      "training loss for step: 4.6642045974731445\n",
      "training loss for step: 4.6639227867126465\n",
      "training loss for step: 4.663637161254883\n",
      "training loss for step: 4.664025783538818\n",
      "training loss for step: 4.664333343505859\n",
      "training loss for step: 4.663909435272217\n",
      "training loss for step: 4.663908958435059\n",
      "training loss for step: 4.6646552085876465\n",
      "training loss for step: 4.66513204574585\n",
      "training loss for step: 4.6649370193481445\n",
      "training loss for step: 4.664782524108887\n",
      "training loss for step: 4.663697719573975\n",
      "training loss for step: 4.6647257804870605\n",
      "training loss for step: 4.665554046630859\n",
      "training loss for step: 4.6652750968933105\n",
      "training loss for step: 4.665188789367676\n",
      "training loss for step: 4.66439962387085\n",
      "training loss for step: 4.663536071777344\n",
      "training loss for step: 4.66467809677124\n",
      "training loss for step: 4.664584159851074\n",
      "training loss for step: 4.664491176605225\n",
      "training loss for step: 4.663641452789307\n",
      "training loss for step: 4.664092540740967\n",
      "training loss for step: 4.664016246795654\n",
      "training loss for step: 4.663997650146484\n",
      "training loss for step: 4.663936138153076\n",
      "training loss for step: 4.663640022277832\n",
      "training loss for step: 4.6640944480896\n",
      "training loss for step: 4.664426326751709\n",
      "training loss for step: 4.664158344268799\n",
      "training loss for step: 4.6638712882995605\n",
      "training loss for step: 4.6645097732543945\n",
      "training loss for step: 4.6652302742004395\n",
      "training loss for step: 4.664684295654297\n",
      "training loss for step: 4.664397716522217\n",
      "training loss for step: 4.663644790649414\n",
      "training loss for step: 4.664706707000732\n",
      "training loss for step: 4.6652750968933105\n",
      "training loss for step: 4.665205955505371\n",
      "training loss for step: 4.665480613708496\n",
      "training loss for step: 4.664678573608398\n",
      "training loss for step: 4.663448810577393\n",
      "training loss for step: 4.664621353149414\n",
      "training loss for step: 4.66525936126709\n",
      "training loss for step: 4.665210723876953\n",
      "training loss for step: 4.664687156677246\n",
      "training loss for step: 4.663562774658203\n",
      "training loss for step: 4.66475772857666\n",
      "training loss for step: 4.665492534637451\n",
      "training loss for step: 4.665372848510742\n",
      "training loss for step: 4.664910316467285\n",
      "training loss for step: 4.664166450500488\n",
      "training loss for step: 4.663780212402344\n",
      "training loss for step: 4.664331912994385\n",
      "training loss for step: 4.664703369140625\n",
      "training loss for step: 4.663844108581543\n",
      "training loss for step: 4.6638054847717285\n",
      "training loss for step: 4.664240837097168\n",
      "training loss for step: 4.663655757904053\n",
      "training loss for step: 4.663837432861328\n",
      "training loss for step: 4.663663387298584\n",
      "training loss for step: 4.663591384887695\n",
      "training loss for step: 4.6635894775390625\n",
      "training loss for step: 4.663535118103027\n",
      "training loss for step: 4.66346549987793\n",
      "training loss for step: 4.6637420654296875\n",
      "training loss for step: 4.663801670074463\n",
      "training loss for step: 4.663517951965332\n",
      "training loss for step: 4.664131164550781\n",
      "training loss for step: 4.663450241088867\n",
      "training loss for step: 4.663959980010986\n",
      "training loss for step: 4.663828372955322\n",
      "training loss for step: 4.663710117340088\n",
      "training loss for step: 4.6640424728393555\n",
      "training loss for step: 4.663569927215576\n",
      "training loss for step: 4.664333343505859\n",
      "training loss for step: 4.664536476135254\n",
      "training loss for step: 4.6646199226379395\n",
      "training loss for step: 4.6639628410339355\n",
      "training loss for step: 4.664213180541992\n",
      "training loss for step: 4.664159297943115\n",
      "training loss for step: 4.66425895690918\n",
      "training loss for step: 4.663657188415527\n",
      "training loss for step: 4.663906097412109\n",
      "training loss for step: 4.664717674255371\n",
      "training loss for step: 4.66412353515625\n",
      "training loss for step: 4.663643836975098\n",
      "training loss for step: 4.664233207702637\n",
      "training loss for step: 4.6648268699646\n",
      "training loss for step: 4.664709091186523\n",
      "training loss for step: 4.664076805114746\n",
      "training loss for step: 4.664200305938721\n",
      "training loss for step: 4.6641411781311035\n",
      "training loss for step: 4.664008617401123\n",
      "training loss for step: 4.663527011871338\n",
      "training loss for step: 4.664490699768066\n",
      "training loss for step: 4.6645283699035645\n",
      "training loss for step: 4.664764881134033\n",
      "training loss for step: 4.66347599029541\n",
      "training loss for step: 4.664067268371582\n",
      "training loss for step: 4.664289951324463\n",
      "training loss for step: 4.664253234863281\n",
      "training loss for step: 4.663686275482178\n",
      "training loss for step: 4.663722038269043\n",
      "training loss for step: 4.663478374481201\n",
      "training loss for step: 4.663792133331299\n",
      "training loss for step: 4.663479328155518\n",
      "training loss for step: 4.664639949798584\n",
      "training loss for step: 4.664707183837891\n",
      "training loss for step: 4.665218830108643\n",
      "training loss for step: 4.663934230804443\n",
      "training loss for step: 4.664270401000977\n",
      "training loss for step: 4.664444923400879\n",
      "training loss for step: 4.66454553604126\n",
      "training loss for step: 4.664078235626221\n",
      "training loss for step: 4.663990020751953\n",
      "training loss for step: 4.664409637451172\n",
      "training loss for step: 4.664097309112549\n",
      "training loss for step: 4.663448810577393\n",
      "training loss for step: 4.663968563079834\n",
      "training loss for step: 4.664037227630615\n",
      "training loss for step: 4.663961410522461\n",
      "training loss for step: 4.66378116607666\n",
      "training loss for step: 4.66439151763916\n",
      "training loss for step: 4.663759231567383\n",
      "training loss for step: 4.663820743560791\n",
      "training loss for step: 4.664053440093994\n",
      "training loss for step: 4.6638102531433105\n",
      "training loss for step: 4.663895606994629\n",
      "training loss for step: 4.6642022132873535\n",
      "training loss for step: 4.663585662841797\n",
      "training loss for step: 4.663886547088623\n",
      "training loss for step: 4.664023399353027\n",
      "training loss for step: 4.6639909744262695\n",
      "training loss for step: 4.663929462432861\n",
      "training loss for step: 4.663985252380371\n",
      "training loss for step: 4.6636061668396\n",
      "training loss for step: 4.663615703582764\n",
      "training loss for step: 4.664061069488525\n",
      "training loss for step: 4.663777828216553\n",
      "training loss for step: 4.663896560668945\n",
      "training loss for step: 4.664209842681885\n",
      "training loss for step: 4.66366720199585\n",
      "training loss for step: 4.664002895355225\n",
      "training loss for step: 4.6644134521484375\n",
      "training loss for step: 4.6638665199279785\n",
      "training loss for step: 4.663638591766357\n",
      "training loss for step: 4.664168834686279\n",
      "training loss for step: 4.663970470428467\n",
      "training loss for step: 4.664062023162842\n",
      "training loss for step: 4.664135932922363\n",
      "training loss for step: 4.6636643409729\n",
      "training loss for step: 4.663604736328125\n",
      "training loss for step: 4.663960933685303\n",
      "training loss for step: 4.6638102531433105\n",
      "training loss for step: 4.663901329040527\n",
      "training loss for step: 4.664207935333252\n",
      "training loss for step: 4.664067268371582\n",
      "training loss for step: 4.663673400878906\n",
      "training loss for step: 4.663916110992432\n",
      "training loss for step: 4.663475513458252\n",
      "training loss for step: 4.663631916046143\n",
      "training loss for step: 4.664377212524414\n",
      "training loss for step: 4.663665771484375\n",
      "training loss for step: 4.664185523986816\n",
      "training loss for step: 4.663858890533447\n",
      "training loss for step: 4.6635613441467285\n",
      "training loss for step: 4.663914203643799\n",
      "training loss for step: 4.6639838218688965\n",
      "training loss for step: 4.663729190826416\n",
      "training loss for step: 4.663891792297363\n",
      "training loss for step: 4.664180278778076\n",
      "training loss for step: 4.6643900871276855\n",
      "training loss for step: 4.663553237915039\n",
      "training loss for step: 4.663573741912842\n",
      "training loss for step: 4.663990020751953\n",
      "training loss for step: 4.663692951202393\n",
      "training loss for step: 4.663818359375\n",
      "training loss for step: 4.664137363433838\n",
      "training loss for step: 4.663975715637207\n",
      "training loss for step: 4.663951873779297\n",
      "training loss for step: 4.664037227630615\n",
      "training loss for step: 4.6639018058776855\n",
      "training loss for step: 4.663966178894043\n",
      "training loss for step: 4.664198398590088\n",
      "training loss for step: 4.663487911224365\n",
      "training loss for step: 4.663925647735596\n",
      "training loss for step: 4.664253234863281\n",
      "training loss for step: 4.66395902633667\n",
      "training loss for step: 4.663777828216553\n",
      "training loss for step: 4.66393518447876\n",
      "training loss for step: 4.663496017456055\n",
      "training loss for step: 4.663658618927002\n",
      "training loss for step: 4.664369583129883\n",
      "training loss for step: 4.66395902633667\n",
      "training loss for step: 4.664196968078613\n",
      "training loss for step: 4.66377067565918\n",
      "training loss for step: 4.663710594177246\n",
      "training loss for step: 4.6642231941223145\n",
      "training loss for step: 4.664189338684082\n",
      "training loss for step: 4.663731575012207\n",
      "training loss for step: 4.663586139678955\n",
      "training loss for step: 4.665431022644043\n",
      "training loss for step: 4.665374755859375\n",
      "training loss for step: 4.665650844573975\n",
      "training loss for step: 4.665159702301025\n",
      "training loss for step: 4.664318561553955\n",
      "training loss for step: 4.664019584655762\n",
      "training loss for step: 4.664516925811768\n",
      "training loss for step: 4.664608955383301\n",
      "training loss for step: 4.664146423339844\n",
      "training loss for step: 4.664103984832764\n",
      "training loss for step: 4.6645026206970215\n",
      "training loss for step: 4.665127754211426\n",
      "training loss for step: 4.665373802185059\n",
      "training loss for step: 4.6647162437438965\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15be469218d5411bb8b49ce9ca123186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26098cbf980a49889a8c48112aae3c53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/447 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss for step: 4.764326095581055\n",
      "training loss for step: 4.764374256134033\n",
      "training loss for step: 4.765026569366455\n",
      "training loss for step: 4.765172481536865\n",
      "training loss for step: 4.765168190002441\n",
      "training loss for step: 4.763813495635986\n",
      "training loss for step: 4.764170169830322\n",
      "training loss for step: 4.764952182769775\n",
      "training loss for step: 4.764833927154541\n",
      "training loss for step: 4.764358043670654\n",
      "training loss for step: 4.7635698318481445\n",
      "training loss for step: 4.764887809753418\n",
      "training loss for step: 4.7655181884765625\n",
      "training loss for step: 4.765746116638184\n",
      "training loss for step: 4.765130043029785\n",
      "training loss for step: 4.764283180236816\n",
      "training loss for step: 4.763544082641602\n",
      "training loss for step: 4.764492034912109\n",
      "training loss for step: 4.76439905166626\n",
      "training loss for step: 4.764066696166992\n",
      "training loss for step: 4.763708591461182\n",
      "training loss for step: 4.763746738433838\n",
      "training loss for step: 4.764023303985596\n",
      "training loss for step: 4.7636590003967285\n",
      "training loss for step: 4.764471530914307\n",
      "training loss for step: 4.763604640960693\n",
      "training loss for step: 4.763467788696289\n",
      "training loss for step: 4.764022350311279\n",
      "training loss for step: 4.764395236968994\n",
      "training loss for step: 4.763797283172607\n",
      "training loss for step: 4.763858795166016\n",
      "training loss for step: 4.763877868652344\n",
      "training loss for step: 4.763487339019775\n",
      "training loss for step: 4.763851642608643\n",
      "training loss for step: 4.764260292053223\n",
      "training loss for step: 4.763803005218506\n",
      "training loss for step: 4.764302730560303\n",
      "training loss for step: 4.764101505279541\n",
      "training loss for step: 4.763544082641602\n",
      "training loss for step: 4.763827800750732\n",
      "training loss for step: 4.7641425132751465\n",
      "training loss for step: 4.763491630554199\n",
      "training loss for step: 4.764122486114502\n",
      "training loss for step: 4.763467311859131\n",
      "training loss for step: 4.76358699798584\n",
      "training loss for step: 4.7636284828186035\n",
      "training loss for step: 4.7634687423706055\n",
      "training loss for step: 4.7643842697143555\n",
      "training loss for step: 4.764686584472656\n",
      "training loss for step: 4.763964653015137\n",
      "training loss for step: 4.763759136199951\n",
      "training loss for step: 4.763781547546387\n",
      "training loss for step: 4.7635626792907715\n",
      "training loss for step: 4.764260768890381\n",
      "training loss for step: 4.764662265777588\n",
      "training loss for step: 4.763782978057861\n",
      "training loss for step: 4.763937473297119\n",
      "training loss for step: 4.764053821563721\n",
      "training loss for step: 4.763956546783447\n",
      "training loss for step: 4.764013767242432\n",
      "training loss for step: 4.764090538024902\n",
      "training loss for step: 4.763782501220703\n",
      "training loss for step: 4.763850688934326\n",
      "training loss for step: 4.764354228973389\n",
      "training loss for step: 4.763669967651367\n",
      "training loss for step: 4.763628959655762\n",
      "training loss for step: 4.764164924621582\n",
      "training loss for step: 4.7635908126831055\n",
      "training loss for step: 4.7638726234436035\n",
      "training loss for step: 4.763628005981445\n",
      "training loss for step: 4.764420032501221\n",
      "training loss for step: 4.76407527923584\n",
      "training loss for step: 4.764639377593994\n",
      "training loss for step: 4.76370906829834\n",
      "training loss for step: 4.763821125030518\n",
      "training loss for step: 4.7634992599487305\n",
      "training loss for step: 4.763765811920166\n",
      "training loss for step: 4.76351261138916\n",
      "training loss for step: 4.763462543487549\n",
      "training loss for step: 4.764155864715576\n",
      "training loss for step: 4.764562606811523\n",
      "training loss for step: 4.763710975646973\n",
      "training loss for step: 4.763803005218506\n",
      "training loss for step: 4.763949871063232\n",
      "training loss for step: 4.7639055252075195\n",
      "training loss for step: 4.763700008392334\n",
      "training loss for step: 4.76392936706543\n",
      "training loss for step: 4.7635955810546875\n",
      "training loss for step: 4.763961315155029\n",
      "training loss for step: 4.764185428619385\n",
      "training loss for step: 4.7639055252075195\n",
      "training loss for step: 4.76371955871582\n",
      "training loss for step: 4.763908386230469\n",
      "training loss for step: 4.763539791107178\n",
      "training loss for step: 4.764013767242432\n",
      "training loss for step: 4.764530658721924\n",
      "training loss for step: 4.764191150665283\n",
      "training loss for step: 4.763503551483154\n",
      "training loss for step: 4.765111923217773\n",
      "training loss for step: 4.765665531158447\n",
      "training loss for step: 4.765635013580322\n",
      "training loss for step: 4.764931678771973\n",
      "training loss for step: 4.764101982116699\n",
      "training loss for step: 4.763509750366211\n",
      "training loss for step: 4.764710903167725\n",
      "training loss for step: 4.764855861663818\n",
      "training loss for step: 4.764225959777832\n",
      "training loss for step: 4.76400899887085\n",
      "training loss for step: 4.764462471008301\n",
      "training loss for step: 4.765909194946289\n",
      "training loss for step: 4.765417575836182\n",
      "training loss for step: 4.764514446258545\n",
      "training loss for step: 4.763552665710449\n",
      "training loss for step: 4.764130592346191\n",
      "training loss for step: 4.765168190002441\n",
      "training loss for step: 4.764125823974609\n",
      "training loss for step: 4.764847278594971\n",
      "training loss for step: 4.763566017150879\n",
      "training loss for step: 4.764021396636963\n",
      "training loss for step: 4.765650749206543\n",
      "training loss for step: 4.765011310577393\n",
      "training loss for step: 4.7649078369140625\n",
      "training loss for step: 4.763909816741943\n",
      "training loss for step: 4.764401435852051\n",
      "training loss for step: 4.765381336212158\n",
      "training loss for step: 4.7652201652526855\n",
      "training loss for step: 4.764528751373291\n",
      "training loss for step: 4.763913631439209\n",
      "training loss for step: 4.764077663421631\n",
      "training loss for step: 4.765430927276611\n",
      "training loss for step: 4.765447616577148\n",
      "training loss for step: 4.764390468597412\n",
      "training loss for step: 4.764049053192139\n",
      "training loss for step: 4.764125347137451\n",
      "training loss for step: 4.765244483947754\n",
      "training loss for step: 4.764650344848633\n",
      "training loss for step: 4.764548301696777\n",
      "training loss for step: 4.763940334320068\n",
      "training loss for step: 4.763734340667725\n",
      "training loss for step: 4.763495922088623\n",
      "training loss for step: 4.764321327209473\n",
      "training loss for step: 4.764378547668457\n",
      "training loss for step: 4.764484882354736\n",
      "training loss for step: 4.763650417327881\n",
      "training loss for step: 4.7646164894104\n",
      "training loss for step: 4.764779567718506\n",
      "training loss for step: 4.765326499938965\n",
      "training loss for step: 4.764622211456299\n",
      "training loss for step: 4.763963222503662\n",
      "training loss for step: 4.7641401290893555\n",
      "training loss for step: 4.764986038208008\n",
      "training loss for step: 4.764718055725098\n",
      "training loss for step: 4.764524936676025\n",
      "training loss for step: 4.763853549957275\n",
      "training loss for step: 4.764530181884766\n",
      "training loss for step: 4.764789581298828\n",
      "training loss for step: 4.765097618103027\n",
      "training loss for step: 4.7646484375\n",
      "training loss for step: 4.7638654708862305\n",
      "training loss for step: 4.76436710357666\n",
      "training loss for step: 4.764951229095459\n",
      "training loss for step: 4.765004634857178\n",
      "training loss for step: 4.764631271362305\n",
      "training loss for step: 4.76375675201416\n",
      "training loss for step: 4.764552116394043\n",
      "training loss for step: 4.764962673187256\n",
      "training loss for step: 4.764901638031006\n",
      "training loss for step: 4.764532089233398\n",
      "training loss for step: 4.763618469238281\n",
      "training loss for step: 4.764682769775391\n",
      "training loss for step: 4.765204429626465\n",
      "training loss for step: 4.765377521514893\n",
      "training loss for step: 4.764945983886719\n",
      "training loss for step: 4.764118194580078\n",
      "training loss for step: 4.764116287231445\n",
      "training loss for step: 4.764826774597168\n",
      "training loss for step: 4.764822006225586\n",
      "training loss for step: 4.764326572418213\n",
      "training loss for step: 4.763548851013184\n",
      "training loss for step: 4.764913082122803\n",
      "training loss for step: 4.765389442443848\n",
      "training loss for step: 4.765346050262451\n",
      "training loss for step: 4.765310287475586\n",
      "training loss for step: 4.764214038848877\n",
      "training loss for step: 4.764195919036865\n",
      "training loss for step: 4.764848232269287\n",
      "training loss for step: 4.764631271362305\n",
      "training loss for step: 4.764370918273926\n",
      "training loss for step: 4.763540744781494\n",
      "training loss for step: 4.763584136962891\n",
      "training loss for step: 4.763660907745361\n",
      "training loss for step: 4.764023303985596\n",
      "training loss for step: 4.764192581176758\n",
      "training loss for step: 4.76389741897583\n",
      "training loss for step: 4.7636847496032715\n",
      "training loss for step: 4.764101028442383\n",
      "training loss for step: 4.763901710510254\n",
      "training loss for step: 4.7638678550720215\n",
      "training loss for step: 4.763925552368164\n",
      "training loss for step: 4.763455867767334\n",
      "training loss for step: 4.763982772827148\n",
      "training loss for step: 4.763626575469971\n",
      "training loss for step: 4.763678073883057\n",
      "training loss for step: 4.763556003570557\n",
      "training loss for step: 4.764267921447754\n",
      "training loss for step: 4.764537811279297\n",
      "training loss for step: 4.763961315155029\n",
      "training loss for step: 4.763554573059082\n",
      "training loss for step: 4.764427185058594\n",
      "training loss for step: 4.764986515045166\n",
      "training loss for step: 4.764836311340332\n",
      "training loss for step: 4.7646565437316895\n",
      "training loss for step: 4.763865947723389\n",
      "training loss for step: 4.764559745788574\n",
      "training loss for step: 4.7652177810668945\n",
      "training loss for step: 4.765334606170654\n",
      "training loss for step: 4.764526844024658\n",
      "training loss for step: 4.764123439788818\n",
      "training loss for step: 4.764227390289307\n",
      "training loss for step: 4.764644622802734\n",
      "training loss for step: 4.76456356048584\n",
      "training loss for step: 4.7642951011657715\n",
      "training loss for step: 4.763827800750732\n",
      "training loss for step: 4.764711380004883\n",
      "training loss for step: 4.7653913497924805\n",
      "training loss for step: 4.765543460845947\n",
      "training loss for step: 4.7652692794799805\n",
      "training loss for step: 4.764140605926514\n",
      "training loss for step: 4.7637224197387695\n",
      "training loss for step: 4.764354705810547\n",
      "training loss for step: 4.764427185058594\n",
      "training loss for step: 4.764211177825928\n",
      "training loss for step: 4.7636847496032715\n",
      "training loss for step: 4.763876914978027\n",
      "training loss for step: 4.763777256011963\n",
      "training loss for step: 4.76352596282959\n",
      "training loss for step: 4.763634204864502\n",
      "training loss for step: 4.763645172119141\n",
      "training loss for step: 4.763509273529053\n",
      "training loss for step: 4.763579845428467\n",
      "training loss for step: 4.764101982116699\n",
      "training loss for step: 4.764812469482422\n",
      "training loss for step: 4.764247417449951\n",
      "training loss for step: 4.763513088226318\n",
      "training loss for step: 4.764431476593018\n",
      "training loss for step: 4.765271186828613\n",
      "training loss for step: 4.765035629272461\n",
      "training loss for step: 4.764712333679199\n",
      "training loss for step: 4.764037132263184\n",
      "training loss for step: 4.76466178894043\n",
      "training loss for step: 4.7641987800598145\n",
      "training loss for step: 4.763905048370361\n",
      "training loss for step: 4.764229774475098\n",
      "training loss for step: 4.763774394989014\n",
      "training loss for step: 4.76421594619751\n",
      "training loss for step: 4.764316558837891\n",
      "training loss for step: 4.764211654663086\n",
      "training loss for step: 4.763718128204346\n",
      "training loss for step: 4.763960838317871\n",
      "training loss for step: 4.763493061065674\n",
      "training loss for step: 4.763770580291748\n",
      "training loss for step: 4.763471603393555\n",
      "training loss for step: 4.763669013977051\n",
      "training loss for step: 4.764458656311035\n",
      "training loss for step: 4.764103889465332\n",
      "training loss for step: 4.7636213302612305\n",
      "training loss for step: 4.763620376586914\n",
      "training loss for step: 4.76404333114624\n",
      "training loss for step: 4.764269828796387\n",
      "training loss for step: 4.765193939208984\n",
      "training loss for step: 4.763813018798828\n",
      "training loss for step: 4.763584613800049\n",
      "training loss for step: 4.7641496658325195\n",
      "training loss for step: 4.764625549316406\n",
      "training loss for step: 4.765054702758789\n",
      "training loss for step: 4.764477252960205\n",
      "training loss for step: 4.764215469360352\n",
      "training loss for step: 4.764288902282715\n",
      "training loss for step: 4.764053821563721\n",
      "training loss for step: 4.765085220336914\n",
      "training loss for step: 4.764720439910889\n",
      "training loss for step: 4.763567924499512\n",
      "training loss for step: 4.763548374176025\n",
      "training loss for step: 4.76353120803833\n",
      "training loss for step: 4.765707492828369\n",
      "training loss for step: 4.765353202819824\n",
      "training loss for step: 4.7656569480896\n",
      "training loss for step: 4.766164302825928\n",
      "training loss for step: 4.765215873718262\n",
      "training loss for step: 4.763570308685303\n",
      "training loss for step: 4.764578342437744\n",
      "training loss for step: 4.765327453613281\n",
      "training loss for step: 4.764955520629883\n",
      "training loss for step: 4.76386022567749\n",
      "training loss for step: 4.763864994049072\n",
      "training loss for step: 4.764194965362549\n",
      "training loss for step: 4.76381254196167\n",
      "training loss for step: 4.763914108276367\n",
      "training loss for step: 4.76375150680542\n",
      "training loss for step: 4.764288902282715\n",
      "training loss for step: 4.76389217376709\n",
      "training loss for step: 4.763932704925537\n",
      "training loss for step: 4.763620376586914\n",
      "training loss for step: 4.7640886306762695\n",
      "training loss for step: 4.763817310333252\n",
      "training loss for step: 4.763694763183594\n",
      "training loss for step: 4.763559341430664\n",
      "training loss for step: 4.763614177703857\n",
      "training loss for step: 4.763817310333252\n",
      "training loss for step: 4.76383113861084\n",
      "training loss for step: 4.764012813568115\n",
      "training loss for step: 4.763638973236084\n",
      "training loss for step: 4.764087677001953\n",
      "training loss for step: 4.76429557800293\n",
      "training loss for step: 4.763916969299316\n",
      "training loss for step: 4.763514995574951\n",
      "training loss for step: 4.7644429206848145\n",
      "training loss for step: 4.765237808227539\n",
      "training loss for step: 4.765135765075684\n",
      "training loss for step: 4.764702320098877\n"
     ]
    }
   ],
   "source": [
    "# Cuda issues\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "vocab_size = tokenizer_recipes.vocab_size\n",
    "param_path = os.path.join(GOOGLE_DRIVE_PATH, \"models/best_model_jake.pth\")\n",
    "recipe_embeddings_path  = os.path.join(GOOGLE_DRIVE_PATH, \"models/recipe_embeddings.pth\")\n",
    "\n",
    "kwargs = {\n",
    "    'epochs': 40,\n",
    "    'ingredient_tokens': filtered_df['tokenized_ingredients'].to_list(),\n",
    "    'instruction_tokens': filtered_df['tokenized_instructions'].to_list(),\n",
    "    'title_tokens': filtered_df['tokenized_titles'].to_list(),\n",
    "    'image_tensors': filtered_tensors,\n",
    "    'image_labels': filtered_df['Image_Name'],\n",
    "    'device': device,\n",
    "    'vocab_size': vocab_size,\n",
    "    'max_len': total_max,\n",
    "    'clip_model': clip_model,\n",
    "    'optimizer': 'adam',\n",
    "    'learning_rate': 1e-2,\n",
    "    'batch_size': 10,\n",
    "    'instance_weight': 1,\n",
    "    'sem_weight': 0.1,\n",
    "    'itm_weight': 0.1,\n",
    "    'initial_margin': 1.0,\n",
    "    'margin_step': 0.3,\n",
    "    'max_margin': 5.0,\n",
    "    'best_model_parameters_path': param_path,\n",
    "    'decoder_lambda': 0.1,\n",
    "    'topk': 10\n",
    "    # 'patience':5\n",
    "    # 'max_lengths': {\n",
    "    #     'ingredient_tokens': max_length_ing,\n",
    "    #     'instruction_tokens': max_length_inst,\n",
    "    #     'title_tokens': max_length_title\n",
    "    # }\n",
    "\n",
    "}\n",
    "image2recipe = Trainer(**kwargs)\n",
    "image2recipe.train()\n",
    "image2recipe.plot_learning_loss_curves()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQDow2_yJQO1"
   },
   "source": [
    "Now that the model is trained, use it to guess images recipes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z-U9830iF48G"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  #same size as training\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) #same norm as training\n",
    "])\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    return transform(image).unsqueeze(0) #add batch dim of 1 at 0 indice\n",
    "\n",
    "def extract_image_features(image, model, device):\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "      image_features = model.image_encoder(image.to(device))\n",
    "      image_features = torch.nn.functional.normalize(image_features, p=2, dim=-1)\n",
    "  return image_features\n",
    "\n",
    "def load_model(model_path):\n",
    "    image_encoder = Image_Encoder(device, clip_model, num_classes).to(device)\n",
    "    recipe_encoder = RecipeEncoder(device, vocab_size, max_len).to(device)\n",
    "    mmr = MMR(hidden_dim=image_encoder.clip_model.config.projection_dim).to(device)\n",
    "    model = Image2Recipe(image_encoder, recipe_encoder, mmr).to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "image_path = \"insert test image path\"\n",
    "tensor_image = preprocess_image(image_path)\n",
    "image2recipe = load_model(param_path)\n",
    "image_features = extract_image_features(tensor_image, image2recipe.model, device)\n",
    "print(image_features.shape)\n",
    "\n",
    "recipe_embeddings = np.load(recipe_embeddings_path)\n",
    "\n",
    "\n",
    "#Compute cosine similarities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TDjNFmyWbeE0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "tfood_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0220ffb3e36e4c75b6ed3e55c24e4c1c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_981ec88de28948649082f7eb09c8514f",
       "IPY_MODEL_c7f944fb8e7b435682ad5eee4822c2e1",
       "IPY_MODEL_673e94a1be884a91a195b17bca6d3d53"
      ],
      "layout": "IPY_MODEL_ebd24a80163b45cd93e42e1b0243f322"
     }
    },
    "023f8cca8c2d4dc5a657a367698fb594": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0ce18775ebd045538b5ea54b6df58534": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0d71ff23238a466b96f70f2d63c5ae65": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0e6ae4acf39c49eea6e6f496f789cb7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cf3612e13a1e47c994109cab4b884967",
      "placeholder": "​",
      "style": "IPY_MODEL_cf1a49de98bb4cb69a498b727d84fe91",
      "value": " 49/49 [00:23&lt;00:00,  2.05it/s]"
     }
    },
    "10f07ecbd4204b628aa8296e82adfdd1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f3b5677a4e594d2bbf34663d6cf2d6ee",
       "IPY_MODEL_987a1cc8348d4198963801c2e530ee70",
       "IPY_MODEL_31edf3abf6444c6c97df5b2d2d592853"
      ],
      "layout": "IPY_MODEL_34bd25cd121443849fcac9a5b1850e27"
     }
    },
    "180f27adaa584be194bb434a0b48b77a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e3e03f6bb2a461d94107c9dd83c5542": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "24a3bf8f35fd442d8da70b0a0a5ba32c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2c7dd2a62b284e4da5cff413b721beca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "311937e002954762a0467c3e3bf2eb9e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_93d326d18c374c54ac924b25b08705d7",
       "IPY_MODEL_b07e34bdf78446d8901b6a87451f34c5",
       "IPY_MODEL_0e6ae4acf39c49eea6e6f496f789cb7b"
      ],
      "layout": "IPY_MODEL_bbcd4ecede72439fb88de790e693022f"
     }
    },
    "31edf3abf6444c6c97df5b2d2d592853": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ae90b16bc0264437961964838c4d785c",
      "placeholder": "​",
      "style": "IPY_MODEL_b5e658893e504d408234329b610e28a9",
      "value": " 49/49 [00:24&lt;00:00,  2.05it/s]"
     }
    },
    "334b6d98b4a84207bda43905ec6be98d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "34bd25cd121443849fcac9a5b1850e27": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "38ed1390393349d0a6cbf52548c9882c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4359e49aea904fbcb184859d1d7720db",
      "placeholder": "​",
      "style": "IPY_MODEL_334b6d98b4a84207bda43905ec6be98d",
      "value": " 216/447 [04:04&lt;04:20,  1.13s/it]"
     }
    },
    "4359e49aea904fbcb184859d1d7720db": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4f36b462b7354e04a88b83740272a8fc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4fcec0ae9f384468a7893bf3af79344a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "502de043794e42f3966355d3f86627c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "51c0f66cc1e8495ca8d5627afb22e409": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_180f27adaa584be194bb434a0b48b77a",
      "placeholder": "​",
      "style": "IPY_MODEL_b6ddd2991ae54bdfa04d400d7aa805b0",
      "value": "100%"
     }
    },
    "5be38af07a83411bb5c952760f6fe756": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "64c57d3ce0284196a29b6635648791e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8bfe54343b854d3987fec90776321edb",
      "max": 447,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_502de043794e42f3966355d3f86627c6",
      "value": 447
     }
    },
    "673e94a1be884a91a195b17bca6d3d53": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4fcec0ae9f384468a7893bf3af79344a",
      "placeholder": "​",
      "style": "IPY_MODEL_e98dc7f1f6c540e5b2d49634d1321b3f",
      "value": " 447/447 [08:23&lt;00:00,  1.13s/it]"
     }
    },
    "6cb9d159e7ab4e95b8c21045af556368": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6e5fc1b8113842f98ddde191d9fe45b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4f36b462b7354e04a88b83740272a8fc",
      "max": 49,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f2cef38d95e64616a6a0573c6002ef49",
      "value": 49
     }
    },
    "6fefe3bf0a86431fb79d84a977b5d8af": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "76dc981d640a4937b4e6fc83ec79c2c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_51c0f66cc1e8495ca8d5627afb22e409",
       "IPY_MODEL_6e5fc1b8113842f98ddde191d9fe45b3",
       "IPY_MODEL_c0fa32d3e07341688a52b3f882f19dc7"
      ],
      "layout": "IPY_MODEL_6cb9d159e7ab4e95b8c21045af556368"
     }
    },
    "784f4db651b04f699ace9cc56c395cf4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7e6e4f6565de4a8aa1e3bd8223e96e5f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f14b1b1b6dea44e38243b6c9e37280e1",
       "IPY_MODEL_e12a7d47cdc24fa9999bbc1d8166b255",
       "IPY_MODEL_ce290fab3c3743178b6f26d715c168f4"
      ],
      "layout": "IPY_MODEL_6fefe3bf0a86431fb79d84a977b5d8af"
     }
    },
    "87a25feb6ddc4879b4443055e58fe300": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8913c40a38b9461680bdc540d2aafd0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bcb247ab9b1b4768bf22ea2bfbf48b40",
       "IPY_MODEL_e1a2f37a9a60492e9c030b0b2e7922fc",
       "IPY_MODEL_38ed1390393349d0a6cbf52548c9882c"
      ],
      "layout": "IPY_MODEL_a017b43c35c9487b9a44230090484369"
     }
    },
    "8bfe54343b854d3987fec90776321edb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8ecc6dd1fc2a427f90122d866679eaff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "907131e8fa8347de8d1da78b1f143825": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "93d326d18c374c54ac924b25b08705d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ea43306fb91b4029b870f78c9346246d",
      "placeholder": "​",
      "style": "IPY_MODEL_f22f6ee1a7844514897c563972b8b4b8",
      "value": "100%"
     }
    },
    "981ec88de28948649082f7eb09c8514f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_907131e8fa8347de8d1da78b1f143825",
      "placeholder": "​",
      "style": "IPY_MODEL_8ecc6dd1fc2a427f90122d866679eaff",
      "value": "100%"
     }
    },
    "987a1cc8348d4198963801c2e530ee70": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_023f8cca8c2d4dc5a657a367698fb594",
      "max": 49,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1e3e03f6bb2a461d94107c9dd83c5542",
      "value": 49
     }
    },
    "9c0bd341d2a2485d90b27b76792c57b7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a017b43c35c9487b9a44230090484369": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7ddb93135e241fd9cb7f9bfb4d0a92e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ab70f71db8b6437e922c30c26c2ef26e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_24a3bf8f35fd442d8da70b0a0a5ba32c",
      "placeholder": "​",
      "style": "IPY_MODEL_b7a76a2e0ece401e8c9216ca371c7a36",
      "value": "100%"
     }
    },
    "ab9e2b297a804228a07878595c608f6b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae90b16bc0264437961964838c4d785c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b07e34bdf78446d8901b6a87451f34c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0ce18775ebd045538b5ea54b6df58534",
      "max": 49,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a7ddb93135e241fd9cb7f9bfb4d0a92e",
      "value": 49
     }
    },
    "b0d1b91728a2408e9a1a82b3a11567b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b56bed9b9afb4f72ba43f98fe41a1869": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b5e658893e504d408234329b610e28a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b6ddd2991ae54bdfa04d400d7aa805b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b7a76a2e0ece401e8c9216ca371c7a36": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bbcd4ecede72439fb88de790e693022f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bcb247ab9b1b4768bf22ea2bfbf48b40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ab9e2b297a804228a07878595c608f6b",
      "placeholder": "​",
      "style": "IPY_MODEL_faf208150d734f7797e66b09624abcf1",
      "value": " 48%"
     }
    },
    "bd6b2b9972db4979b7ffad15b5456ea9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ab70f71db8b6437e922c30c26c2ef26e",
       "IPY_MODEL_64c57d3ce0284196a29b6635648791e2",
       "IPY_MODEL_c3434c53948c426c9ec57b4ea218ebe7"
      ],
      "layout": "IPY_MODEL_e08fc50be3a44efdb689c26f03f9a576"
     }
    },
    "c0fa32d3e07341688a52b3f882f19dc7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_87a25feb6ddc4879b4443055e58fe300",
      "placeholder": "​",
      "style": "IPY_MODEL_e83ee0eb906843c5b0d8e47c22259bac",
      "value": " 49/49 [00:23&lt;00:00,  2.05it/s]"
     }
    },
    "c3434c53948c426c9ec57b4ea218ebe7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d07e2fcd72e74782b0e35f9e3c4ce0f7",
      "placeholder": "​",
      "style": "IPY_MODEL_f217b28a84c2466fa579f4bb08f39e97",
      "value": " 447/447 [08:23&lt;00:00,  1.11s/it]"
     }
    },
    "c7f944fb8e7b435682ad5eee4822c2e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e7a6df47fbef4fa4aeb811a0d4a8e38a",
      "max": 447,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d3fd9de28eca4dd285c6cdc57eefbaf2",
      "value": 447
     }
    },
    "ce290fab3c3743178b6f26d715c168f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_784f4db651b04f699ace9cc56c395cf4",
      "placeholder": "​",
      "style": "IPY_MODEL_edb141a6cd9f4523bce88667f63cf9e8",
      "value": " 447/447 [08:25&lt;00:00,  1.12s/it]"
     }
    },
    "cf1a49de98bb4cb69a498b727d84fe91": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cf3612e13a1e47c994109cab4b884967": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d07e2fcd72e74782b0e35f9e3c4ce0f7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d3fd9de28eca4dd285c6cdc57eefbaf2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e08fc50be3a44efdb689c26f03f9a576": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e12a7d47cdc24fa9999bbc1d8166b255": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9c0bd341d2a2485d90b27b76792c57b7",
      "max": 447,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f19f1268095a4d5f8c43a87cc2f7893b",
      "value": 447
     }
    },
    "e1a2f37a9a60492e9c030b0b2e7922fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5be38af07a83411bb5c952760f6fe756",
      "max": 447,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2c7dd2a62b284e4da5cff413b721beca",
      "value": 216
     }
    },
    "e7a6df47fbef4fa4aeb811a0d4a8e38a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e83ee0eb906843c5b0d8e47c22259bac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e98dc7f1f6c540e5b2d49634d1321b3f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ea43306fb91b4029b870f78c9346246d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ebd24a80163b45cd93e42e1b0243f322": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "edb141a6cd9f4523bce88667f63cf9e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f14b1b1b6dea44e38243b6c9e37280e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b56bed9b9afb4f72ba43f98fe41a1869",
      "placeholder": "​",
      "style": "IPY_MODEL_0d71ff23238a466b96f70f2d63c5ae65",
      "value": "100%"
     }
    },
    "f19f1268095a4d5f8c43a87cc2f7893b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f217b28a84c2466fa579f4bb08f39e97": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f22f6ee1a7844514897c563972b8b4b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f2cef38d95e64616a6a0573c6002ef49": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f3b5677a4e594d2bbf34663d6cf2d6ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fda7113daebb41508ee38d03da200052",
      "placeholder": "​",
      "style": "IPY_MODEL_b0d1b91728a2408e9a1a82b3a11567b0",
      "value": "100%"
     }
    },
    "faf208150d734f7797e66b09624abcf1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fda7113daebb41508ee38d03da200052": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
